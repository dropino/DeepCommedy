{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepComedy_v2_letter_tokenization_noSyll.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nrEohsVOSEg2",
        "fnAp1FL5LbYO"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPROQ2iTWsN5"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAwbk6Au6JCU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7c62707e-c77a-43f1-ace8-3f776f0929c2"
      },
      "source": [
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\"\"\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfrom google.colab import drive\\ndrive.mount('/content/gdrive')\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M2JG_2Xb6j8",
        "outputId": "5c79cecb-5063-491a-f317-204bf5cdbca8"
      },
      "source": [
        "!pip install tensorflow_datasets\n",
        "!pip install -U tensorflow-text"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (21.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.16.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.3.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.62.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (5.2.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.15.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow_datasets) (3.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.7,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (2.6.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (5.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.12)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.12.1)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.19.5)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (0.37.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (3.7.4.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (2.6.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (2.6.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (2.6.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (1.40.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow-text) (3.5.0)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPzlEKUbPIIE",
        "outputId": "d91ea683-e122-4510-a1c6-fc065691ab8c"
      },
      "source": [
        "#imports\n",
        "#@title Import & seed\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import requests\n",
        "import collections\n",
        "import pickle\n",
        "import copy, random\n",
        "import nltk as nl\n",
        "nl.download('punkt')\n",
        "from itertools import zip_longest\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Reshape, BatchNormalization, Dense, Dropout, concatenate,\n",
        "    Embedding, LSTM, Dense, GRU, Bidirectional, Add\n",
        ")\n",
        "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "#Variables\n",
        "FILENAME = \"divineComedyProf.txt\"\n",
        "CLEANFILENAME = \"cleanComedyProf.txt\"\n",
        "INFERNO = \"inferno_syllnew.txt\"\n",
        "PURGATORIO = \"purgatorio_syllnew.txt\"\n",
        "PARADISO = \"paradiso_syllnew.txt\"\n",
        "PARADISOCLEAN = \"paradiso_clean.txt\"\n",
        "\n",
        "\n",
        "data_path = 'noSyllComedy.txt'  \n",
        "# dataset location, here just the name of the source file\n",
        "vocab_size = 1900\n",
        "terces_per_batch = 8\n",
        "terces_len = 200\n",
        "batch_len = terces_per_batch * (terces_len + 1)\n",
        "EPOCHS = 10\n",
        "\n",
        "\n",
        "#Setup\n",
        "print(tf.__version__)\n",
        "\n",
        "#np.random.seed(1234)\n",
        "!nvidia-smi"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2.6.0\n",
            "Fri Sep 24 10:54:04 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0    73W / 149W |   1192MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bkfS7t6vpYIJ",
        "outputId": "04442013-b2eb-466a-cdd0-2327ff76748b"
      },
      "source": [
        "#@title Setup wandb\n",
        "!pip install wandb\n",
        "!wandb login e6569c556c797b5bed38bc6cba40d24b68e8973d\n",
        "import wandb\n",
        "wandb.init(project=\"DeepComedy\", name=\"bananito96\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.2-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 44.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.4.1-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 50.9 MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=9e11113a4dab429fd2ebd913f0d3bd5fed6d764f9b2a5be776991a60f95bc27d\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=67cf098bd55056557ad0639a51112275800abe7ab32fac3be92e250e2bca422e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.24 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.4.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.12.2 yaspin-2.1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdropino\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">bananito96</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/dropino/DeepComedy\" target=\"_blank\">https://wandb.ai/dropino/DeepComedy</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/dropino/DeepComedy/runs/3nskzfca\" target=\"_blank\">https://wandb.ai/dropino/DeepComedy/runs/3nskzfca</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210924_103028-3nskzfca</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f0f2f80d3d0>"
            ],
            "text/html": [
              "<h1>Run(3nskzfca)</h1><iframe src=\"https://wandb.ai/dropino/DeepComedy/runs/3nskzfca\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrEohsVOSEg2"
      },
      "source": [
        "#Clean files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "CczzJREJ0-Y4",
        "outputId": "d56d1bc5-3308-4f48-df98-1acf9d472404"
      },
      "source": [
        "\"\"\"\n",
        "#count chars\n",
        "char_list = []\n",
        "with open(FILENAME) as file:\n",
        "  while True:\n",
        "    char = file.read(1)\n",
        "    if not char:\n",
        "      print(\"End of file\")\n",
        "      break\n",
        "    char = char.lower()\n",
        "\n",
        "    #add good char to char list\n",
        "    if char not in char_list:\n",
        "      char_list.append(char)\n",
        "\n",
        "print(char_list)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n#count chars\\nchar_list = []\\nwith open(FILENAME) as file:\\n  while True:\\n    char = file.read(1)\\n    if not char:\\n      print(\"End of file\")\\n      break\\n    char = char.lower()\\n\\n    #add good char to char list\\n    if char not in char_list:\\n      char_list.append(char)\\n\\nprint(char_list)\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "_0tniPNkO1S4",
        "outputId": "797bf01a-8920-4692-a257-eace6c58c9b0"
      },
      "source": [
        "\"\"\"\n",
        "removable_chars = ['•', '—', '-', '(', ')', ',', '.', ':', ';', '“', '”', '«', '»', '\"','0','1','2','3','4','5','6','7','8','9']\n",
        "#‘ e ’ usate per racchiudere parti parlate in latino\n",
        "#“ e ” racchiude il parlato sia in italiano che latino\n",
        "#idem « e »\n",
        "\n",
        "\n",
        "clean_char_list = []\n",
        "removedCharsCounter = 0\n",
        "fileOut = open(CLEANFILENAME, \"a\")\n",
        "\n",
        "with open(FILENAME) as file:\n",
        "  while True:\n",
        "    char = file.read(1)\n",
        "    if not char:\n",
        "      print(\"End of file\")\n",
        "      break\n",
        "\n",
        "    #transform upper case to lower case\n",
        "    char = char.lower()\n",
        "\n",
        "    #remove unwanted chars\n",
        "    if char not in removable_chars:\n",
        "\n",
        "      #add acceptable chars to char list\n",
        "      if char not in clean_char_list:\n",
        "        clean_char_list.append(char)\n",
        "      \n",
        "      #output in new file\n",
        "      fileOut.write(char)\n",
        "    else:\n",
        "      removedCharsCounter += 1\n",
        "\n",
        "\n",
        "print(char_list)\n",
        "print(removedCharsCounter)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nremovable_chars = [\\'•\\', \\'—\\', \\'-\\', \\'(\\', \\')\\', \\',\\', \\'.\\', \\':\\', \\';\\', \\'“\\', \\'”\\', \\'«\\', \\'»\\', \\'\"\\',\\'0\\',\\'1\\',\\'2\\',\\'3\\',\\'4\\',\\'5\\',\\'6\\',\\'7\\',\\'8\\',\\'9\\']\\n#‘ e ’ usate per racchiudere parti parlate in latino\\n#“ e ” racchiude il parlato sia in italiano che latino\\n#idem « e »\\n\\n\\nclean_char_list = []\\nremovedCharsCounter = 0\\nfileOut = open(CLEANFILENAME, \"a\")\\n\\nwith open(FILENAME) as file:\\n  while True:\\n    char = file.read(1)\\n    if not char:\\n      print(\"End of file\")\\n      break\\n\\n    #transform upper case to lower case\\n    char = char.lower()\\n\\n    #remove unwanted chars\\n    if char not in removable_chars:\\n\\n      #add acceptable chars to char list\\n      if char not in clean_char_list:\\n        clean_char_list.append(char)\\n      \\n      #output in new file\\n      fileOut.write(char)\\n    else:\\n      removedCharsCounter += 1\\n\\n\\nprint(char_list)\\nprint(removedCharsCounter)\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "lxF6HYN5ANpF",
        "outputId": "541395e5-9115-4be7-baf4-7c128f310dc3"
      },
      "source": [
        "\"\"\"\n",
        "f = open(CLEANFILENAME, \"r\")\n",
        "lines = f.readlines()\n",
        "\n",
        "number_of_syllables = []\n",
        "line_counter = 1\n",
        "\n",
        "for line in lines:\n",
        "  #excludes all empty lines and canto intros\n",
        "  if '|' in line:\n",
        "    syllables = line.count('|')\n",
        "    number_of_syllables.append(syllables)\n",
        "    if syllables != 11:\n",
        "      print(\"line number {} with text {} has {} syllables\".format(line_counter,line,syllables))\n",
        "    line_counter += 1\n",
        "\n",
        "Counter(number_of_syllables)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nf = open(CLEANFILENAME, \"r\")\\nlines = f.readlines()\\n\\nnumber_of_syllables = []\\nline_counter = 1\\n\\nfor line in lines:\\n  #excludes all empty lines and canto intros\\n  if \\'|\\' in line:\\n    syllables = line.count(\\'|\\')\\n    number_of_syllables.append(syllables)\\n    if syllables != 11:\\n      print(\"line number {} with text {} has {} syllables\".format(line_counter,line,syllables))\\n    line_counter += 1\\n\\nCounter(number_of_syllables)\\n'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnAp1FL5LbYO"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "\"\"\"Once tokenized the dataset, a map of the syllables with the integer index is created. The final dimension\n",
        "of the vocabulary is 1874 tokens but it is limited to 1800 to remove the tail of infrequent syllables.\n",
        "\n",
        "We substituted every space between words with the special token < SEP > and inserted at the beginning\n",
        "of each verse the token < GO >. To make all verses the same lengths, we used the special character\n",
        "“< PAD >” to pad every terces to the length of 75 tokens. At the end of each verse we appended the\n",
        "symbol “< EOV > “, while at the end of each sentence “< EOS >”.\"\"\"\n",
        "\n",
        "number_of_syllables = []\n",
        "line_counter = 1\n",
        "\"\"\"\n",
        "space -> <SPA>\n",
        "verst start -> <VST>\n",
        "padding (up to 75) -> <PAD>\n",
        "end of verse -> <EOV>\n",
        "end of sentence -> <EOS>\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1mcovxWPadi",
        "outputId": "3fc0aaf2-cfcf-4a25-d890-e8b63c50e4ef"
      },
      "source": [
        "def get_hyp_lm_tercets(tercets):\n",
        "    new_tercets = []\n",
        "    for tercet in tercets:\n",
        "        new_tercets.append([])\n",
        "        for verse in tercet:\n",
        "            new_tercets[-1].append([])\n",
        "            for hyp_w in verse:\n",
        "                new_tercets[-1][-1].extend(hyp_w)\n",
        "                new_tercets[-1][-1].append('<SEP>')\n",
        "            new_tercets[-1][-1] = new_tercets[-1][-1][:-1]\n",
        "\n",
        "    return new_tercets\n",
        "\n",
        "\n",
        "\n",
        "def get_dc_cantos(filename, encoding=None):\n",
        "    # raw_data = read_words(filename=filename)\n",
        "    cantos = []\n",
        "    cantoCounter =  0\n",
        "    cantos.append([])\n",
        "    with open(filename, \"r\", encoding=encoding) as f:\n",
        "        for line in f:\n",
        "            sentence = line.strip()\n",
        "            if 'canto' not in sentence and len(sentence) > 2:\n",
        "                if len(sentence) > 2:\n",
        "                    cantos[cantoCounter].append(sentence)\n",
        "                    \"\"\"\n",
        "                    if len(cantos[cantoCounter]) == 3:\n",
        "                      cantoCounter += 1\n",
        "                      cantos.append([])\n",
        "                    \"\"\"\n",
        "    return cantos\n",
        "\n",
        "\n",
        "def create_tercets(cantos):\n",
        "    tercets = []\n",
        "    for i,canto in enumerate(cantos):\n",
        "        for v,verse in enumerate(canto):\n",
        "            if v%3 == 0:\n",
        "                tercets.append([])\n",
        "\n",
        "            tercets[-1].append(verse)\n",
        "        tercets = tercets[:-1]  # removes the last malformed tercets (only 2 verses)\n",
        "\n",
        "    return tercets\n",
        "\n",
        "def pad_list(l, pad_token, max_l_size, keep_lasts=False, pad_right=True):\n",
        "    \"\"\"\n",
        "    Adds a padding token to a list\n",
        "    inputs:\n",
        "    :param l: input list to pad.\n",
        "    :param pad_token: value to add as padding.\n",
        "    :param max_l_size: length of the new padded list to return,\n",
        "    it truncates lists longer that 'max_l_size' without adding\n",
        "    padding values.\n",
        "    :param keep_lasts: If True, preserves the max_l_size last elements\n",
        "    of a sequence (by keeping the same order).  E.g.:\n",
        "    if keep_lasts is True and max_l_size=3 [1,2,3,4] becomes [2,3,4].\n",
        "\n",
        "\n",
        "    :return: the list padded or truncated.\n",
        "    \"\"\"\n",
        "    to_pad = []\n",
        "    max_l = min(max_l_size, len(l))  # maximum len\n",
        "    l_init = len(l) - max_l if len(l) > max_l and keep_lasts else 0  # initial position where to sample from the list\n",
        "    l_end = len(l) if len(l) > max_l and keep_lasts else max_l\n",
        "    for i in range(l_init, l_end):\n",
        "        to_pad.append(l[i])\n",
        "\n",
        "    # for j in range(len(l), max_l_size):\n",
        "    #     to_pad.append(pad_token)\n",
        "    pad_tokens = [pad_token] * (max_l_size-len(l))\n",
        "    padded_l = to_pad + pad_tokens if pad_right else pad_tokens + to_pad\n",
        "\n",
        "    return padded_l\n",
        "\n",
        "\n",
        "def save_data(data, file):\n",
        "    with open(file, 'wb') as output:\n",
        "        pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_data(file):\n",
        "    with open(file, 'rb') as obj:\n",
        "        return pickle.load(obj)\n",
        "\n",
        "def print_and_write(file, s):\n",
        "    print(s)\n",
        "    file.write(s)\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, vocab_size=None):\n",
        "        self.dictionary = dict()\n",
        "        self.rev_dictionary = dict()\n",
        "        self.count = []\n",
        "        self.special_tokens = []\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build_vocabulary_from_counts(self, count, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Sets all the attributes of the Vocabulary object.\n",
        "        :param count: a list of lists as follows: [['token', number_of_occurrences],...]\n",
        "        :param special_tokens: a list of strings. E.g. ['<EOS>', '<PAD>',...]\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        dictionary = dict()\n",
        "        for word, _ in count:\n",
        "            dictionary[word] = len(dictionary)\n",
        "\n",
        "        # adding eventual special tokens to the dictionary (e.g. <EOS>,<PAD> etc..)\n",
        "        d = len(dictionary)\n",
        "        for i, token in enumerate(special_tokens):\n",
        "            dictionary[token] = d + i\n",
        "\n",
        "        self.count = count\n",
        "        self.dictionary = dictionary\n",
        "        self.rev_dictionary = dict(zip(self.dictionary.values(), self.dictionary.keys()))\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = len(dictionary)\n",
        "\n",
        "    def build_vocabulary_from_tokens(self, tokens, vocabulary_size=None, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Given a list of tokens, it sets the Vocabulary object attributes by constructing\n",
        "        a dictionary mapping each token to a unique id.\n",
        "        :param tokens: a list of strings.\n",
        "         E.g. [\"the\", \"cat\", \"is\", ... \".\", \"the\", \"house\" ,\"is\" ...].\n",
        "         NB: Here you should put all your token instances of the corpus.\n",
        "        :param vocabulary_size: The number of elements of your vocabulary. If there are more\n",
        "        than 'vocabulary_size' elements on tokens, it considers only the 'vocabulary_size'\n",
        "        most frequent ones.\n",
        "        :param special_tokens: Optional. A list of strings. Useful to add special tokens in vocabulary.\n",
        "        If you don't have any, keep it empty.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        vocabulary_size = vocabulary_size if vocabulary_size is not None else self.vocab_size\n",
        "        vocabulary_size = vocabulary_size - (len(special_tokens) + 1) if vocabulary_size else None\n",
        "        # counts occurrences of each token\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(collections.Counter(tokens).most_common(vocabulary_size))  # takes only the most frequent ones, if size is None takes them all\n",
        "        self.build_vocabulary_from_counts(count, special_tokens)  # actually build the vocabulary\n",
        "        self._set_unk_count(tokens)  # set the number of OOV instances\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabulary(vocab0, vocab1, vocabulary_size=-1):\n",
        "        \"\"\"\n",
        "        Merge two Vocabulary objects into a new one.\n",
        "        :param vocab0: first Vocabulary object\n",
        "        :param vocab1: second Vocabulary object\n",
        "        :param vocabulary_size: parameter to decide the merged vocabulary size.\n",
        "        With default value -1, all the words of both vocabularies are preserved.\n",
        "        When set to 0, the size of the vocabulary is set to the size of vocab0,\n",
        "        when set to 1 it is kept the size of vocab1.\n",
        "        :return: a new vocabulary\n",
        "        \"\"\"\n",
        "        # get size of the new vocabulary\n",
        "        vocab_size = vocab0.vocab_size + vocab1.vocab_size if vocabulary_size == -1 else vocabulary_size\n",
        "        merged_special_tokens = list(set(vocab0.special_tokens) | set(vocab1.special_tokens))\n",
        "\n",
        "        # merge the counts from the two vocabularies and then selects the most_common tokens\n",
        "        merged_counts = collections.Counter(dict(vocab0.count)) + collections.Counter(dict(vocab1.count))\n",
        "        merged_counts = merged_counts.most_common(vocab_size)\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(merged_counts)\n",
        "\n",
        "        # create the new vocabulary\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        merged_vocab.build_vocabulary_from_counts(count, merged_special_tokens)\n",
        "        return merged_vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_vocabularies(vocab_list, vocab_size=None):\n",
        "        \"\"\"\n",
        "        Join a list of vocabularies into a new one.\n",
        "        :param vocab_list: a list of Vocabulary objects\n",
        "        :param vocab_size: the maximum size of the merged vocabulary.\n",
        "        :return: a vocabulary merging them all.\n",
        "        \"\"\"\n",
        "        vocab_size = vocab_size if vocab_size else sum([v.vocab_size for v in vocab_list])\n",
        "        merged_vocab = Vocabulary(vocab_size)\n",
        "        for voc in vocab_list:\n",
        "            merged_vocab = Vocabulary.merge_vocabulary(merged_vocab, voc, vocab_size)\n",
        "        return merged_vocab\n",
        "\n",
        "    def string2id(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of strings into a dataset of ids according to the object dictionary.\n",
        "        :param dataset: any string-based dataset with any nested lists.\n",
        "        :return: a new dataset, with the same shape of dataset, where each string is mapped into its\n",
        "        corresponding id associated in the dictionary (0 for unknown tokens).\n",
        "        \"\"\"\n",
        "\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, str) or isinstance(item, int) or isinstance(item, float):\n",
        "                    new_items.append(self.word2id(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def id2string(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of integer ids into a dataset of string according to the reverse dictionary.\n",
        "        :param dataset: any int-based dataset with any nested lists. Allowed types are int, np.int32, np.int64.\n",
        "        :return: a new dataset, with the same shape of dataset, where each token is mapped into its\n",
        "        corresponding string associated in the reverse dictionary.\n",
        "        \"\"\"\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, int) or isinstance(item, np.int) or isinstance(item, np.int32) or isinstance(item, np.int64):\n",
        "                    new_items.append(self.id2word(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def word2id(self, item):\n",
        "        \"\"\"\n",
        "        Maps a string token to its corresponding id.\n",
        "        :param item: a string.\n",
        "        :return: If the token belongs to the vocabulary, it returns an integer id > 0, otherwise\n",
        "        it returns the value associated to the unknown symbol, that is typically 0.\n",
        "        \"\"\"\n",
        "        return self.dictionary[item] if item in self.dictionary else self.dictionary['<UNK>']\n",
        "\n",
        "    def id2word(self, token_id):\n",
        "        \"\"\"\n",
        "        Maps an integer token to its corresponding string.\n",
        "        :param token_id: an integer.\n",
        "        :return: If the id belongs to the vocabulary, it returns the string\n",
        "        associated to it, otherwise it returns the string associated\n",
        "        to the unknown symbol, that is '<UNK>'.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.rev_dictionary[token_id] if token_id in self.rev_dictionary else self.rev_dictionary[self.dictionary['<UNK>']]\n",
        "\n",
        "    def get_unk_count(self):\n",
        "        return self.count[0][1]\n",
        "\n",
        "    def _set_unk_count(self, tokens):\n",
        "        \"\"\"\n",
        "        Sets the number of OOV instances in the tokens provided\n",
        "        :param tokens: a list of tokens\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        data = list()\n",
        "        unk_count = 0\n",
        "        for word in tokens:\n",
        "            if word in self.dictionary:\n",
        "                index = self.dictionary[word]\n",
        "            else:\n",
        "                index = 0  # dictionary['<UNK>']\n",
        "                unk_count += 1\n",
        "            data.append(index)\n",
        "        self.count[0][1] = unk_count\n",
        "\n",
        "    def add_element(self, name, is_special_token=False):\n",
        "        if name not in self.dictionary:\n",
        "            self.vocab_size += 1\n",
        "            self.dictionary[name] = self.vocab_size\n",
        "            self.rev_dictionary[self.vocab_size] = name\n",
        "\n",
        "            if is_special_token:\n",
        "                self.special_tokens = list(self.special_tokens)\n",
        "                self.special_tokens.append(name)\n",
        "\n",
        "            self.count.append([name, 1])\n",
        "\n",
        "    def set_vocabulary(self, dictionary, rev_dictionary, special_tokens, vocab_size):\n",
        "        self.dictionary = dictionary,\n",
        "        self.rev_dictionary = rev_dictionary\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vocabulary(filename):\n",
        "        return load_data(filename)\n",
        "\n",
        "    def save_vocabulary(self, filename):\n",
        "        save_data(self, filename)\n",
        "\n",
        "class SyLMDataset(object):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        self.config = config\n",
        "        self.vocabulary = sy_vocab\n",
        "\n",
        "        self.raw_train_x = []\n",
        "        self.raw_val_x = []\n",
        "        self.raw_test_x = []\n",
        "        self.raw_x = []\n",
        "\n",
        "        self.train_x, self.train_y = [], []\n",
        "        self.val_x, self.val_y = [], []\n",
        "        self.test_x, self.test_y = [], []\n",
        "        self.x, self.y = [], []\n",
        "\n",
        "    def initialize(self, sess):\n",
        "        pass\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Extract raw texts form sources and gather them all together.\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :return: a list of raw strings.\n",
        "        \"\"\"\n",
        "        return NotImplementedError\n",
        "\n",
        "    def build(self, sources, split_size=0.8):\n",
        "        \"\"\"\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :param split_size: the size to split the dataset, set >=1.0 to not split.\n",
        "        \"\"\"\n",
        "\n",
        "        raw_x = self.load(sources)\n",
        "        # raw_x = self.tokenize([self.preprocess(ex) for ex in raw_x])  # fixme\n",
        "        # splitting data\n",
        "        self.raw_x = raw_x\n",
        "        if split_size < 1.0:\n",
        "            self.raw_train_x, self.raw_test_x = self.split(self.raw_x, train_size=split_size)\n",
        "            self.raw_train_x, self.raw_val_x = self.split(self.raw_train_x, train_size=split_size)\n",
        "        else:\n",
        "            self.raw_train_x = self.raw_x\n",
        "\n",
        "        if self.vocabulary is None:\n",
        "            # creates vocabulary\n",
        "            tokens = [item for sublist in self.raw_train_x for item in sublist]  # get tokens\n",
        "            special_tokens = (\"<GO>\", \"<PAD>\", \"<SEP>\", \"<EOS>\", \"<EOV>\")\n",
        "            self._create_vocab(tokens, special_tokens=special_tokens)\n",
        "\n",
        "        # creates x,y for train\n",
        "        self.train_x = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.train_y = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.val_x = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.val_y = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for testing\n",
        "        self.test_x = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.test_y = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "    def _create_vocab(self, tokens, special_tokens=(\"<PAD>\", \"<GO>\", \"<SEP>\", \"<EOV>\", \"<EOS>\")):\n",
        "        \"\"\"\n",
        "        Create the vocabulary. Special tokens can be added to the tokens obtained from\n",
        "        the corpus.\n",
        "        :param tokens: a list of all the tokens in the corpus. Each token is a string.\n",
        "        :param special_tokens: a list of strings.\n",
        "        \"\"\"\n",
        "        print(\"creating_vocabulary\") \n",
        "\n",
        "        vocab = Vocabulary(vocab_size=self.config.input_vocab_size)\n",
        "        vocab.build_vocabulary_from_tokens(tokens, special_tokens=special_tokens)\n",
        "        self.vocabulary = vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def split(raw_data, train_size=0.8):\n",
        "        size = math.floor(len(raw_data)*train_size)\n",
        "        return raw_data[:size], raw_data[size:]\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(txt):\n",
        "        return txt\n",
        "\n",
        "    @staticmethod\n",
        "    def shuffle(x):\n",
        "        return random.sample(x, len(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(txt):\n",
        "        return txt\n",
        "\n",
        "    def _build_dataset(self, raw_data, max_len=200, insert_go=True, keep_lasts=False, pad_right=True, shuffle=True):\n",
        "        \"\"\"\n",
        "        Converts all the tokens in e1_raw_data by mapping each token with its corresponding\n",
        "        value in the dictionary. In case of token not in the dictionary, they are assigned to\n",
        "        a specific id. Each sequence is padded up to the seq_max_len setup in the config.\n",
        "\n",
        "        :param raw_data: list of sequences, each sequence is a list of tokens (strings).\n",
        "        :param max_len: max length of a sequence, crop longer and pad smaller ones.\n",
        "        :param insert_go: True to insert <GO>, False otherwise.\n",
        "        :param keep_lasts: True to truncate initial elements of a sequence.\n",
        "        :param pad_right: pad to the right (default value True), otherwise pads to left.\n",
        "        :param shuffle: Optional. If True data are shuffled.\n",
        "        :return: A list of sequences where each token in each sequence is an int id.\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        for sentence in raw_data:\n",
        "            sentence_ids = [self.vocabulary.word2id(\"<GO>\")] if insert_go else []\n",
        "            sentence_ids.extend([self.vocabulary.word2id(w) for w in sentence])\n",
        "            sentence_ids.append(self.vocabulary.word2id(\"<EOS>\"))\n",
        "            sentence_ids = pad_list(sentence_ids, self.vocabulary.word2id(\"<PAD>\"), max_len, keep_lasts=keep_lasts, pad_right=pad_right)\n",
        "\n",
        "            dataset.append(sentence_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "    def get_batches(self, batch_size=32, split_sel='train'):\n",
        "        \"\"\"\n",
        "        Iterator over the training set. Useful method to run experiments.\n",
        "        :param batch_size: size of the mini_batch\n",
        "        :return: input and target.\n",
        "        \"\"\"\n",
        "        if split_sel == 'train':\n",
        "            x, y = self.train_x, self.train_y\n",
        "        elif split_sel == 'val':\n",
        "            x, y = self.val_x, self.val_y\n",
        "        else:\n",
        "            x, y = self.test_x, self.test_y\n",
        "        \n",
        "        i = 0 #random.randint(0, batch_size)\n",
        "        batches = []\n",
        "        eov = self.vocabulary.word2id(\"<EOV>\")\n",
        "        go = self.vocabulary.word2id(\"<GO>\")\n",
        "        # prepare batches\n",
        "        while i < len(x):\n",
        "            j = 0\n",
        "            batch_x, batch_y = [], []\n",
        "            while j < batch_size and i+j<len(x):\n",
        "                for c in x[i+j]:\n",
        "                  batch_x.append(c)\n",
        "                batch_x.append(eov)\n",
        "                for c in y[i+j]:\n",
        "                  batch_y.append(c)\n",
        "                batch_y.append(eov)\n",
        "                j += 1\n",
        "            i += batch_size\n",
        "            batches.append((batch_x, batch_y))\n",
        "\n",
        "        # supply\n",
        "        i = 0\n",
        "        while i < len(batches):\n",
        "            yield batches[i][0], batches[i][1]\n",
        "            i += 1\n",
        "\n",
        "class DanteSyLMDataset(SyLMDataset):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        \"\"\"\n",
        "        Class to create a dataset from Dante Alighieri's Divine Comedy.\n",
        "        :param config: a Config object\n",
        "        :param sy_vocab: (optional) a Vocabulary object where tokens of the dictionary\n",
        "        are syllables. If None, the vocabulary is create automatically from the source.\n",
        "        \"\"\"\n",
        "        super().__init__(config, sy_vocab)\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Load examples from dataset\n",
        "        :param sources: data filepath.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        canti = get_dc_cantos(filename=sources)  # get raw data from files\n",
        "\n",
        "        tercets = create_tercets(canti)\n",
        "        #tercets = get_hyp_lm_tercets(tercets)\n",
        "        x = []\n",
        "        for tercet in tercets:\n",
        "            x.append([])\n",
        "            for verse in tercet:\n",
        "                x[-1].extend(verse)\n",
        "                x[-1].append(\"<EOV>\")\n",
        "\n",
        "        #x = self.shuffle(x)\n",
        "        return x\n",
        "\n",
        "def seq2str(seq):\n",
        "    def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n",
        "        to_print = ''\n",
        "        for token in batch:\n",
        "            if token in special_tokens:\n",
        "                to_print += ' '\n",
        "            elif end_of_tokens and token in end_of_tokens:\n",
        "                to_print += '\\n'\n",
        "            elif token in rev_vocabulary:\n",
        "                to_print += rev_vocabulary[token]\n",
        "            else:\n",
        "                to_print += '<UNK>'\n",
        "        return to_print\n",
        "\n",
        "    return output2string(seq, poetry_sy_lm_dataset.vocabulary.rev_dictionary,\n",
        "      special_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\"), \n",
        "                      0, \n",
        "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<SEP>\"),\n",
        "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\"), \n",
        "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")],\n",
        "      end_of_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")])\n",
        "\n",
        "class cnfg:\n",
        "  vocab_size = vocab_size\n",
        "  input_vocab_size = vocab_size\n",
        "  sentence_max_len = terces_len\n",
        "\n",
        "config = cnfg()\n",
        "poetry_sy_lm_dataset = DanteSyLMDataset(config, sy_vocab=None)\n",
        "\n",
        "poetry_sy_lm_dataset.build(data_path, split_size=0.99)  # actual creation of  vocabulary (if not provided) and dataset\n",
        "print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n",
        "print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n",
        "print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n",
        "\n",
        "eov = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")\n",
        "pad = poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\")\n",
        "go = poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\")\n",
        "eos = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")\n",
        "\n",
        "\n",
        "batches = [b for b in poetry_sy_lm_dataset.get_batches(terces_per_batch)]\n",
        "print(batches[0][0])\n",
        "#print(batches[0][1])\n",
        "print(len(batches[0][0]))\n",
        "test_b = [b for b in poetry_sy_lm_dataset.get_batches(terces_per_batch, split_sel='test')]\n",
        "#print(test_b[0][0])\n",
        "#print(test_b[0][1])\n",
        "print(len(test_b[0][0]))\n",
        "val_b = [b for b in poetry_sy_lm_dataset.get_batches(terces_per_batch, split_sel='val')]\n",
        "#print(val_b[0][0])\n",
        "#print(val_b[0][1])\n",
        "print(len(val_b[0][0]))\n",
        "len(poetry_sy_lm_dataset.vocabulary.dictionary.items())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating_vocabulary\n",
            "Train size: 4636\n",
            "Val size: 47\n",
            "Test size: 48\n",
            "[38, 6, 2, 8, 1, 15, 2, 24, 24, 5, 1, 12, 2, 8, 1, 11, 3, 15, 15, 4, 6, 1, 12, 4, 1, 6, 5, 10, 9, 7, 3, 1, 17, 4, 9, 3, 42, 15, 4, 1, 7, 4, 9, 7, 5, 17, 3, 4, 1, 16, 2, 7, 1, 14, 6, 3, 1, 10, 2, 8, 17, 3, 1, 5, 10, 11, 14, 7, 3, 42, 11, 20, 29, 1, 8, 3, 1, 12, 4, 7, 4, 9, 9, 3, 1, 17, 4, 3, 1, 2, 7, 3, 1, 10, 15, 3, 7, 7, 4, 9, 3, 42, 41, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 42, 38, 3, 20, 4, 1, 22, 14, 3, 6, 9, 5, 1, 3, 1, 12, 4, 7, 1, 22, 14, 3, 8, 1, 2, 7, 3, 1, 28, 1, 11, 5, 10, 3, 1, 12, 14, 7, 3, 42, 2, 10, 9, 3, 1, 10, 2, 8, 17, 3, 1, 10, 2, 8, 17, 3, 19, 19, 4, 3, 1, 2, 1, 3, 10, 16, 7, 3, 1, 2, 1, 21, 5, 7, 9, 2, 42, 11, 20, 2, 1, 6, 2, 8, 1, 16, 2, 6, 10, 4, 2, 7, 1, 7, 4, 6, 5, 17, 3, 1, 8, 3, 1, 16, 3, 14, 7, 3, 32, 42, 41, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 42, 38, 9, 3, 6, 9, 18, 1, 28, 1, 3, 15, 3, 7, 3, 1, 11, 20, 2, 1, 16, 5, 11, 5, 1, 28, 1, 16, 4, 26, 1, 15, 5, 7, 9, 2, 42, 15, 3, 1, 16, 2, 7, 1, 9, 7, 3, 9, 9, 3, 7, 1, 12, 2, 8, 1, 23, 2, 6, 1, 11, 20, 18, 1, 4, 18, 1, 17, 4, 1, 9, 7, 5, 17, 3, 4, 42, 12, 4, 7, 27, 1, 12, 2, 1, 8, 18, 1, 3, 8, 9, 7, 2, 1, 11, 5, 10, 2, 1, 11, 20, 18, 1, 4, 18, 1, 17, 18, 1, 20, 5, 1, 10, 11, 5, 7, 9, 2, 42, 41, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 42, 38, 4, 5, 1, 6, 5, 6, 1, 10, 5, 1, 23, 2, 6, 1, 7, 4, 12, 4, 7, 1, 11, 5, 15, 18, 1, 4, 18, 1, 17, 18, 1, 4, 6, 9, 7, 3, 4, 42, 9, 3, 6, 9, 18, 1, 2, 7, 3, 1, 16, 4, 2, 6, 1, 12, 4, 1, 10, 5, 6, 6, 5, 1, 3, 1, 22, 14, 2, 8, 1, 16, 14, 6, 9, 5, 42, 11, 20, 2, 1, 8, 3, 1, 17, 2, 7, 3, 11, 2, 1, 17, 4, 3, 1, 3, 23, 23, 3, 6, 12, 5, 6, 3, 4, 42, 41, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 42, 38, 15, 3, 1, 16, 5, 4, 1, 11, 20, 18, 1, 4, 18, 1, 21, 14, 4, 1, 3, 8, 1, 16, 4, 28, 1, 12, 18, 1, 14, 6, 1, 11, 5, 8, 8, 2, 1, 19, 4, 14, 6, 9, 5, 42, 8, 30, 1, 12, 5, 17, 2, 1, 9, 2, 7, 15, 4, 6, 3, 17, 3, 1, 22, 14, 2, 8, 8, 3, 1, 17, 3, 8, 8, 2, 42, 11, 20, 2, 1, 15, 18, 1, 3, 17, 2, 3, 1, 12, 4, 1, 16, 3, 14, 7, 3, 1, 4, 8, 1, 11, 5, 7, 1, 11, 5, 15, 16, 14, 6, 9, 5, 42, 41, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 42, 38, 19, 14, 3, 7, 12, 3, 4, 1, 4, 6, 1, 3, 8, 9, 5, 1, 2, 1, 17, 4, 12, 4, 1, 8, 2, 1, 10, 14, 2, 1, 10, 16, 3, 8, 8, 2, 42, 17, 2, 10, 9, 4, 9, 2, 1, 19, 4, 30, 1, 12, 2, 18, 1, 7, 3, 19, 19, 4, 1, 12, 2, 8, 1, 16, 4, 3, 6, 2, 9, 3, 42, 11, 20, 2, 1, 15, 2, 6, 3, 1, 12, 7, 4, 9, 9, 5, 1, 3, 8, 9, 7, 14, 4, 1, 16, 2, 7, 1, 5, 19, 6, 2, 1, 11, 3, 8, 8, 2, 42, 41, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 42, 38, 3, 8, 8, 5, 7, 1, 21, 14, 1, 8, 3, 1, 16, 3, 14, 7, 3, 1, 14, 6, 1, 16, 5, 11, 5, 1, 22, 14, 2, 9, 3, 42, 11, 20, 2, 1, 6, 2, 8, 1, 8, 3, 19, 5, 1, 12, 2, 8, 1, 11, 5, 7, 1, 15, 18, 1, 2, 7, 3, 1, 12, 14, 7, 3, 9, 3, 42, 8, 3, 1, 6, 5, 9, 9, 2, 1, 11, 20, 18, 1, 4, 18, 1, 16, 3, 10, 10, 3, 4, 1, 11, 5, 6, 1, 9, 3, 6, 9, 3, 1, 16, 4, 2, 9, 3, 42, 41, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 42, 38, 2, 1, 11, 5, 15, 2, 1, 22, 14, 2, 4, 1, 11, 20, 2, 1, 11, 5, 6, 1, 8, 2, 6, 3, 1, 3, 21, 21, 3, 6, 6, 3, 9, 3, 42, 14, 10, 11, 4, 9, 5, 1, 21, 14, 5, 7, 1, 12, 2, 8, 1, 16, 2, 8, 3, 19, 5, 1, 3, 1, 8, 3, 1, 7, 4, 17, 3, 42, 10, 4, 1, 17, 5, 8, 19, 2, 1, 3, 1, 8, 18, 1, 3, 11, 22, 14, 3, 1, 16, 2, 7, 4, 19, 8, 4, 5, 10, 3, 1, 2, 1, 19, 14, 3, 9, 3, 42, 41, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 42]\n",
            "1608\n",
            "1608\n",
            "1608\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_6n273Y1I18",
        "outputId": "ea7ba226-777c-421e-b868-6ade5a04ccbb"
      },
      "source": [
        "poetry_sy_lm_dataset.vocabulary.dictionary"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 1,\n",
              " '!': 32,\n",
              " '<EOS>': 41,\n",
              " '<EOV>': 42,\n",
              " '<GO>': 38,\n",
              " '<PAD>': 39,\n",
              " '<SEP>': 40,\n",
              " '<UNK>': 0,\n",
              " '?': 31,\n",
              " 'a': 3,\n",
              " 'b': 23,\n",
              " 'c': 11,\n",
              " 'd': 12,\n",
              " 'e': 2,\n",
              " 'f': 21,\n",
              " 'g': 19,\n",
              " 'h': 20,\n",
              " 'i': 4,\n",
              " 'j': 36,\n",
              " 'l': 8,\n",
              " 'm': 15,\n",
              " 'n': 6,\n",
              " 'o': 5,\n",
              " 'p': 16,\n",
              " 'q': 22,\n",
              " 'r': 7,\n",
              " 's': 10,\n",
              " 't': 9,\n",
              " 'u': 14,\n",
              " 'v': 17,\n",
              " 'x': 35,\n",
              " 'y': 37,\n",
              " 'z': 24,\n",
              " 'à': 30,\n",
              " 'è': 28,\n",
              " 'é': 29,\n",
              " 'ì': 25,\n",
              " 'ò': 27,\n",
              " 'ó': 33,\n",
              " 'ù': 26,\n",
              " '‘': 34,\n",
              " '’': 18}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xURM0fNiOIKj",
        "outputId": "cf03779b-a6a1-446b-8ab7-66bb201c7700"
      },
      "source": [
        "print(seq2str(batches[14][0]))\n",
        "print(seq2str(batches[15][0]))\n",
        "print(seq2str(batches[16][0]))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " vidi e conobbi l’ ombra di colui\n",
            "che fece per viltade il gran rifiuto\n",
            "incontanente intesi e certo fui\n",
            "                                                                                                 \n",
            " che questa era la setta d’ i cattivi\n",
            "a dio spiacenti e a’ nemici sui\n",
            "questi sciaurati che mai non fur vivi\n",
            "                                                                                            \n",
            " erano ignudi e stimolati molto\n",
            "da mosconi e da vespe ch’ eran ivi\n",
            "elle rigavan lor di sangue il volto\n",
            "                                                                                                 \n",
            " che mischiato di lagrime a’ lor piedi\n",
            "da fastidiosi vermi era ricolto\n",
            "e poi ch’ a riguardar oltre mi diedi\n",
            "                                                                                            \n",
            " vidi genti a la riva d’ un gran fiume\n",
            "per ch’ io dissi maestro or mi concedi\n",
            "ch’ i’ sappia quali sono e qual costume\n",
            "                                                                                  \n",
            " le fa di trapassar parer sì pronte\n",
            "com’ i’ discerno per lo fioco lume\n",
            "ed elli a me le cose ti fier conte\n",
            "                                                                                              \n",
            " quando noi fermerem li nostri passi\n",
            "su la trista riviera d’ acheronte\n",
            "allor con li occhi vergognosi e bassi\n",
            "                                                                                           \n",
            " temendo no ’l mio dir li fosse grave\n",
            "infino al fiume del parlar mi trassi\n",
            "ed ecco verso noi venir per nave\n",
            "                                                                                            \n",
            "\n",
            " un vecchio bianco per antico pelo\n",
            "gridando guai a voi anime prave!\n",
            "non isperate mai veder lo cielo\n",
            "                                                                                                    \n",
            " i’ vegno per menarvi a l’ altra riva\n",
            "ne le tenebre etterne in caldo e ’n gelo\n",
            "e tu che se’ costì anima viva\n",
            "                                                                                           \n",
            " pàrtiti da cotesti che son morti\n",
            "ma poi che vide ch’ io non mi partiva\n",
            "disse per altra via per altri porti\n",
            "                                                                                            \n",
            " verrai a piaggia non qui per passare\n",
            "più lieve legno convien che ti porti\n",
            "e ’l duca lui caron non ti crucciare\n",
            "                                                                                        \n",
            " vuolsi così colà dove si puote\n",
            "ciò che si vuole e più non dimandare\n",
            "quinci fuor quete le lanose gote\n",
            "                                                                                                  \n",
            " al nocchier de la livida palude\n",
            "che ’ntorno a li occhi avea di fiamme rote\n",
            "ma quell’ anime ch’ eran lasse e nude\n",
            "                                                                                      \n",
            " cangiar colore e dibattero i denti\n",
            "ratto che ’nteser le parole crude\n",
            "bestemmiavano dio e lor parenti\n",
            "                                                                                                  \n",
            " l’ umana spezie e ’l loco e ’l tempo e ’l seme\n",
            "di lor semenza e di lor nascimenti\n",
            "poi si ritrasser tutte quante insieme\n",
            "                                                                               \n",
            "\n",
            " forte piangendo a la riva malvagia\n",
            "ch’ attende ciascun uom che dio non teme\n",
            "caron dimonio con occhi di bragia\n",
            "                                                                                         \n",
            " loro accennando tutte le raccoglie\n",
            "batte col remo qualunque s’ adagia\n",
            "come d’ autunno si levan le foglie\n",
            "                                                                                              \n",
            " l’ una appresso de l’ altra fin che ’l ramo\n",
            "vede a la terra tutte le sue spoglie\n",
            "similemente il mal seme d’ adamo\n",
            "                                                                                     \n",
            " gittansi di quel lito ad una ad una\n",
            "per cenni come augel per suo richiamo\n",
            "così sen vanno su per l’ onda bruna\n",
            "                                                                                         \n",
            " e avanti che sien di là discese\n",
            "anche di qua nuova schiera s’ auna\n",
            "figliuol mio disse ’l maestro cortese\n",
            "                                                                                              \n",
            " quelli che muoion ne l’ ira di dio\n",
            "tutti convegnon qui d’ ogne paese\n",
            "e pronti sono a trapassar lo rio\n",
            "                                                                                                 \n",
            " ché la divina giustizia li sprona\n",
            "sì che la tema si volve in disio\n",
            "quinci non passa mai anima buona\n",
            "                                                                                                   \n",
            " e però se caron di te si lagna\n",
            "ben puoi sapere omai che ’l suo dir suona\n",
            "finito questo la buia campagna\n",
            "                                                                                               \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmSJj8BMqVY_"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICqI4vK-pC4k"
      },
      "source": [
        "wandb.config.num_layers = 4\n",
        "wandb.config.d_model = 128\n",
        "wandb.config.dff = 256\n",
        "wandb.config.num_heads = 4\n",
        "wandb.config.dropout = 0.1\n",
        "wandb.config.learning_rate = 2e-4 \n",
        "\n",
        "generate_at = [] #[10,20,30,40,50,60,70,80,90,100,110,120,130,140,150]\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "def create_padding_mask(seq):   \n",
        "    seq = tf.cast(tf.math.equal(seq, pad), tf.float32)\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "    \n",
        "    Args:\n",
        "        q: query shape == (..., seq_len_q, depth)\n",
        "        k: key shape == (..., seq_len_k, depth)\n",
        "        v: value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "        \n",
        "    Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    return output, attention_weights\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % self.num_heads == 0\n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "            \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        \n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "        \n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        return output, attention_weights\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def __call__(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        return out2\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "    \n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "        \n",
        "    def __call__(self, x, enc_output, training, \n",
        "            look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "        \n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "        \n",
        "        ffn_output = self.ffn(out2)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "        \n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.d_model)\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                        for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "            \n",
        "    def __call__(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "        return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                        for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def __call__(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                look_ahead_mask, padding_mask)\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "        \n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "                target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                            input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                            target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "        \n",
        "    def __call__(self, inp, tar, training, enc_padding_mask, \n",
        "            look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "        \n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "        \n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "        \n",
        "        return final_output, attention_weights\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(wandb.config.learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "val_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "input_vocab_size = vocab_size\n",
        "target_vocab_size = vocab_size\n",
        "max_len = batch_len\n",
        "\n",
        "transformer = Transformer(wandb.config.num_layers, wandb.config.d_model, \n",
        "                          wandb.config.num_heads, wandb.config.dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=max_len, \n",
        "                          pe_target=max_len,\n",
        "                          rate=wandb.config.dropout)\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')\n",
        "\n",
        "@tf.function()\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                    True, \n",
        "                                    enc_padding_mask, \n",
        "                                    combined_mask, \n",
        "                                    dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "    \n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)\n",
        "\n",
        "@tf.function()\n",
        "def val_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    \n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                False, \n",
        "                                enc_padding_mask, \n",
        "                                combined_mask, \n",
        "                                dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "    \n",
        "    val_loss(loss)\n",
        "    val_accuracy(tar_real, predictions)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaxwE-zBpx4S"
      },
      "source": [
        "#@title Generation\n",
        "def generate(index=0, k=1, t=1):\n",
        "\n",
        "    def evaluate_greedy(inp_sentence, decoder_input):\n",
        "        inp_sentence = inp_sentence\n",
        "        encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "        \n",
        "        output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "        terces = 0\n",
        "        for i in range(batch_len):\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "        \n",
        "            # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "            predictions, attention_weights = transformer(encoder_input, \n",
        "                                                        output,\n",
        "                                                        False,\n",
        "                                                        enc_padding_mask,\n",
        "                                                        combined_mask,\n",
        "                                                        dec_padding_mask)\n",
        "            \n",
        "            # select the last word from the seq_len dimension\n",
        "            predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "            predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "            # return the result if the predicted_id is equal to the end token\n",
        "            if predicted_id == eos:\n",
        "                terces += 1\n",
        "                if terces == terces_per_batch-1:\n",
        "                    return tf.squeeze(output, axis=0), attention_weights\n",
        "            # concatentate the predicted_id to the output which is given to the decoder\n",
        "            # as its input.\n",
        "            output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "        return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "\n",
        "    def evaluate_topk(inp_sentence, decoder_input, k=5, temperature=0.5):\n",
        "        inp_sentence = inp_sentence\n",
        "        encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "        \n",
        "        output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "        def scale(tensor):\n",
        "            tensor = tf.math.divide(\n",
        "                tf.subtract(\n",
        "                    tensor, \n",
        "                    tf.reduce_min(tensor)\n",
        "                ), \n",
        "                tf.subtract(\n",
        "                    tf.reduce_max(tensor), \n",
        "                    tf.reduce_min(tensor))\n",
        "                )\n",
        "            return tensor\n",
        "\n",
        "        terces = 0\n",
        "        for i in range(batch_len):\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "        \n",
        "            # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "            predictions, attention_weights = transformer(encoder_input, \n",
        "                                                        output,\n",
        "                                                        False,\n",
        "                                                        enc_padding_mask,\n",
        "                                                        combined_mask,\n",
        "                                                        dec_padding_mask)\n",
        "            # select the last word from the seq_len dimension\n",
        "            predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "            predictions, indices = tf.math.top_k(predictions,k=k)\n",
        "            predictions /= temperature\n",
        "            #predictions = scale(predictions)\n",
        "            predictions = np.squeeze(predictions, axis=0)\n",
        "            indices = np.squeeze(indices, axis=0)\n",
        "            indices = np.squeeze(indices, axis=0)\n",
        "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "            predicted_id = indices[predicted_id]\n",
        "\n",
        "            # return the result if the predicted_id is equal to the end token\n",
        "            if predicted_id == eos:\n",
        "                terces += 1\n",
        "                if terces == terces_per_batch-1:\n",
        "                    return tf.squeeze(output, axis=0), attention_weights\n",
        "            # concatentate the predicted_id to the output which is given to the decoder\n",
        "            # as its input.\n",
        "            predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "            predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "            output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "        return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "    out_list = test_b[index][0]\n",
        "    offset = terces_len # a tercet\n",
        "    txt_gen = seq2str(out_list[-offset:])\n",
        "\n",
        "    #print(\"params: k={}, t={}\".format(k,t))\n",
        "    for i in range(32//(terces_per_batch-1)): # 30 terces = cantica\n",
        "        out, att_w = evaluate_topk([pad], out_list[-offset:], k, t)\n",
        "        out_list = out.numpy().tolist()\n",
        "        out_str = seq2str(out_list[offset:])\n",
        "        txt_gen += out_str\n",
        "\n",
        "    #print(txt_gen)\n",
        "    wandb.log({\"generated\":\n",
        "            wandb.Html(\"k=\"+str(k)+\" t=\"+str(t)+\n",
        "                       \"<pre>\"+txt_gen+\"</pre>\", inject=False)})\n",
        "    return(txt_gen)\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv5bxDVqR0zv"
      },
      "source": [
        "# Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-izCTWvp23H",
        "outputId": "c1dfa525-3c98-42b2-8475-9d25567422d0"
      },
      "source": [
        "#@title Train loop\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "    random.shuffle(batches)\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (inp, tar)) in enumerate(batches):\n",
        "        if (len(inp) != batch_len or len(tar) != batch_len):\n",
        "            print(\"discarded batch\", batch)\n",
        "            continue\n",
        "        train_step(np.expand_dims(inp, axis=0), np.expand_dims(tar, axis=0))\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "        \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, train_loss.result(), train_accuracy.result()))\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss.result(),\n",
        "        'train_accuracy': train_accuracy.result()\n",
        "    }, step=epoch+1)\n",
        "\n",
        "    # validation\n",
        "    if epoch % 5 == 0:\n",
        "        loss_l, acc_l = [], []\n",
        "        for (batch, (inp, tar)) in enumerate(val_b):\n",
        "            val_loss.reset_states()\n",
        "            val_accuracy.reset_states()\n",
        "            \n",
        "            if (len(inp) != batch_len or len(tar) != batch_len):\n",
        "                print(\"discarded batch\", batch)\n",
        "                continue\n",
        "\n",
        "            val_step(np.expand_dims(inp, axis=0), np.expand_dims(tar, axis=0))\n",
        "\n",
        "            loss_l.append(val_loss.result())\n",
        "            acc_l.append(val_accuracy.result())\n",
        "\n",
        "        loss_mean = sum(loss_l)/len(loss_l)\n",
        "        acc_mean = sum(acc_l)/len(acc_l)\n",
        "        print('Epoch {} VALIDATION: Loss {:.4f} Accuracy {:.4f}\\n'.format(epoch + 1, loss_mean, acc_mean))\n",
        "\n",
        "        wandb.log({\n",
        "            'val_loss': loss_mean,\n",
        "            'val_accuracy': acc_mean\n",
        "        }, step=epoch+1)\n",
        "\n",
        "    # generation\n",
        "    if epoch in generate_at:\n",
        "        generate(1)\n",
        "    \n",
        "    print(\"epoch lasted: {}\".format(time.time()-start_time))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 7.4697 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 5.1911 Accuracy 0.4365\n",
            "Epoch 1 Batch 100 Loss 4.2461 Accuracy 0.4422\n",
            "Epoch 1 Batch 150 Loss 3.5756 Accuracy 0.4516\n",
            "Epoch 1 Batch 200 Loss 3.1412 Accuracy 0.4760\n",
            "Epoch 1 Batch 250 Loss 2.8377 Accuracy 0.4962\n",
            "Epoch 1 Batch 300 Loss 2.6155 Accuracy 0.5103\n",
            "Epoch 1 Batch 350 Loss 2.4476 Accuracy 0.5211\n",
            "discarded batch 354\n",
            "Epoch 1 Batch 400 Loss 2.3187 Accuracy 0.5298\n",
            "Epoch 1 Batch 450 Loss 2.2111 Accuracy 0.5373\n",
            "Epoch 1 Batch 500 Loss 2.1225 Accuracy 0.5433\n",
            "Epoch 1 Batch 550 Loss 2.0480 Accuracy 0.5485\n",
            "Epoch 1 Loss 2.0095 Accuracy 0.5512\n",
            "Time taken for 1 epoch: 127.01142835617065 secs\n",
            "\n",
            "discarded batch 5\n",
            "Epoch 1 VALIDATION: Loss 1.2280 Accuracy 0.6180\n",
            "\n",
            "epoch lasted: 129.5223937034607\n",
            "Epoch 2 Batch 0 Loss 1.2631 Accuracy 0.5999\n",
            "Epoch 2 Batch 50 Loss 1.2690 Accuracy 0.6047\n",
            "discarded batch 72\n",
            "Epoch 2 Batch 100 Loss 1.2682 Accuracy 0.6045\n",
            "Epoch 2 Batch 150 Loss 1.2634 Accuracy 0.6053\n",
            "Epoch 2 Batch 200 Loss 1.2604 Accuracy 0.6056\n",
            "Epoch 2 Batch 250 Loss 1.2592 Accuracy 0.6055\n",
            "Epoch 2 Batch 300 Loss 1.2566 Accuracy 0.6056\n",
            "Epoch 2 Batch 350 Loss 1.2560 Accuracy 0.6054\n",
            "Epoch 2 Batch 400 Loss 1.2541 Accuracy 0.6057\n",
            "Epoch 2 Batch 450 Loss 1.2525 Accuracy 0.6057\n",
            "Epoch 2 Batch 500 Loss 1.2499 Accuracy 0.6061\n",
            "Epoch 2 Batch 550 Loss 1.2485 Accuracy 0.6062\n",
            "Epoch 2 Loss 1.2472 Accuracy 0.6065\n",
            "Time taken for 1 epoch: 112.62095999717712 secs\n",
            "\n",
            "epoch lasted: 112.62536287307739\n",
            "Epoch 3 Batch 0 Loss 1.2130 Accuracy 0.6179\n",
            "Epoch 3 Batch 50 Loss 1.2299 Accuracy 0.6079\n",
            "Epoch 3 Batch 100 Loss 1.2289 Accuracy 0.6090\n",
            "Epoch 3 Batch 150 Loss 1.2275 Accuracy 0.6101\n",
            "Epoch 3 Batch 200 Loss 1.2264 Accuracy 0.6099\n",
            "Epoch 3 Batch 250 Loss 1.2276 Accuracy 0.6094\n",
            "Epoch 3 Batch 300 Loss 1.2275 Accuracy 0.6091\n",
            "Epoch 3 Batch 350 Loss 1.2266 Accuracy 0.6092\n",
            "Epoch 3 Batch 400 Loss 1.2244 Accuracy 0.6097\n",
            "Epoch 3 Batch 450 Loss 1.2238 Accuracy 0.6098\n",
            "discarded batch 483\n",
            "Epoch 3 Batch 500 Loss 1.2235 Accuracy 0.6097\n",
            "Epoch 3 Batch 550 Loss 1.2227 Accuracy 0.6099\n",
            "Epoch 3 Loss 1.2226 Accuracy 0.6101\n",
            "Time taken for 1 epoch: 112.5944664478302 secs\n",
            "\n",
            "epoch lasted: 112.59891605377197\n",
            "Epoch 4 Batch 0 Loss 1.1914 Accuracy 0.6167\n",
            "Epoch 4 Batch 50 Loss 1.2201 Accuracy 0.6102\n",
            "Epoch 4 Batch 100 Loss 1.2154 Accuracy 0.6117\n",
            "Epoch 4 Batch 150 Loss 1.2152 Accuracy 0.6119\n",
            "Epoch 4 Batch 200 Loss 1.2135 Accuracy 0.6124\n",
            "Epoch 4 Batch 250 Loss 1.2133 Accuracy 0.6123\n",
            "discarded batch 258\n",
            "Epoch 4 Batch 300 Loss 1.2136 Accuracy 0.6122\n",
            "Epoch 4 Batch 350 Loss 1.2137 Accuracy 0.6124\n",
            "Epoch 4 Batch 400 Loss 1.2142 Accuracy 0.6122\n",
            "Epoch 4 Batch 450 Loss 1.2134 Accuracy 0.6124\n",
            "Epoch 4 Batch 500 Loss 1.2136 Accuracy 0.6123\n",
            "Epoch 4 Batch 550 Loss 1.2136 Accuracy 0.6124\n",
            "Epoch 4 Loss 1.2133 Accuracy 0.6125\n",
            "Time taken for 1 epoch: 112.55072283744812 secs\n",
            "\n",
            "epoch lasted: 112.55543541908264\n",
            "Epoch 5 Batch 0 Loss 1.2105 Accuracy 0.6067\n",
            "Epoch 5 Batch 50 Loss 1.2019 Accuracy 0.6144\n",
            "Epoch 5 Batch 100 Loss 1.2065 Accuracy 0.6132\n",
            "discarded batch 129\n",
            "Epoch 5 Batch 150 Loss 1.2069 Accuracy 0.6137\n",
            "Epoch 5 Batch 200 Loss 1.2078 Accuracy 0.6137\n",
            "Epoch 5 Batch 250 Loss 1.2065 Accuracy 0.6139\n",
            "Epoch 5 Batch 300 Loss 1.2077 Accuracy 0.6135\n",
            "Epoch 5 Batch 350 Loss 1.2068 Accuracy 0.6138\n",
            "Epoch 5 Batch 400 Loss 1.2065 Accuracy 0.6141\n",
            "Epoch 5 Batch 450 Loss 1.2071 Accuracy 0.6141\n",
            "Epoch 5 Batch 500 Loss 1.2065 Accuracy 0.6144\n",
            "Epoch 5 Batch 550 Loss 1.2070 Accuracy 0.6145\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 1.2069 Accuracy 0.6145\n",
            "Time taken for 1 epoch: 112.76589918136597 secs\n",
            "\n",
            "epoch lasted: 112.76932716369629\n",
            "Epoch 6 Batch 0 Loss 1.2142 Accuracy 0.6229\n",
            "Epoch 6 Batch 50 Loss 1.2001 Accuracy 0.6169\n",
            "Epoch 6 Batch 100 Loss 1.2008 Accuracy 0.6161\n",
            "Epoch 6 Batch 150 Loss 1.2016 Accuracy 0.6160\n",
            "Epoch 6 Batch 200 Loss 1.2030 Accuracy 0.6158\n",
            "discarded batch 213\n",
            "Epoch 6 Batch 250 Loss 1.2028 Accuracy 0.6159\n",
            "Epoch 6 Batch 300 Loss 1.2024 Accuracy 0.6159\n",
            "Epoch 6 Batch 350 Loss 1.2014 Accuracy 0.6161\n",
            "Epoch 6 Batch 400 Loss 1.2024 Accuracy 0.6158\n",
            "Epoch 6 Batch 450 Loss 1.2015 Accuracy 0.6161\n",
            "Epoch 6 Batch 500 Loss 1.2009 Accuracy 0.6165\n",
            "Epoch 6 Batch 550 Loss 1.2008 Accuracy 0.6166\n",
            "Epoch 6 Loss 1.2012 Accuracy 0.6166\n",
            "Time taken for 1 epoch: 112.34393429756165 secs\n",
            "\n",
            "discarded batch 5\n",
            "Epoch 6 VALIDATION: Loss 1.1591 Accuracy 0.6286\n",
            "\n",
            "epoch lasted: 112.72737050056458\n",
            "Epoch 7 Batch 0 Loss 1.2081 Accuracy 0.6154\n",
            "Epoch 7 Batch 50 Loss 1.1933 Accuracy 0.6174\n",
            "Epoch 7 Batch 100 Loss 1.1971 Accuracy 0.6166\n",
            "discarded batch 147\n",
            "Epoch 7 Batch 150 Loss 1.1968 Accuracy 0.6172\n",
            "Epoch 7 Batch 200 Loss 1.1976 Accuracy 0.6176\n",
            "Epoch 7 Batch 250 Loss 1.1985 Accuracy 0.6176\n",
            "Epoch 7 Batch 300 Loss 1.1977 Accuracy 0.6179\n",
            "Epoch 7 Batch 350 Loss 1.1970 Accuracy 0.6181\n",
            "Epoch 7 Batch 400 Loss 1.1968 Accuracy 0.6181\n",
            "Epoch 7 Batch 450 Loss 1.1964 Accuracy 0.6183\n",
            "Epoch 7 Batch 500 Loss 1.1964 Accuracy 0.6183\n",
            "Epoch 7 Batch 550 Loss 1.1964 Accuracy 0.6183\n",
            "Epoch 7 Loss 1.1959 Accuracy 0.6183\n",
            "Time taken for 1 epoch: 112.44132041931152 secs\n",
            "\n",
            "epoch lasted: 112.44588208198547\n",
            "Epoch 8 Batch 0 Loss 1.1763 Accuracy 0.6192\n",
            "Epoch 8 Batch 50 Loss 1.1916 Accuracy 0.6182\n",
            "Epoch 8 Batch 100 Loss 1.1938 Accuracy 0.6180\n",
            "Epoch 8 Batch 150 Loss 1.1904 Accuracy 0.6192\n",
            "Epoch 8 Batch 200 Loss 1.1911 Accuracy 0.6193\n",
            "Epoch 8 Batch 250 Loss 1.1920 Accuracy 0.6194\n",
            "Epoch 8 Batch 300 Loss 1.1926 Accuracy 0.6193\n",
            "Epoch 8 Batch 350 Loss 1.1923 Accuracy 0.6197\n",
            "Epoch 8 Batch 400 Loss 1.1918 Accuracy 0.6198\n",
            "discarded batch 426\n",
            "Epoch 8 Batch 450 Loss 1.1910 Accuracy 0.6201\n",
            "Epoch 8 Batch 500 Loss 1.1922 Accuracy 0.6197\n",
            "Epoch 8 Batch 550 Loss 1.1914 Accuracy 0.6200\n",
            "Epoch 8 Loss 1.1914 Accuracy 0.6201\n",
            "Time taken for 1 epoch: 112.62999200820923 secs\n",
            "\n",
            "epoch lasted: 112.63464498519897\n",
            "Epoch 9 Batch 0 Loss 1.2373 Accuracy 0.5968\n",
            "Epoch 9 Batch 50 Loss 1.1904 Accuracy 0.6212\n",
            "Epoch 9 Batch 100 Loss 1.1894 Accuracy 0.6212\n",
            "Epoch 9 Batch 150 Loss 1.1890 Accuracy 0.6219\n",
            "Epoch 9 Batch 200 Loss 1.1877 Accuracy 0.6219\n",
            "Epoch 9 Batch 250 Loss 1.1879 Accuracy 0.6217\n",
            "discarded batch 294\n",
            "Epoch 9 Batch 300 Loss 1.1886 Accuracy 0.6215\n",
            "Epoch 9 Batch 350 Loss 1.1886 Accuracy 0.6214\n",
            "Epoch 9 Batch 400 Loss 1.1880 Accuracy 0.6214\n",
            "Epoch 9 Batch 450 Loss 1.1877 Accuracy 0.6214\n",
            "Epoch 9 Batch 500 Loss 1.1869 Accuracy 0.6216\n",
            "Epoch 9 Batch 550 Loss 1.1868 Accuracy 0.6216\n",
            "Epoch 9 Loss 1.1869 Accuracy 0.6216\n",
            "Time taken for 1 epoch: 112.5996401309967 secs\n",
            "\n",
            "epoch lasted: 112.60430240631104\n",
            "Epoch 10 Batch 0 Loss 1.1615 Accuracy 0.6329\n",
            "discarded batch 2\n",
            "Epoch 10 Batch 50 Loss 1.1852 Accuracy 0.6232\n",
            "Epoch 10 Batch 100 Loss 1.1853 Accuracy 0.6225\n",
            "Epoch 10 Batch 150 Loss 1.1824 Accuracy 0.6232\n",
            "Epoch 10 Batch 200 Loss 1.1822 Accuracy 0.6229\n",
            "Epoch 10 Batch 250 Loss 1.1838 Accuracy 0.6223\n",
            "Epoch 10 Batch 300 Loss 1.1836 Accuracy 0.6224\n",
            "Epoch 10 Batch 350 Loss 1.1830 Accuracy 0.6226\n",
            "Epoch 10 Batch 400 Loss 1.1833 Accuracy 0.6223\n",
            "Epoch 10 Batch 450 Loss 1.1831 Accuracy 0.6224\n",
            "Epoch 10 Batch 500 Loss 1.1828 Accuracy 0.6225\n",
            "Epoch 10 Batch 550 Loss 1.1827 Accuracy 0.6226\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 1.1826 Accuracy 0.6225\n",
            "Time taken for 1 epoch: 113.25300741195679 secs\n",
            "\n",
            "epoch lasted: 113.25674414634705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "aHHI5hivp5T7",
        "outputId": "bd691d56-5b81-4c10-de18-b0b861288ba0"
      },
      "source": [
        "#@title Parameter persistence\n",
        "\n",
        "transformer.save_weights(\"./optimus_rhyme\")\n",
        "#transformer.load_weights(\"./optimus_rhyme\")\n",
        "\n",
        "\n",
        "#emb_enc_w = transformer.encoder.embedding.get_weights()[0]\n",
        "emb_enc_w = transformer.decoder.embedding.get_weights()[0]\n",
        "print(emb_enc_w.shape)\n",
        "\n",
        "out_v = open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = open('meta.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for num, word in enumerate(poetry_sy_lm_dataset.vocabulary.dictionary):\n",
        "  vec = emb_enc_w[num] # skip 0, it's padding.\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "\n",
        "\n",
        "'''\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir .\n",
        "'''"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1900, 128)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n%load_ext tensorboard\\n%tensorboard --logdir .\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diK06zwBW6v9"
      },
      "source": [
        "# Syllabize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ25Qvc8WHCk"
      },
      "source": [
        "# Import syllabifier\n",
        "\n",
        "model_name = \"Syllabifier\"\n",
        "tf.keras.utils.get_file(\n",
        "    f\"{model_name}.zip\",\n",
        "    f\"{model_name}.zip\",\n",
        "    cache_dir='.', cache_subdir='', extract=True\n",
        ")\n",
        "syllabifier = tf.saved_model.load('Syllabifier')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUAEkR39WHCm"
      },
      "source": [
        "def clean(txt):\n",
        "    txt = txt.decode('utf-8')\n",
        "    x = txt.replace(\"[START]\", \"\")\n",
        "    y = x.replace(\"[END]\", \"\")\n",
        "    return y"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbpE65_QWHCl",
        "outputId": "289ebb4b-2a27-4766-dd6e-d075238b231f"
      },
      "source": [
        "test_txt = \"nel mezzo del cammin de nostra vita\"\n",
        "print(clean(syllabifier(test_txt).numpy()))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | nel | mez | zo | del | cam | min | de | no | stra | vi | ta \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7_wygFNRwgf"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNfKbicYn1Fk"
      },
      "source": [
        "generated_text = generate(1, k = 3, t = 0.9)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "XLwioX-vsp6j",
        "outputId": "ead403b4-7643-4d3c-bb6d-51b0ce181cc8"
      },
      "source": [
        "generated_text"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'nel qual non si dee creder che s’ invii\\nper creatura l’ occhio tanto chiaro\\ne io ch’ al fine di tutt’ i disii\\n                                                                                         \\n co di l cionorin ch’ dor se ste cori\\nchio sto cheri che se co si se stori\\nche si che conor lino l stri co chen sto\\n                                                                                     \\n din chi che di co dere co chio de coro\\ncon chi ser cherenorer co li son se\\ne co si stun de ch’ co sto chi lino\\n                                                                                        \\n di ci sto chen dino l si si sto son co\\ndi di di si de ci dio stonteri pe\\nche l’ se do sto ste sen che li so\\n                                                                                           \\n e ch’ co der se di ch’ derer so stere\\ndere di che chino din che che stero\\ne di de ste co co lari ch’ chente se\\n                                                                                        \\n di si se di do co di di co che seri\\nce cor sì sì corino di di de ste\\nde co do cere se se de di l cor ce ce\\n                                                                                            \\n chere son con storino serion ch’ co\\nco ser co si co che core ser se di\\nchi co serio deri ch’ donor sere li ce\\n                                                                                         \\n e ch’ co de de ch’ che co cori le serore\\nper de se stre chi sto se ce ste\\nchere do dori do cono che di do cerororo\\n dere co si le le co dinonde l si se conto\\ndiomin se l ste co co se li ste li\\ncome co se den co leriontrore corore\\n                                                                                      \\n dindi di che che ch’ de ser do serinto\\neno chere dino cerenor che le chiono\\ndo che l di do dino sto sto l chio\\n                                                                                        \\n che der con sionono l cor ste chin che\\nco se den se ch’ de che sio che le l li serere din cherere con corore\\n                                                                                          \\n chion steno se sto cor sion li che dino\\nco de sto co con che di con coren l che\\nch’ de la si che l che stor l so ste\\n                                                                                  \\n e se str se ste ste di sen sto le\\nde che sì sì chen str chenono chin doreno\\nche co conor co ste le siontre se\\n                                                                                         \\n de che con de chi si donore che co\\nchinono con den seno de sio diori co\\nche di ch’ che che de co si con chi so\\n                                                                                        \\n co cori che de co con cono stor sto\\ne ce che stono co ch’ se stonte sto\\nco de co chere che do di din so che cero\\ne che ch’ len co l l ste se che li corin cerenor l che de do si sino coro cere\\nde sto cen de co l stroro l str centto\\n                                                                                   \\n din do le l co si dinor si di di che conte co siono str do ce co coro\\nco str do cor con di ste sto se li si si\\n                                                                                        \\n che si di stri di cor sto dono chen dor so\\ne di ce si str cher si si di corior che chi chi se sino l de co\\n                                                                                            \\n er che stono de se l ste deri che core\\ne de co din ce ce l cheri corio cheri se co\\nco la cor che cin sio ste cor l so\\n                                                                                 \\n e cin cor sere de di che ce ce cono\\ner di de ce stre sten cher sin ste\\nche che se sinon din si l cherio\\n                                                                                               \\n e l co che chi si co che storeno do\\nco di di ch’ si che chi din corer so\\ne l’ se si cori co se dor strio se ste\\n                                                                                       \\n ce sto che di di co stri co l cori sto\\ndi di ce che steri chen ston cor che cer der so dontreno\\npi der se l co ce ce che stono cor di ste\\ncone co se l chen conore cer serere\\nch’ che cor de ce corore co se sere\\n                                                                                       \\n co der chi che do chi de si donte\\ndi cono denene do se de che cer se se\\ndine de ster ste do l sto l le co sto\\n                                                                                         \\n e co si do sere chino cor se si ste\\nene che din con co ste de con stte\\nche chere ser ste co sere ste so stero\\n                                                                                         \\n co dono steno ste co chi di ste do\\nch’ cheri che seri str l co sterer sto\\ne de se sino che che lar ste cono co\\n                                                                                        \\n e di si se de che ste ste sto le\\ne conore dor de ceri se do do storo\\nche l che co de di co cor li se l core\\n                                                                                           \\n con de se do dine co sio ce di do\\ncheri che de ste ste di di de si si\\ncherin co co se sto che se se che l chi l co\\n                                                                                    \\n e de chi dorer l ch’ ce din l stenteno\\nche la co co siono str ste sto ce\\ncer con se de se che cher dere stro pi\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhy3-vhhsT6R",
        "outputId": "4f97a10f-36d7-4d32-a276-a52d2d86edea"
      },
      "source": [
        "print(generated_text)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nel qual non si dee creder che s’ invii\n",
            "per creatura l’ occhio tanto chiaro\n",
            "e io ch’ al fine di tutt’ i disii\n",
            "                                                                                         \n",
            " co di l cionorin ch’ dor se ste cori\n",
            "chio sto cheri che se co si se stori\n",
            "che si che conor lino l stri co chen sto\n",
            "                                                                                     \n",
            " din chi che di co dere co chio de coro\n",
            "con chi ser cherenorer co li son se\n",
            "e co si stun de ch’ co sto chi lino\n",
            "                                                                                        \n",
            " di ci sto chen dino l si si sto son co\n",
            "di di di si de ci dio stonteri pe\n",
            "che l’ se do sto ste sen che li so\n",
            "                                                                                           \n",
            " e ch’ co der se di ch’ derer so stere\n",
            "dere di che chino din che che stero\n",
            "e di de ste co co lari ch’ chente se\n",
            "                                                                                        \n",
            " di si se di do co di di co che seri\n",
            "ce cor sì sì corino di di de ste\n",
            "de co do cere se se de di l cor ce ce\n",
            "                                                                                            \n",
            " chere son con storino serion ch’ co\n",
            "co ser co si co che core ser se di\n",
            "chi co serio deri ch’ donor sere li ce\n",
            "                                                                                         \n",
            " e ch’ co de de ch’ che co cori le serore\n",
            "per de se stre chi sto se ce ste\n",
            "chere do dori do cono che di do cerororo\n",
            " dere co si le le co dinonde l si se conto\n",
            "diomin se l ste co co se li ste li\n",
            "come co se den co leriontrore corore\n",
            "                                                                                      \n",
            " dindi di che che ch’ de ser do serinto\n",
            "eno chere dino cerenor che le chiono\n",
            "do che l di do dino sto sto l chio\n",
            "                                                                                        \n",
            " che der con sionono l cor ste chin che\n",
            "co se den se ch’ de che sio che le l li serere din cherere con corore\n",
            "                                                                                          \n",
            " chion steno se sto cor sion li che dino\n",
            "co de sto co con che di con coren l che\n",
            "ch’ de la si che l che stor l so ste\n",
            "                                                                                  \n",
            " e se str se ste ste di sen sto le\n",
            "de che sì sì chen str chenono chin doreno\n",
            "che co conor co ste le siontre se\n",
            "                                                                                         \n",
            " de che con de chi si donore che co\n",
            "chinono con den seno de sio diori co\n",
            "che di ch’ che che de co si con chi so\n",
            "                                                                                        \n",
            " co cori che de co con cono stor sto\n",
            "e ce che stono co ch’ se stonte sto\n",
            "co de co chere che do di din so che cero\n",
            "e che ch’ len co l l ste se che li corin cerenor l che de do si sino coro cere\n",
            "de sto cen de co l stroro l str centto\n",
            "                                                                                   \n",
            " din do le l co si dinor si di di che conte co siono str do ce co coro\n",
            "co str do cor con di ste sto se li si si\n",
            "                                                                                        \n",
            " che si di stri di cor sto dono chen dor so\n",
            "e di ce si str cher si si di corior che chi chi se sino l de co\n",
            "                                                                                            \n",
            " er che stono de se l ste deri che core\n",
            "e de co din ce ce l cheri corio cheri se co\n",
            "co la cor che cin sio ste cor l so\n",
            "                                                                                 \n",
            " e cin cor sere de di che ce ce cono\n",
            "er di de ce stre sten cher sin ste\n",
            "che che se sinon din si l cherio\n",
            "                                                                                               \n",
            " e l co che chi si co che storeno do\n",
            "co di di ch’ si che chi din corer so\n",
            "e l’ se si cori co se dor strio se ste\n",
            "                                                                                       \n",
            " ce sto che di di co stri co l cori sto\n",
            "di di ce che steri chen ston cor che cer der so dontreno\n",
            "pi der se l co ce ce che stono cor di ste\n",
            "cone co se l chen conore cer serere\n",
            "ch’ che cor de ce corore co se sere\n",
            "                                                                                       \n",
            " co der chi che do chi de si donte\n",
            "di cono denene do se de che cer se se\n",
            "dine de ster ste do l sto l le co sto\n",
            "                                                                                         \n",
            " e co si do sere chino cor se si ste\n",
            "ene che din con co ste de con stte\n",
            "che chere ser ste co sere ste so stero\n",
            "                                                                                         \n",
            " co dono steno ste co chi di ste do\n",
            "ch’ cheri che seri str l co sterer sto\n",
            "e de se sino che che lar ste cono co\n",
            "                                                                                        \n",
            " e di si se de che ste ste sto le\n",
            "e conore dor de ceri se do do storo\n",
            "che l che co de di co cor li se l core\n",
            "                                                                                           \n",
            " con de se do dine co sio ce di do\n",
            "cheri che de ste ste di di de si si\n",
            "cherin co co se sto che se se che l chi l co\n",
            "                                                                                    \n",
            " e de chi dorer l ch’ ce din l stenteno\n",
            "che la co co siono str ste sto ce\n",
            "cer con se de se che cher dere stro pi\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "YQpP7oiF3iEE",
        "outputId": "ba4dbf07-8ac5-49f8-9457-af9f83379005"
      },
      "source": [
        "syllabifier(generated_text).numpy()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-60ee104fd2f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msyllabifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  required broadcastable shapes\n\t [[{{node StatefulPartitionedCall/while/body/_208/while/transformer_1/encoder_2/add}}]]\n  (1) Invalid argument:  required broadcastable shapes\n\t [[{{node StatefulPartitionedCall/while/body/_208/while/transformer_1/encoder_2/add}}]]\n\t [[StatefulPartitionedCall/StatefulPartitionedCall_2/StatefulPartitionedCall/RaggedSegmentJoin_1/RaggedSplitsToSegmentIds/Repeat/boolean_mask/GatherV2/_896]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_restored_function_body_100520]\n\nFunction call stack:\nrestored_function_body -> restored_function_body\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30K9u1Y2A7l8"
      },
      "source": [
        "#k=3\n",
        "generate(1, k = 3, t = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byt1GoaLA585"
      },
      "source": [
        "#k=5\n",
        "generate(1, k = 5, t = 1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}