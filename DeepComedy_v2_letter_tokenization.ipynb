{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepComedy_v2_letter_tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nrEohsVOSEg2"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAwbk6Au6JCU"
      },
      "source": [
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPzlEKUbPIIE",
        "outputId": "c54fb902-29d6-428a-8c73-8b4e339ac172"
      },
      "source": [
        "#imports\n",
        "#@title Import & seed\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import requests\n",
        "import collections\n",
        "import pickle\n",
        "import copy, random\n",
        "import nltk as nl\n",
        "nl.download('punkt')\n",
        "from itertools import zip_longest\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Reshape, BatchNormalization, Dense, Dropout, concatenate,\n",
        "    Embedding, LSTM, Dense, GRU, Bidirectional, Add\n",
        ")\n",
        "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "vocab_size = 1900\n",
        "terces_per_batch = 8\n",
        "terces_len = 200\n",
        "batch_len = terces_per_batch * (terces_len + 1)\n",
        "EPOCHS = 30\n",
        "\n",
        "\n",
        "#Setup\n",
        "print(tf.__version__)\n",
        "\n",
        "!nvidia-smi"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2.6.0\n",
            "Thu Sep 23 11:16:31 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P0    58W / 149W |   1192MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bkfS7t6vpYIJ",
        "outputId": "7af9ac72-fed6-435c-e8ef-10b8b3169ad3"
      },
      "source": [
        "# title Setup wandb\n",
        "!pip install wandb\n",
        "!wandb login e6569c556c797b5bed38bc6cba40d24b68e8973d\n",
        "import wandb\n",
        "wandb.init(project=\"DeepComedy\", name=\"bananito96\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.2-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 4.2 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.4.1-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 45.1 MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 45.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=6c0c07cc224a51bfb9dbd333665075ae4046c008ddfc46df2354c58d7f847c18\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=8dfcf92f0604b79228c5201ef3283e59ee9cf4e50dc8d4d49fe9a3ffee5b125b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.24 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.4.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.12.2 yaspin-2.1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdropino\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">bananito96</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/dropino/DeepComedy\" target=\"_blank\">https://wandb.ai/dropino/DeepComedy</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/dropino/DeepComedy/runs/1abj5sbm\" target=\"_blank\">https://wandb.ai/dropino/DeepComedy/runs/1abj5sbm</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210923_093317-1abj5sbm</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f326f0421d0>"
            ],
            "text/html": [
              "<h1>Run(1abj5sbm)</h1><iframe src=\"https://wandb.ai/dropino/DeepComedy/runs/1abj5sbm\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrEohsVOSEg2"
      },
      "source": [
        "#Clean files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CczzJREJ0-Y4"
      },
      "source": [
        "\"\"\"\n",
        "#count chars\n",
        "char_list = []\n",
        "with open(FILENAME) as file:\n",
        "  while True:\n",
        "    char = file.read(1)\n",
        "    if not char:\n",
        "      print(\"End of file\")\n",
        "      break\n",
        "    char = char.lower()\n",
        "\n",
        "    #add good char to char list\n",
        "    if char not in char_list:\n",
        "      char_list.append(char)\n",
        "\n",
        "print(char_list)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0tniPNkO1S4"
      },
      "source": [
        "\"\"\"\n",
        "removable_chars = ['•', '—', '-', '(', ')', ',', '.', ':', ';', '“', '”', '«', '»', '\"','0','1','2','3','4','5','6','7','8','9']\n",
        "#‘ e ’ usate per racchiudere parti parlate in latino\n",
        "#“ e ” racchiude il parlato sia in italiano che latino\n",
        "#idem « e »\n",
        "\n",
        "\n",
        "clean_char_list = []\n",
        "removedCharsCounter = 0\n",
        "fileOut = open(CLEANFILENAME, \"a\")\n",
        "\n",
        "with open(FILENAME) as file:\n",
        "  while True:\n",
        "    char = file.read(1)\n",
        "    if not char:\n",
        "      print(\"End of file\")\n",
        "      break\n",
        "\n",
        "    #transform upper case to lower case\n",
        "    char = char.lower()\n",
        "\n",
        "    #remove unwanted chars\n",
        "    if char not in removable_chars:\n",
        "\n",
        "      #add acceptable chars to char list\n",
        "      if char not in clean_char_list:\n",
        "        clean_char_list.append(char)\n",
        "      \n",
        "      #output in new file\n",
        "      fileOut.write(char)\n",
        "    else:\n",
        "      removedCharsCounter += 1\n",
        "\n",
        "\n",
        "print(char_list)\n",
        "print(removedCharsCounter)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxF6HYN5ANpF"
      },
      "source": [
        "\"\"\"\n",
        "f = open(CLEANFILENAME, \"r\")\n",
        "lines = f.readlines()\n",
        "\n",
        "number_of_syllables = []\n",
        "line_counter = 1\n",
        "\n",
        "for line in lines:\n",
        "  #excludes all empty lines and canto intros\n",
        "  if '|' in line:\n",
        "    syllables = line.count('|')\n",
        "    number_of_syllables.append(syllables)\n",
        "    if syllables != 11:\n",
        "      print(\"line number {} with text {} has {} syllables\".format(line_counter,line,syllables))\n",
        "    line_counter += 1\n",
        "\n",
        "Counter(number_of_syllables)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnAp1FL5LbYO"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "\"\"\"Once tokenized the dataset, a map of the syllables with the integer index is created. The final dimension\n",
        "of the vocabulary is 1874 tokens but it is limited to 1800 to remove the tail of infrequent syllables.\n",
        "\n",
        "We substituted every space between words with the special token < SEP > and inserted at the beginning\n",
        "of each verse the token < GO >. To make all verses the same lengths, we used the special character\n",
        "“< PAD >” to pad every terces to the length of 75 tokens. At the end of each verse we appended the\n",
        "symbol “< EOV > “, while at the end of each sentence “< EOS >”.\"\"\"\n",
        "\n",
        "number_of_syllables = []\n",
        "line_counter = 1\n",
        "\"\"\"\n",
        "space -> <SPA>\n",
        "verst start -> <VST>\n",
        "padding (up to 75) -> <PAD>\n",
        "end of verse -> <EOV>\n",
        "end of sentence -> <EOS>\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1mcovxWPadi",
        "outputId": "e14709e6-f157-4726-99bf-561b2fd57b48"
      },
      "source": [
        "def get_hyp_lm_tercets(tercets):\n",
        "    new_tercets = []\n",
        "    for tercet in tercets:\n",
        "        new_tercets.append([])\n",
        "        for verse in tercet:\n",
        "            new_tercets[-1].append([])\n",
        "            for hyp_w in verse:\n",
        "                new_tercets[-1][-1].extend(hyp_w)\n",
        "                new_tercets[-1][-1].append('<SEP>')\n",
        "            new_tercets[-1][-1] = new_tercets[-1][-1][:-1]\n",
        "\n",
        "    return new_tercets\n",
        "\n",
        "def hyphenation(verse):\n",
        "    \"\"\"\n",
        "    Split word in syllables\n",
        "    :param verse: input string\n",
        "    :return: a list containing syllables of the word\n",
        "    \"\"\"\n",
        "    syllables = []\n",
        "    syllable = \"\"\n",
        "\n",
        "    verse = verse[1:]\n",
        "    verse = verse+\"|\" \n",
        "\n",
        "    for letter in verse:\n",
        "        syllable += letter\n",
        "        if letter == '|':\n",
        "            syllables.append(syllable)\n",
        "            syllable = \"\"\n",
        "               \n",
        "    return syllables\n",
        "\n",
        "def get_dc_hyphenation(canti):\n",
        "    hyp_canti, hyp_tokens = [], []\n",
        "    for canto in canti:\n",
        "        hyp_canti.append([])\n",
        "        for verso in canto:\n",
        "            syllables = hyphenation(verso)\n",
        "            hyp_canti[-1].append(syllables)\n",
        "            for syllable in syllables:\n",
        "                hyp_tokens.extend(syllable)\n",
        "\n",
        "    return hyp_canti, hyp_tokens\n",
        "    \n",
        "\n",
        "def get_dc_cantos(filename, encoding=None):\n",
        "    cantos = []\n",
        "    cantoCounter =  0\n",
        "    cantos.append([])\n",
        "    with open(filename, \"r\", encoding=encoding) as f:\n",
        "        for line in f:\n",
        "            sentence = line.strip()\n",
        "            if 'canto' not in sentence and len(sentence) > 2:\n",
        "                if len(sentence) > 2:\n",
        "                    cantos[cantoCounter].append(sentence)\n",
        "    return cantos\n",
        "\n",
        "\n",
        "def create_tercets(cantos):\n",
        "    tercets = []\n",
        "    for i,canto in enumerate(cantos):\n",
        "        for v,verse in enumerate(canto):\n",
        "            if v%3 == 0:\n",
        "                tercets.append([])\n",
        "\n",
        "            tercets[-1].append(verse)\n",
        "        tercets = tercets[:-1]  # removes the last malformed tercets (only 2 verses)\n",
        "\n",
        "    return tercets\n",
        "\n",
        "\n",
        "def pad_list(l, pad_token, max_l_size, keep_lasts=False, pad_right=True):\n",
        "    \"\"\"\n",
        "    Adds a padding token to a list\n",
        "    inputs:\n",
        "    :param l: input list to pad.\n",
        "    :param pad_token: value to add as padding.\n",
        "    :param max_l_size: length of the new padded list to return,\n",
        "    it truncates lists longer that 'max_l_size' without adding\n",
        "    padding values.\n",
        "    :param keep_lasts: If True, preserves the max_l_size last elements\n",
        "    of a sequence (by keeping the same order).  E.g.:\n",
        "    if keep_lasts is True and max_l_size=3 [1,2,3,4] becomes [2,3,4].\n",
        "\n",
        "\n",
        "    :return: the list padded or truncated.\n",
        "    \"\"\"\n",
        "    to_pad = []\n",
        "    max_l = min(max_l_size, len(l))  # maximum len\n",
        "    l_init = len(l) - max_l if len(l) > max_l and keep_lasts else 0  # initial position where to sample from the list\n",
        "    l_end = len(l) if len(l) > max_l and keep_lasts else max_l\n",
        "    for i in range(l_init, l_end):\n",
        "        to_pad.append(l[i])\n",
        "\n",
        "    # for j in range(len(l), max_l_size):\n",
        "    #     to_pad.append(pad_token)\n",
        "    pad_tokens = [pad_token] * (max_l_size-len(l))\n",
        "    padded_l = to_pad + pad_tokens if pad_right else pad_tokens + to_pad\n",
        "\n",
        "    return padded_l\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, vocab_size=None):\n",
        "        self.dictionary = dict()\n",
        "        self.rev_dictionary = dict()\n",
        "        self.count = []\n",
        "        self.special_tokens = []\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build_vocabulary_from_counts(self, count, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Sets all the attributes of the Vocabulary object.\n",
        "        :param count: a list of lists as follows: [['token', number_of_occurrences],...]\n",
        "        :param special_tokens: a list of strings. E.g. ['<EOS>', '<PAD>',...]\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        dictionary = dict()\n",
        "        for word, _ in count:\n",
        "            dictionary[word] = len(dictionary)\n",
        "\n",
        "        # adding eventual special tokens to the dictionary (e.g. <EOS>,<PAD> etc..)\n",
        "        d = len(dictionary)\n",
        "        for i, token in enumerate(special_tokens):\n",
        "            dictionary[token] = d + i\n",
        "\n",
        "        self.count = count\n",
        "        self.dictionary = dictionary\n",
        "        self.rev_dictionary = dict(zip(self.dictionary.values(), self.dictionary.keys()))\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = len(dictionary)\n",
        "\n",
        "    def build_vocabulary_from_tokens(self, tokens, vocabulary_size=None, special_tokens=[]):\n",
        "        \"\"\"\n",
        "        Given a list of tokens, it sets the Vocabulary object attributes by constructing\n",
        "        a dictionary mapping each token to a unique id.\n",
        "        :param tokens: a list of strings.\n",
        "         E.g. [\"the\", \"cat\", \"is\", ... \".\", \"the\", \"house\" ,\"is\" ...].\n",
        "         NB: Here you should put all your token instances of the corpus.\n",
        "        :param vocabulary_size: The number of elements of your vocabulary. If there are more\n",
        "        than 'vocabulary_size' elements on tokens, it considers only the 'vocabulary_size'\n",
        "        most frequent ones.\n",
        "        :param special_tokens: Optional. A list of strings. Useful to add special tokens in vocabulary.\n",
        "        If you don't have any, keep it empty.\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "\n",
        "        vocabulary_size = vocabulary_size if vocabulary_size is not None else self.vocab_size\n",
        "        vocabulary_size = vocabulary_size - (len(special_tokens) + 1) if vocabulary_size else None\n",
        "        # counts occurrences of each token\n",
        "        count = [['<UNK>', -1]]\n",
        "        count.extend(collections.Counter(tokens).most_common(vocabulary_size))  # takes only the most frequent ones, if size is None takes them all\n",
        "        self.build_vocabulary_from_counts(count, special_tokens)  # actually build the vocabulary\n",
        "        self._set_unk_count(tokens)  # set the number of OOV instances\n",
        "\n",
        "    def string2id(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of strings into a dataset of ids according to the object dictionary.\n",
        "        :param dataset: any string-based dataset with any nested lists.\n",
        "        :return: a new dataset, with the same shape of dataset, where each string is mapped into its\n",
        "        corresponding id associated in the dictionary (0 for unknown tokens).\n",
        "        \"\"\"\n",
        "\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, str) or isinstance(item, int) or isinstance(item, float):\n",
        "                    new_items.append(self.word2id(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def id2string(self, dataset):\n",
        "        \"\"\"\n",
        "        Converts a dataset of integer ids into a dataset of string according to the reverse dictionary.\n",
        "        :param dataset: any int-based dataset with any nested lists. Allowed types are int, np.int32, np.int64.\n",
        "        :return: a new dataset, with the same shape of dataset, where each token is mapped into its\n",
        "        corresponding string associated in the reverse dictionary.\n",
        "        \"\"\"\n",
        "        def _recursive_call(items):\n",
        "            new_items = []\n",
        "            for item in items:\n",
        "                if isinstance(item, int) or isinstance(item, np.int) or isinstance(item, np.int32) or isinstance(item, np.int64):\n",
        "                    new_items.append(self.id2word(item))\n",
        "                else:\n",
        "                    new_items.append(_recursive_call(item))\n",
        "            return new_items\n",
        "\n",
        "        return _recursive_call(dataset)\n",
        "\n",
        "    def word2id(self, item):\n",
        "        \"\"\"\n",
        "        Maps a string token to its corresponding id.\n",
        "        :param item: a string.\n",
        "        :return: If the token belongs to the vocabulary, it returns an integer id > 0, otherwise\n",
        "        it returns the value associated to the unknown symbol, that is typically 0.\n",
        "        \"\"\"\n",
        "        return self.dictionary[item] if item in self.dictionary else self.dictionary['<UNK>']\n",
        "\n",
        "    def id2word(self, token_id):\n",
        "        \"\"\"\n",
        "        Maps an integer token to its corresponding string.\n",
        "        :param token_id: an integer.\n",
        "        :return: If the id belongs to the vocabulary, it returns the string\n",
        "        associated to it, otherwise it returns the string associated\n",
        "        to the unknown symbol, that is '<UNK>'.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.rev_dictionary[token_id] if token_id in self.rev_dictionary else self.rev_dictionary[self.dictionary['<UNK>']]\n",
        "\n",
        "    def _set_unk_count(self, tokens):\n",
        "        \"\"\"\n",
        "        Sets the number of OOV instances in the tokens provided\n",
        "        :param tokens: a list of tokens\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        data = list()\n",
        "        unk_count = 0\n",
        "        for word in tokens:\n",
        "            if word in self.dictionary:\n",
        "                index = self.dictionary[word]\n",
        "            else:\n",
        "                index = 0  # dictionary['<UNK>']\n",
        "                unk_count += 1\n",
        "            data.append(index)\n",
        "        self.count[0][1] = unk_count\n",
        "\n",
        "    def add_element(self, name, is_special_token=False):\n",
        "        if name not in self.dictionary:\n",
        "            self.vocab_size += 1\n",
        "            self.dictionary[name] = self.vocab_size\n",
        "            self.rev_dictionary[self.vocab_size] = name\n",
        "\n",
        "            if is_special_token:\n",
        "                self.special_tokens = list(self.special_tokens)\n",
        "                self.special_tokens.append(name)\n",
        "\n",
        "            self.count.append([name, 1])\n",
        "\n",
        "    def set_vocabulary(self, dictionary, rev_dictionary, special_tokens, vocab_size):\n",
        "        self.dictionary = dictionary,\n",
        "        self.rev_dictionary = rev_dictionary\n",
        "        self.special_tokens = special_tokens\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "\n",
        "class SyLMDataset(object):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        self.config = config\n",
        "        self.vocabulary = sy_vocab\n",
        "\n",
        "        self.raw_train_x = []\n",
        "        self.raw_val_x = []\n",
        "        self.raw_test_x = []\n",
        "        self.raw_x = []\n",
        "\n",
        "        self.train_x, self.train_y = [], []\n",
        "        self.val_x, self.val_y = [], []\n",
        "        self.test_x, self.test_y = [], []\n",
        "        self.x, self.y = [], []\n",
        "\n",
        "    def initialize(self, sess):\n",
        "        pass\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Extract raw texts form sources and gather them all together.\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :return: a list of raw strings.\n",
        "        \"\"\"\n",
        "        return NotImplementedError\n",
        "\n",
        "    def build(self, sources, split_size=0.8):\n",
        "        \"\"\"\n",
        "        :param sources: a string or an iterable of strings containing the file(s)\n",
        "        to process in order to build the dataset.\n",
        "        :param split_size: the size to split the dataset, set >=1.0 to not split.\n",
        "        \"\"\"\n",
        "\n",
        "        raw_x = self.load(sources)\n",
        "        # raw_x = self.tokenize([self.preprocess(ex) for ex in raw_x])  # fixme\n",
        "        # splitting data\n",
        "        self.raw_x = raw_x\n",
        "        if split_size < 1.0:\n",
        "            self.raw_train_x, self.raw_test_x = self.split(self.raw_x, train_size=split_size)\n",
        "            self.raw_train_x, self.raw_val_x = self.split(self.raw_train_x, train_size=split_size)\n",
        "        else:\n",
        "            self.raw_train_x = self.raw_x\n",
        "\n",
        "        if self.vocabulary is None:\n",
        "            # creates vocabulary\n",
        "            tokens = [item for sublist in self.raw_train_x for item in sublist]  # get tokens\n",
        "            special_tokens = (\"<GO>\", \"<PAD>\", \"<SEP>\", \"<EOS>\", \"<EOV>\")\n",
        "            self._create_vocab(tokens, special_tokens=special_tokens)\n",
        "\n",
        "        # creates x,y for train\n",
        "        self.train_x = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.train_y = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for validation\n",
        "        self.val_x = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.val_y = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "        # creates x,y for testing\n",
        "        self.test_x = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "        self.test_y = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
        "\n",
        "    def _create_vocab(self, tokens, special_tokens=(\"<PAD>\", \"<GO>\", \"<SEP>\", \"<EOV>\", \"<EOS>\")):\n",
        "        \"\"\"\n",
        "        Create the vocabulary. Special tokens can be added to the tokens obtained from\n",
        "        the corpus.\n",
        "        :param tokens: a list of all the tokens in the corpus. Each token is a string.\n",
        "        :param special_tokens: a list of strings.\n",
        "        \"\"\"\n",
        "        print(\"creating_vocabulary\") \n",
        "\n",
        "        vocab = Vocabulary(vocab_size=self.config.input_vocab_size)\n",
        "        vocab.build_vocabulary_from_tokens(tokens, special_tokens=special_tokens)\n",
        "        self.vocabulary = vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def split(raw_data, train_size=0.8):\n",
        "        size = math.floor(len(raw_data)*train_size)\n",
        "        return raw_data[:size], raw_data[size:]\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(txt):\n",
        "        return txt\n",
        "\n",
        "    @staticmethod\n",
        "    def shuffle(x):\n",
        "        return random.sample(x, len(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(txt):\n",
        "        return txt\n",
        "\n",
        "    def _build_dataset(self, raw_data, max_len=200, insert_go=True, keep_lasts=False, pad_right=True, shuffle=True):\n",
        "        \"\"\"\n",
        "        Converts all the tokens in e1_raw_data by mapping each token with its corresponding\n",
        "        value in the dictionary. In case of token not in the dictionary, they are assigned to\n",
        "        a specific id. Each sequence is padded up to the seq_max_len setup in the config.\n",
        "\n",
        "        :param raw_data: list of sequences, each sequence is a list of tokens (strings).\n",
        "        :param max_len: max length of a sequence, crop longer and pad smaller ones.\n",
        "        :param insert_go: True to insert <GO>, False otherwise.\n",
        "        :param keep_lasts: True to truncate initial elements of a sequence.\n",
        "        :param pad_right: pad to the right (default value True), otherwise pads to left.\n",
        "        :param shuffle: Optional. If True data are shuffled.\n",
        "        :return: A list of sequences where each token in each sequence is an int id.\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        for sentence in raw_data:\n",
        "            sentence_ids = [self.vocabulary.word2id(\"<GO>\")] if insert_go else []\n",
        "            sentence_ids.extend([self.vocabulary.word2id(w) for w in sentence])\n",
        "            sentence_ids.append(self.vocabulary.word2id(\"<EOS>\"))\n",
        "            sentence_ids = pad_list(sentence_ids, self.vocabulary.word2id(\"<PAD>\"), max_len, keep_lasts=keep_lasts, pad_right=pad_right)\n",
        "\n",
        "            dataset.append(sentence_ids)\n",
        "\n",
        "        if shuffle:\n",
        "            return random.sample(dataset, len(dataset))\n",
        "        else:\n",
        "            return dataset\n",
        "\n",
        "    def get_batches(self, batch_size=32, split_sel='train'):\n",
        "        \"\"\"\n",
        "        Iterator over the training set. Useful method to run experiments.\n",
        "        :param batch_size: size of the mini_batch\n",
        "        :return: input and target.\n",
        "        \"\"\"\n",
        "        if split_sel == 'train':\n",
        "            x, y = self.train_x, self.train_y\n",
        "        elif split_sel == 'val':\n",
        "            x, y = self.val_x, self.val_y\n",
        "        else:\n",
        "            x, y = self.test_x, self.test_y\n",
        "        \n",
        "        i = 0 #random.randint(0, batch_size)\n",
        "        batches = []\n",
        "        eov = self.vocabulary.word2id(\"<EOV>\")\n",
        "        go = self.vocabulary.word2id(\"<GO>\")\n",
        "        # prepare batches\n",
        "        while i < len(x):\n",
        "            j = 0\n",
        "            batch_x, batch_y = [], []\n",
        "            while j < batch_size and i+j<len(x):\n",
        "                for c in x[i+j]:\n",
        "                  batch_x.append(c)\n",
        "                batch_x.append(eov)\n",
        "                for c in y[i+j]:\n",
        "                  batch_y.append(c)\n",
        "                batch_y.append(eov)\n",
        "                j += 1\n",
        "            i += batch_size\n",
        "            batches.append((batch_x, batch_y))\n",
        "\n",
        "        # supply\n",
        "        i = 0\n",
        "        while i < len(batches):\n",
        "            yield batches[i][0], batches[i][1]\n",
        "            i += 1\n",
        "\n",
        "class DanteSyLMDataset(SyLMDataset):\n",
        "    def __init__(self, config, sy_vocab=None):\n",
        "        \"\"\"\n",
        "        Class to create a dataset from Dante Alighieri's Divine Comedy.\n",
        "        :param config: a Config object\n",
        "        :param sy_vocab: (optional) a Vocabulary object where tokens of the dictionary\n",
        "        are syllables. If None, the vocabulary is create automatically from the source.\n",
        "        \"\"\"\n",
        "        super().__init__(config, sy_vocab)\n",
        "\n",
        "    def load(self, sources):\n",
        "        \"\"\"\n",
        "        Load examples from dataset\n",
        "        :param sources: data filepath.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        canti = get_dc_cantos(filename=sources)  # get raw data from file\n",
        "        canti, tokens = get_dc_hyphenation(canti)  # converts each\n",
        "\n",
        "        tercets = create_tercets(canti)\n",
        "        tercets = get_hyp_lm_tercets(tercets)\n",
        "        x = []\n",
        "        for tercet in tercets:\n",
        "            x.append([])\n",
        "            for verse in tercet:\n",
        "                x[-1].extend(verse)\n",
        "                x[-1].append(\"<EOV>\")\n",
        "\n",
        "        #x = self.shuffle(x)\n",
        "        return x\n",
        "\n",
        "def seq2str(seq):\n",
        "    def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n",
        "        to_print = ''\n",
        "        for token in batch:\n",
        "            if token in special_tokens:\n",
        "                to_print += ' '\n",
        "            elif end_of_tokens and token in end_of_tokens:\n",
        "                to_print += '\\n'\n",
        "            elif token in rev_vocabulary:\n",
        "                to_print += rev_vocabulary[token]\n",
        "            else:\n",
        "                to_print += '<UNK>'\n",
        "        return to_print\n",
        "\n",
        "    return output2string(seq, poetry_sy_lm_dataset.vocabulary.rev_dictionary,\n",
        "      special_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\"), \n",
        "                      0, \n",
        "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<SEP>\"),\n",
        "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\"), \n",
        "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")],\n",
        "      end_of_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")])\n",
        "\n",
        "class cnfg:\n",
        "  vocab_size = vocab_size\n",
        "  input_vocab_size = vocab_size\n",
        "  sentence_max_len = terces_len\n",
        "\n",
        "config = cnfg()\n",
        "poetry_sy_lm_dataset = DanteSyLMDataset(config, sy_vocab=None)\n",
        "\n",
        "data_path = 'syllComedy.txt'  # dataset location, here just the name of the source file\n",
        "\n",
        "poetry_sy_lm_dataset.build(data_path, split_size=0.99)  # actual creation of  vocabulary (if not provided) and dataset\n",
        "print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n",
        "print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n",
        "print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n",
        "\n",
        "eov = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")\n",
        "pad = poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\")\n",
        "go = poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\")\n",
        "eos = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")\n",
        "\n",
        "\n",
        "batches = [b for b in poetry_sy_lm_dataset.get_batches(terces_per_batch)]\n",
        "print(batches[0][0])\n",
        "print(len(batches[0][0]))\n",
        "test_b = [b for b in poetry_sy_lm_dataset.get_batches(terces_per_batch, split_sel='test')]\n",
        "print(len(test_b[0][0]))\n",
        "val_b = [b for b in poetry_sy_lm_dataset.get_batches(terces_per_batch, split_sel='val')]\n",
        "print(len(val_b[0][0]))\n",
        "len(poetry_sy_lm_dataset.vocabulary.dictionary.items())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating_vocabulary\n",
            "Train size: 4649\n",
            "Val size: 47\n",
            "Test size: 48\n",
            "[40, 8, 4, 10, 3, 1, 42, 17, 4, 26, 1, 42, 26, 7, 3, 1, 42, 14, 4, 10, 3, 1, 42, 13, 5, 17, 1, 42, 17, 6, 8, 3, 1, 42, 14, 6, 3, 1, 42, 8, 7, 1, 42, 12, 11, 9, 5, 3, 1, 42, 19, 6, 1, 42, 11, 5, 1, 44, 17, 6, 3, 1, 42, 9, 6, 1, 42, 11, 9, 7, 1, 42, 19, 5, 6, 3, 1, 42, 18, 4, 9, 3, 1, 42, 16, 1, 42, 8, 5, 3, 1, 42, 12, 4, 10, 1, 42, 19, 5, 3, 7, 1, 42, 12, 13, 16, 1, 42, 9, 5, 1, 44, 13, 22, 31, 3, 1, 42, 10, 5, 3, 1, 42, 14, 6, 1, 42, 9, 6, 11, 1, 42, 11, 5, 3, 1, 42, 19, 6, 5, 3, 1, 42, 4, 1, 42, 9, 5, 3, 1, 42, 12, 17, 5, 9, 1, 42, 9, 6, 1, 42, 11, 5, 1, 44, 43, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 44, 40, 5, 22, 6, 3, 1, 42, 24, 16, 5, 8, 1, 42, 11, 7, 3, 5, 3, 1, 42, 14, 6, 9, 3, 1, 42, 24, 16, 5, 10, 3, 1, 42, 4, 1, 42, 9, 5, 3, 30, 3, 1, 42, 13, 7, 1, 42, 12, 5, 3, 1, 42, 14, 16, 1, 42, 9, 5, 1, 44, 4, 1, 42, 12, 11, 5, 3, 1, 42, 12, 4, 10, 1, 42, 19, 5, 3, 1, 42, 12, 4, 10, 1, 42, 19, 5, 21, 1, 42, 21, 6, 5, 3, 4, 3, 1, 42, 5, 1, 42, 12, 18, 9, 5, 3, 4, 3, 1, 42, 23, 7, 9, 1, 42, 11, 4, 1, 44, 13, 22, 4, 3, 1, 42, 8, 4, 10, 3, 1, 42, 18, 4, 8, 1, 42, 12, 6, 4, 9, 3, 1, 42, 9, 6, 1, 42, 8, 7, 1, 42, 19, 5, 3, 1, 42, 10, 5, 3, 1, 42, 18, 5, 1, 42, 16, 1, 42, 9, 5, 34, 1, 44, 43, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 44, 40, 11, 5, 8, 1, 42, 11, 20, 3, 30, 3, 1, 42, 5, 1, 42, 17, 5, 1, 42, 9, 5, 3, 1, 42, 13, 22, 4, 3, 1, 42, 18, 7, 1, 42, 13, 7, 3, 30, 3, 1, 42, 18, 6, 28, 3, 1, 42, 17, 7, 9, 1, 42, 11, 4, 1, 44, 17, 5, 3, 1, 42, 18, 4, 9, 3, 1, 42, 11, 9, 5, 11, 1, 42, 11, 5, 9, 3, 1, 42, 14, 4, 10, 3, 1, 42, 25, 4, 8, 3, 1, 42, 13, 22, 20, 3, 6, 20, 3, 1, 42, 19, 6, 3, 1, 42, 11, 9, 7, 1, 42, 19, 5, 6, 1, 44, 14, 6, 1, 42, 9, 29, 3, 1, 42, 14, 4, 3, 1, 42, 10, 20, 3, 5, 10, 1, 42, 11, 9, 4, 3, 1, 42, 13, 7, 1, 42, 12, 4, 3, 1, 42, 13, 22, 20, 3, 6, 20, 3, 1, 42, 19, 20, 3, 22, 7, 3, 1, 42, 12, 13, 7, 9, 1, 42, 11, 4, 1, 44, 43, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 44, 40, 6, 7, 3, 1, 42, 8, 7, 8, 3, 1, 42, 12, 7, 3, 1, 42, 25, 4, 8, 3, 1, 42, 9, 6, 1, 42, 14, 6, 9, 3, 1, 42, 13, 7, 17, 20, 3, 1, 42, 6, 20, 3, 1, 42, 19, 20, 3, 6, 8, 1, 42, 11, 9, 5, 6, 1, 44, 11, 5, 8, 1, 42, 11, 20, 3, 4, 1, 42, 9, 5, 3, 1, 42, 18, 6, 4, 8, 3, 1, 42, 14, 6, 3, 1, 42, 12, 7, 8, 1, 42, 8, 7, 3, 1, 42, 5, 3, 1, 42, 24, 16, 4, 10, 3, 1, 42, 18, 16, 8, 1, 42, 11, 7, 1, 44, 13, 22, 4, 3, 1, 42, 10, 5, 3, 1, 42, 19, 4, 1, 42, 9, 5, 1, 42, 13, 4, 3, 1, 42, 19, 6, 5, 3, 1, 42, 5, 25, 1, 42, 25, 5, 8, 1, 42, 14, 7, 1, 42, 8, 5, 6, 1, 44, 43, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 44, 40, 17, 5, 3, 1, 42, 18, 7, 6, 3, 1, 42, 13, 22, 20, 3, 6, 20, 3, 1, 42, 23, 16, 6, 3, 1, 42, 5, 10, 3, 1, 42, 18, 6, 30, 3, 1, 42, 14, 20, 3, 16, 8, 3, 1, 42, 13, 7, 10, 1, 42, 10, 4, 3, 1, 42, 21, 6, 16, 8, 1, 42, 11, 7, 1, 44, 10, 32, 3, 1, 42, 14, 7, 1, 42, 19, 4, 3, 1, 42, 11, 4, 9, 1, 42, 17, 6, 1, 42, 8, 5, 1, 42, 19, 5, 3, 1, 42, 24, 16, 4, 10, 1, 42, 10, 5, 3, 1, 42, 19, 5, 10, 1, 42, 10, 4, 1, 44, 13, 22, 4, 3, 1, 42, 17, 20, 3, 5, 1, 42, 19, 4, 5, 3, 1, 42, 14, 6, 3, 1, 42, 18, 5, 1, 42, 16, 1, 42, 9, 5, 3, 6, 10, 3, 1, 42, 13, 7, 9, 3, 1, 42, 13, 7, 17, 1, 42, 18, 16, 8, 1, 42, 11, 7, 1, 44, 43, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 44, 40, 21, 16, 5, 9, 1, 42, 14, 5, 6, 3, 1, 42, 6, 8, 3, 1, 42, 5, 10, 1, 42, 11, 7, 3, 4, 3, 1, 42, 19, 6, 1, 42, 14, 6, 3, 1, 42, 10, 4, 3, 1, 42, 12, 16, 4, 3, 1, 42, 12, 18, 5, 10, 1, 42, 10, 4, 1, 44, 19, 4, 1, 42, 12, 11, 6, 1, 42, 11, 4, 3, 1, 42, 21, 6, 32, 3, 1, 42, 14, 4, 20, 3, 1, 42, 9, 5, 21, 1, 42, 21, 6, 3, 1, 42, 14, 4, 10, 3, 1, 42, 18, 6, 5, 1, 42, 8, 4, 1, 42, 11, 5, 1, 44, 13, 22, 4, 3, 1, 42, 17, 4, 1, 42, 8, 5, 3, 1, 42, 14, 9, 6, 11, 1, 42, 11, 7, 3, 5, 10, 1, 42, 11, 9, 16, 6, 3, 1, 42, 18, 4, 9, 3, 1, 42, 7, 1, 42, 21, 8, 4, 3, 1, 42, 13, 5, 10, 1, 42, 10, 4, 1, 44, 43, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 44, 40, 5, 10, 1, 42, 10, 7, 9, 3, 1, 42, 23, 16, 3, 1, 42, 10, 5, 3, 1, 42, 18, 5, 1, 42, 16, 1, 42, 9, 5, 3, 16, 8, 3, 1, 42, 18, 7, 1, 42, 13, 7, 3, 1, 42, 24, 16, 4, 1, 42, 11, 5, 1, 44, 13, 22, 4, 3, 1, 42, 8, 4, 10, 3, 1, 42, 10, 5, 1, 42, 21, 7, 3, 1, 42, 14, 4, 10, 3, 1, 42, 13, 7, 9, 3, 1, 42, 17, 20, 3, 4, 1, 42, 9, 5, 3, 1, 42, 14, 16, 1, 42, 9, 5, 1, 42, 11, 5, 1, 44, 10, 5, 3, 1, 42, 8, 7, 11, 1, 42, 11, 4, 3, 1, 42, 13, 22, 20, 3, 6, 20, 3, 1, 42, 18, 5, 12, 1, 42, 12, 5, 6, 3, 1, 42, 13, 7, 8, 3, 1, 42, 11, 5, 8, 1, 42, 11, 5, 3, 1, 42, 18, 6, 4, 1, 42, 11, 5, 1, 44, 43, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 44, 40, 4, 3, 1, 42, 13, 7, 1, 42, 17, 4, 3, 1, 42, 24, 16, 4, 6, 3, 1, 42, 13, 22, 4, 3, 1, 42, 13, 7, 8, 3, 1, 42, 10, 4, 1, 42, 8, 5, 3, 5, 23, 1, 42, 23, 5, 8, 1, 42, 8, 5, 1, 42, 11, 5, 1, 44, 16, 1, 42, 12, 13, 6, 1, 42, 11, 7, 3, 1, 42, 23, 16, 7, 9, 3, 1, 42, 14, 4, 10, 3, 1, 42, 18, 4, 1, 42, 10, 5, 1, 42, 21, 7, 3, 5, 3, 1, 42, 10, 5, 3, 1, 42, 9, 6, 1, 42, 19, 5, 1, 44, 12, 6, 3, 1, 42, 19, 7, 10, 1, 42, 21, 4, 3, 5, 3, 1, 42, 10, 20, 3, 5, 13, 1, 42, 24, 16, 5, 3, 1, 42, 18, 4, 1, 42, 9, 6, 1, 42, 21, 10, 6, 7, 1, 42, 12, 5, 3, 4, 3, 1, 42, 21, 16, 5, 1, 42, 11, 5, 1, 44, 43, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 44]\n",
            "1608\n",
            "1608\n",
            "1608\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_6n273Y1I18",
        "outputId": "298b7fa2-7eeb-46c8-e318-6e0c15cc902c"
      },
      "source": [
        "poetry_sy_lm_dataset.vocabulary.dictionary"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 3,\n",
              " '!': 34,\n",
              " '<EOS>': 43,\n",
              " '<EOV>': 44,\n",
              " '<GO>': 40,\n",
              " '<PAD>': 41,\n",
              " '<SEP>': 42,\n",
              " '<UNK>': 0,\n",
              " '?': 33,\n",
              " 'a': 5,\n",
              " 'b': 25,\n",
              " 'c': 13,\n",
              " 'd': 14,\n",
              " 'e': 4,\n",
              " 'f': 23,\n",
              " 'g': 21,\n",
              " 'h': 22,\n",
              " 'i': 6,\n",
              " 'j': 38,\n",
              " 'l': 10,\n",
              " 'm': 17,\n",
              " 'n': 8,\n",
              " 'o': 7,\n",
              " 'p': 18,\n",
              " 'q': 24,\n",
              " 'r': 9,\n",
              " 's': 12,\n",
              " 't': 11,\n",
              " 'u': 16,\n",
              " 'v': 19,\n",
              " 'x': 37,\n",
              " 'y': 39,\n",
              " 'z': 26,\n",
              " '|': 1,\n",
              " 'à': 32,\n",
              " 'è': 30,\n",
              " 'é': 31,\n",
              " 'ì': 27,\n",
              " 'ò': 29,\n",
              " 'ó': 35,\n",
              " 'ù': 28,\n",
              " '‘': 36,\n",
              " '’': 20}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xURM0fNiOIKj",
        "outputId": "3987828d-b210-485b-c6be-36863e2d1f29"
      },
      "source": [
        "print(seq2str(batches[14][0]))\n",
        "print(seq2str(batches[15][0]))\n",
        "print(seq2str(batches[16][0]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " vi| di e | co| nob| bi | l’ om| bra | di | co| lui|\n",
            "che | fe| ce | per | vil| ta| de il | gran | ri| fiu| to|\n",
            "in| con| ta| nen| te in| te| si e | cer| to | fui|\n",
            "                                      \n",
            " che | que| sta e| ra | la | set| ta | d’ i | cat| ti| vi|\n",
            "a | dio | spia| cen| ti e | a’ | ne| mi| ci | sui|\n",
            "que| sti | sciau| ra| ti | che | mai | non | fur | vi| vi|\n",
            "                               \n",
            " e| ra| no i| gnu| di e | sti| mo| la| ti | mol| to|\n",
            "da | mo| sco| ni e | da | ve| spe | ch’ e| ran | i| vi|\n",
            "el| le | ri| ga| van | lor | di | san| gue il | vol| to|\n",
            "                                  \n",
            " che | mi| schia| to | di | la| gri| me a’ | lor | pie| di|\n",
            "da | fa| sti| dio| si | ver| mi e| ra | ri| col| to|\n",
            "e | poi | ch’ a | ri| guar| dar | ol| tre | mi | die| di|\n",
            "                             \n",
            " vi| di | gen| ti a | la | ri| va | d’ un | gran | fiu| me|\n",
            "per | ch’ io | dis| si | ma| e| stro or | mi | con| ce| di|\n",
            "ch’ i’ | sap| pia | qua| li | so| no e | qual | co| stu| me|\n",
            "                   \n",
            " le | fa | di | tra| pas| sar | pa| rer | sì | pron| te|\n",
            "com’ | i’ | di| scer| no | per | lo | fio| co | lu| me|\n",
            "ed | el| li a | me | le | co| se | ti | fier | con| te|\n",
            "                               \n",
            " quan| do | noi | fer| me| rem | li | no| stri | pas| si|\n",
            "su | la | tri| sta | ri| vie| ra | d’ a| che| ron| te|\n",
            "al| lor | con | li oc| chi | ver| go| gno| si e | bas| si|\n",
            "                            \n",
            " te| men| do | no ’l | mio | dir | li | fos| se | gra| ve|\n",
            "in| fi| no al | fiu| me | del | par| lar | mi | tras| si|\n",
            "ed | ec| co | ver| so | noi | ve| nir | per | na| ve|\n",
            "                             \n",
            "\n",
            " un | vec| chio | bian| co | per | an| ti| co | pe| lo|\n",
            "gri| dan| do | guai | a | voi | a| ni| me | pra| ve!|\n",
            "non | i| spe| ra| te | mai | ve| der | lo | cie| lo|\n",
            "                                     \n",
            " i’ | ve| gno | per | me| nar| vi a | l’ al| tra | ri| va|\n",
            "ne | le | te| ne| bre et| ter| ne in | cal| do e ’n | ge| lo|\n",
            "e | tu | che | se’ | co| stì | a| ni| ma | vi| va|\n",
            "                            \n",
            " pàr| ti| ti | da | co| te| sti | che | son | mor| ti|\n",
            "ma | poi | che | vi| de | ch’ io | non | mi | par| ti| va|\n",
            "dis| se | per | al| tra | via | per | al| tri | por| ti|\n",
            "                             \n",
            " ver| rai | a | piag| gia | non | qui | per | pas| sa| re|\n",
            "più | lie| ve | le| gno | con| vien | che | ti | por| ti|\n",
            "e ’l | du| ca | lui | ca| ron | non | ti | cruc| cia| re|\n",
            "                         \n",
            " vuol| si | co| sì | co| là | do| ve | si | puo| te|\n",
            "ciò | che | si | vuo| le e | più | non | di| man| da| re|\n",
            "quin| ci | fuor | que| te | le | la| no| se | go| te|\n",
            "                                   \n",
            " al | noc| chier | de | la | li| vi| da | pa| lu| de|\n",
            "che ’n| tor| no a | li oc| chi a| vea | di | fiam| me | ro| te|\n",
            "ma | quel| l’ a| ni| me | ch’ e| ran | las| se e | nu| de|\n",
            "                       \n",
            " can| giar | co| lo| re e | di| bat| te| ro i | den| ti|\n",
            "rat| to | che ’n| te| ser | le | pa| ro| le | cru| de|\n",
            "be| stem| mia| va| no | dio | e | lor | pa| ren| ti|\n",
            "                                   \n",
            " l’ u| ma| na | spe| zie e ’l | lo| co e ’l | tem| po e ’l | se| me|\n",
            "di | lor | se| men| za e | di | lor | na| sci| men| ti|\n",
            "poi | si | ri| tras| ser | tut| te | quan| te in| sie| me|\n",
            "                \n",
            "\n",
            " for| te | pian| gen| do a | la | ri| va | mal| va| gia|\n",
            "ch’ at| ten| de | cia| scun | uom | che | dio | non | te| me|\n",
            "ca| ron | di| mo| nio | con | oc| chi | di | bra| gia|\n",
            "                          \n",
            " lo| ro ac| cen| nan| do | tut| te | le | rac| co| glie|\n",
            "bat| te | col | re| mo | qua| lun| que | s’ a| da| gia|\n",
            "co| me | d’ au| tun| no | si | le| van | le | fo| glie|\n",
            "                               \n",
            " l’ u| na ap| pres| so | de | l’ al| tra | fin | che ’l | ra| mo|\n",
            "ve| de a | la | ter| ra | tut| te | le | sue | spo| glie|\n",
            "si| mi| le| men| te il | mal | se| me | d’ a| da| mo|\n",
            "                      \n",
            " git| tan| si | di | quel | li| to ad | u| na ad | u| na|\n",
            "per | cen| ni | co| me au| gel | per | suo | ri| chia| mo|\n",
            "co| sì | sen | van| no | su | per | l’ on| da | bru| na|\n",
            "                          \n",
            " e | a| van| ti | che | sien | di | là | di| sce| se|\n",
            "an| che | di | qua | nuo| va | schie| ra | s’ a| u| na|\n",
            "fi| gliuol | mio | dis| se ’l | ma| e| stro | cor| te| se|\n",
            "                               \n",
            " quel| li | che | muo| ion | ne | l’ i| ra | di | dio|\n",
            "tut| ti | con| ve| gnon | qui | d’ o| gne | pa| e| se|\n",
            "e | pron| ti | so| no a | tra| pas| sar | lo | rio|\n",
            "                                      \n",
            " ché | la | di| vi| na | giu| sti| zia | li | spro| na|\n",
            "sì | che | la | te| ma | si | vol| ve in | di| sio|\n",
            "quin| ci | non | pas| sa | mai | a| ni| ma | buo| na|\n",
            "                                      \n",
            " e | pe| rò | se | ca| ron | di | te | si | la| gna|\n",
            "ben | puoi | sa| pe| re o| mai | che ’l | suo | dir | suo| na|\n",
            "fi| ni| to | que| sto | la | bu| ia | cam| pa| gna|\n",
            "                                \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmSJj8BMqVY_"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICqI4vK-pC4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a62937c-b43e-4a49-b10b-8bb5a298c5c7"
      },
      "source": [
        "wandb.config.num_layers = 4\n",
        "wandb.config.d_model = 128\n",
        "wandb.config.dff = 256\n",
        "wandb.config.num_heads = 4\n",
        "wandb.config.dropout = 0.1\n",
        "wandb.config.learning_rate = 2e-4 \n",
        "\n",
        "generate_at = [] #[10,20,30,40,50,60,70,80,90,100,110,120,130,140,150]\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "def create_padding_mask(seq):   \n",
        "    seq = tf.cast(tf.math.equal(seq, pad), tf.float32)\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "    \n",
        "    Args:\n",
        "        q: query shape == (..., seq_len_q, depth)\n",
        "        k: key shape == (..., seq_len_k, depth)\n",
        "        v: value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "        \n",
        "    Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    return output, attention_weights\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % self.num_heads == 0\n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "            \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        \n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "        \n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        return output, attention_weights\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def __call__(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        return out2\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "    \n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "        \n",
        "    def __call__(self, x, enc_output, training, \n",
        "            look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "        \n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "        \n",
        "        ffn_output = self.ffn(out2)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "        \n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.d_model)\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                        for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "            \n",
        "    def __call__(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "        return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                        for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def __call__(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                look_ahead_mask, padding_mask)\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "        \n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "                target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                            input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                            target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "        \n",
        "    def __call__(self, inp, tar, training, enc_padding_mask, \n",
        "            look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "        \n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "        \n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "        \n",
        "        return final_output, attention_weights\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(wandb.config.learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "val_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "input_vocab_size = vocab_size\n",
        "target_vocab_size = vocab_size\n",
        "max_len = batch_len\n",
        "\n",
        "transformer = Transformer(wandb.config.num_layers, wandb.config.d_model, \n",
        "                          wandb.config.num_heads, wandb.config.dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=max_len, \n",
        "                          pe_target=max_len,\n",
        "                          rate=wandb.config.dropout)\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    \n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')\n",
        "\n",
        "@tf.function()\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                    True, \n",
        "                                    enc_padding_mask, \n",
        "                                    combined_mask, \n",
        "                                    dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "    \n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)\n",
        "\n",
        "@tf.function()\n",
        "def val_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    \n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                False, \n",
        "                                enc_padding_mask, \n",
        "                                combined_mask, \n",
        "                                dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "    \n",
        "    val_loss(loss)\n",
        "    val_accuracy(tar_real, predictions)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.final_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.final_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.embedding.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.embedding.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.layernorm1.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.layernorm2.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.layernorm1.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.layernorm2.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.layernorm1.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.layernorm2.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.layernorm1.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.layernorm2.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.layernorm1.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.layernorm2.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.layernorm3.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.layernorm3.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.layernorm3.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.layernorm1.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.layernorm2.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.layernorm3.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.layernorm3.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.layernorm3.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.layernorm1.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.layernorm2.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.layernorm3.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.layernorm3.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.layernorm3.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.layernorm1.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.layernorm2.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.layernorm3.axis\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.layernorm3.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.layernorm3.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.mha.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.mha.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.mha.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.mha.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.mha.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.mha.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.mha.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.mha.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.0.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.mha.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.mha.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.mha.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.mha.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.mha.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.mha.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.mha.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.mha.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.1.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.mha.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.mha.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.mha.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.mha.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.mha.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.mha.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.mha.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.mha.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.2.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.mha.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.mha.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.mha.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.mha.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.mha.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.mha.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.mha.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.mha.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.encoder.enc_layers.3.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha1.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha1.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha1.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha1.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha1.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha1.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha1.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha1.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha2.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha2.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha2.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha2.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha2.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha2.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha2.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.mha2.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.0.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha1.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha1.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha1.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha1.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha1.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha1.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha1.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha1.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha2.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha2.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha2.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha2.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha2.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha2.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha2.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.mha2.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.1.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha1.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha1.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha1.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha1.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha1.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha1.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha1.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha1.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha2.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha2.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha2.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha2.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha2.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha2.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha2.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.mha2.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.2.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha1.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha1.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha1.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha1.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha1.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha1.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha1.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha1.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha2.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha2.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha2.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha2.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha2.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha2.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha2.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.mha2.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).transformer.decoder.dec_layers.3.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.final_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.final_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.embedding.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.embedding.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.layernorm3.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.layernorm3.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.layernorm3.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.layernorm3.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.layernorm3.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.layernorm3.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.layernorm3.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.layernorm3.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.mha.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.mha.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.mha.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.mha.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.mha.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.mha.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.mha.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.mha.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.0.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.mha.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.mha.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.mha.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.mha.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.mha.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.mha.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.mha.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.mha.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.1.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.mha.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.mha.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.mha.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.mha.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.mha.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.mha.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.mha.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.mha.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.2.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.mha.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.mha.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.mha.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.mha.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.mha.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.mha.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.mha.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.mha.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.encoder.enc_layers.3.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha1.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha1.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha1.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha1.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha1.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha1.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha1.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha1.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha2.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha2.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha2.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha2.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha2.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha2.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha2.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.mha2.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.0.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha1.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha1.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha1.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha1.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha1.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha1.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha1.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha1.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha2.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha2.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha2.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha2.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha2.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha2.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha2.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.mha2.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.1.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha1.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha1.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha1.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha1.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha1.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha1.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha1.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha1.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha2.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha2.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha2.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha2.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha2.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha2.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha2.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.mha2.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.2.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha1.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha1.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha1.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha1.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha1.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha1.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha1.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha1.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha2.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha2.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha2.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha2.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha2.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha2.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha2.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.mha2.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).transformer.decoder.dec_layers.3.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.final_layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.final_layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.embedding.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.embedding.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.layernorm3.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.layernorm3.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.layernorm3.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.layernorm3.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.layernorm3.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.layernorm3.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.layernorm1.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.layernorm1.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.layernorm2.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.layernorm2.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.layernorm3.gamma\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.layernorm3.beta\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.mha.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.mha.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.mha.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.mha.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.mha.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.mha.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.mha.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.mha.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.0.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.mha.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.mha.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.mha.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.mha.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.mha.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.mha.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.mha.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.mha.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.1.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.mha.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.mha.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.mha.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.mha.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.mha.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.mha.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.mha.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.mha.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.2.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.mha.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.mha.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.mha.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.mha.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.mha.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.mha.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.mha.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.mha.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.encoder.enc_layers.3.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha1.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha1.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha1.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha1.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha1.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha1.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha1.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha1.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha2.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha2.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha2.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha2.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha2.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha2.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha2.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.mha2.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.0.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha1.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha1.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha1.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha1.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha1.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha1.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha1.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha1.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha2.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha2.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha2.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha2.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha2.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha2.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha2.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.mha2.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.1.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha1.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha1.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha1.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha1.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha1.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha1.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha1.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha1.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha2.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha2.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha2.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha2.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha2.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha2.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha2.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.mha2.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.2.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha1.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha1.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha1.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha1.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha1.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha1.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha1.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha1.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha2.wq.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha2.wq.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha2.wk.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha2.wk.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha2.wv.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha2.wv.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha2.dense.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.mha2.dense.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.ffn.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.ffn.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.ffn.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).transformer.decoder.dec_layers.3.ffn.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "Latest checkpoint restored!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaxwE-zBpx4S"
      },
      "source": [
        "#@title Generation\n",
        "def generate(index=0, k=1, t=1):\n",
        "\n",
        "    def evaluate_greedy(inp_sentence, decoder_input):\n",
        "        inp_sentence = inp_sentence\n",
        "        encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "        \n",
        "        output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "        terces = 0\n",
        "        for i in range(batch_len):\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "        \n",
        "            # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "            predictions, attention_weights = transformer(encoder_input, \n",
        "                                                        output,\n",
        "                                                        False,\n",
        "                                                        enc_padding_mask,\n",
        "                                                        combined_mask,\n",
        "                                                        dec_padding_mask)\n",
        "            \n",
        "            # select the last word from the seq_len dimension\n",
        "            predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "            predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "            # return the result if the predicted_id is equal to the end token\n",
        "            if predicted_id == eos:\n",
        "                terces += 1\n",
        "                if terces == terces_per_batch-1:\n",
        "                    return tf.squeeze(output, axis=0), attention_weights\n",
        "            # concatentate the predicted_id to the output which is given to the decoder\n",
        "            # as its input.\n",
        "            output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "        return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "\n",
        "    def evaluate_topk(inp_sentence, decoder_input, k=5, temperature=0.5):\n",
        "        inp_sentence = inp_sentence\n",
        "        encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "        \n",
        "        output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "        def scale(tensor):\n",
        "            tensor = tf.math.divide(\n",
        "                tf.subtract(\n",
        "                    tensor, \n",
        "                    tf.reduce_min(tensor)\n",
        "                ), \n",
        "                tf.subtract(\n",
        "                    tf.reduce_max(tensor), \n",
        "                    tf.reduce_min(tensor))\n",
        "                )\n",
        "            return tensor\n",
        "\n",
        "        terces = 0\n",
        "        for i in range(batch_len):\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "        \n",
        "            # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "            predictions, attention_weights = transformer(encoder_input, \n",
        "                                                        output,\n",
        "                                                        False,\n",
        "                                                        enc_padding_mask,\n",
        "                                                        combined_mask,\n",
        "                                                        dec_padding_mask)\n",
        "            # select the last word from the seq_len dimension\n",
        "            predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "            predictions, indices = tf.math.top_k(predictions,k=k)\n",
        "            predictions /= temperature\n",
        "            #predictions = scale(predictions)\n",
        "            predictions = np.squeeze(predictions, axis=0)\n",
        "            indices = np.squeeze(indices, axis=0)\n",
        "            indices = np.squeeze(indices, axis=0)\n",
        "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "            predicted_id = indices[predicted_id]\n",
        "\n",
        "            # return the result if the predicted_id is equal to the end token\n",
        "            if predicted_id == eos:\n",
        "                terces += 1\n",
        "                if terces == terces_per_batch-1:\n",
        "                    return tf.squeeze(output, axis=0), attention_weights\n",
        "            # concatentate the predicted_id to the output which is given to the decoder\n",
        "            # as its input.\n",
        "            predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "            predicted_id = tf.expand_dims(predicted_id, 0)\n",
        "            output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "        return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "    out_list = test_b[index][0]\n",
        "    offset = terces_len # a tercet\n",
        "    txt_gen = seq2str(out_list[-offset:])\n",
        "\n",
        "    print(\"params: k={}, t={}\".format(k,t))\n",
        "    for i in range(32//(terces_per_batch-1)): # 30 terces = cantica\n",
        "        out, att_w = evaluate_topk([pad], out_list[-offset:], k, t)\n",
        "        out_list = out.numpy().tolist()\n",
        "        out_str = seq2str(out_list[offset:])\n",
        "        txt_gen += out_str\n",
        "\n",
        "    print(txt_gen)\n",
        "    wandb.log({\"generated\":\n",
        "            wandb.Html(\"k=\"+str(k)+\" t=\"+str(t)+\n",
        "                       \"<pre>\"+txt_gen+\"</pre>\", inject=False)})"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv5bxDVqR0zv"
      },
      "source": [
        "# Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-izCTWvp23H",
        "outputId": "b08f7b1e-1665-4187-a2b5-27ca6f0a68b7"
      },
      "source": [
        "#@title Train loop\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "    random.shuffle(batches)\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (inp, tar)) in enumerate(batches):\n",
        "        if (len(inp) != batch_len or len(tar) != batch_len):\n",
        "            print(\"discarded batch\", batch)\n",
        "            continue\n",
        "        train_step(np.expand_dims(inp, axis=0), np.expand_dims(tar, axis=0))\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "        \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, train_loss.result(), train_accuracy.result()))\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss.result(),\n",
        "        'train_accuracy': train_accuracy.result()\n",
        "    }, step=epoch+1)\n",
        "\n",
        "    # validation\n",
        "    if epoch % 5 == 0:\n",
        "        loss_l, acc_l = [], []\n",
        "        for (batch, (inp, tar)) in enumerate(val_b):\n",
        "            val_loss.reset_states()\n",
        "            val_accuracy.reset_states()\n",
        "            \n",
        "            if (len(inp) != batch_len or len(tar) != batch_len):\n",
        "                print(\"discarded batch\", batch)\n",
        "                continue\n",
        "\n",
        "            val_step(np.expand_dims(inp, axis=0), np.expand_dims(tar, axis=0))\n",
        "\n",
        "            loss_l.append(val_loss.result())\n",
        "            acc_l.append(val_accuracy.result())\n",
        "\n",
        "        loss_mean = sum(loss_l)/len(loss_l)\n",
        "        acc_mean = sum(acc_l)/len(acc_l)\n",
        "        print('Epoch {} VALIDATION: Loss {:.4f} Accuracy {:.4f}\\n'.format(epoch + 1, loss_mean, acc_mean))\n",
        "\n",
        "        wandb.log({\n",
        "            'val_loss': loss_mean,\n",
        "            'val_accuracy': acc_mean\n",
        "        }, step=epoch+1)\n",
        "\n",
        "    # generation\n",
        "    if epoch in generate_at:\n",
        "        generate(1)\n",
        "    \n",
        "    print(\"epoch lasted: {}\".format(time.time()-start_time))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 1.1158 Accuracy 0.6192\n",
            "Epoch 1 Batch 50 Loss 1.0749 Accuracy 0.6304\n",
            "Epoch 1 Batch 100 Loss 1.0739 Accuracy 0.6317\n",
            "Epoch 1 Batch 150 Loss 1.0740 Accuracy 0.6319\n",
            "discarded batch 184\n",
            "Epoch 1 Batch 200 Loss 1.0742 Accuracy 0.6313\n",
            "Epoch 1 Batch 250 Loss 1.0731 Accuracy 0.6317\n",
            "Epoch 1 Batch 300 Loss 1.0721 Accuracy 0.6316\n",
            "Epoch 1 Batch 350 Loss 1.0719 Accuracy 0.6319\n",
            "Epoch 1 Batch 400 Loss 1.0719 Accuracy 0.6319\n",
            "Epoch 1 Batch 450 Loss 1.0712 Accuracy 0.6323\n",
            "Epoch 1 Batch 500 Loss 1.0709 Accuracy 0.6324\n",
            "Epoch 1 Batch 550 Loss 1.0698 Accuracy 0.6324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 1 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.070079>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.63223183>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss 1.0701 Accuracy 0.6322\n",
            "Time taken for 1 epoch: 122.40270566940308 secs\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 1 < 21; dropping {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.0209386>, 'val_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.64741755>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "discarded batch 5\n",
            "Epoch 1 VALIDATION: Loss 1.0209 Accuracy 0.6474\n",
            "\n",
            "epoch lasted: 124.76849341392517\n",
            "Epoch 2 Batch 0 Loss 1.0774 Accuracy 0.6397\n",
            "Epoch 2 Batch 50 Loss 1.0667 Accuracy 0.6343\n",
            "Epoch 2 Batch 100 Loss 1.0663 Accuracy 0.6338\n",
            "Epoch 2 Batch 150 Loss 1.0628 Accuracy 0.6351\n",
            "Epoch 2 Batch 200 Loss 1.0630 Accuracy 0.6345\n",
            "Epoch 2 Batch 250 Loss 1.0625 Accuracy 0.6346\n",
            "Epoch 2 Batch 300 Loss 1.0616 Accuracy 0.6350\n",
            "Epoch 2 Batch 350 Loss 1.0618 Accuracy 0.6347\n",
            "Epoch 2 Batch 400 Loss 1.0613 Accuracy 0.6352\n",
            "Epoch 2 Batch 450 Loss 1.0615 Accuracy 0.6353\n",
            "discarded batch 459\n",
            "Epoch 2 Batch 500 Loss 1.0606 Accuracy 0.6355\n",
            "Epoch 2 Batch 550 Loss 1.0603 Accuracy 0.6356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 2 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.0603051>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.635822>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss 1.0603 Accuracy 0.6358\n",
            "Time taken for 1 epoch: 112.0050482749939 secs\n",
            "\n",
            "epoch lasted: 112.0100691318512\n",
            "Epoch 3 Batch 0 Loss 1.0342 Accuracy 0.6484\n",
            "Epoch 3 Batch 50 Loss 1.0489 Accuracy 0.6399\n",
            "Epoch 3 Batch 100 Loss 1.0542 Accuracy 0.6389\n",
            "Epoch 3 Batch 150 Loss 1.0554 Accuracy 0.6374\n",
            "Epoch 3 Batch 200 Loss 1.0551 Accuracy 0.6378\n",
            "Epoch 3 Batch 250 Loss 1.0543 Accuracy 0.6379\n",
            "Epoch 3 Batch 300 Loss 1.0542 Accuracy 0.6381\n",
            "Epoch 3 Batch 350 Loss 1.0538 Accuracy 0.6385\n",
            "Epoch 3 Batch 400 Loss 1.0528 Accuracy 0.6388\n",
            "discarded batch 449\n",
            "Epoch 3 Batch 450 Loss 1.0522 Accuracy 0.6389\n",
            "Epoch 3 Batch 500 Loss 1.0531 Accuracy 0.6386\n",
            "Epoch 3 Batch 550 Loss 1.0529 Accuracy 0.6385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 3 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.0525717>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.6385167>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss 1.0526 Accuracy 0.6385\n",
            "Time taken for 1 epoch: 111.74080038070679 secs\n",
            "\n",
            "epoch lasted: 111.74600172042847\n",
            "Epoch 4 Batch 0 Loss 1.0338 Accuracy 0.6260\n",
            "Epoch 4 Batch 50 Loss 1.0404 Accuracy 0.6411\n",
            "Epoch 4 Batch 100 Loss 1.0413 Accuracy 0.6416\n",
            "Epoch 4 Batch 150 Loss 1.0409 Accuracy 0.6414\n",
            "Epoch 4 Batch 200 Loss 1.0426 Accuracy 0.6405\n",
            "Epoch 4 Batch 250 Loss 1.0442 Accuracy 0.6401\n",
            "Epoch 4 Batch 300 Loss 1.0450 Accuracy 0.6397\n",
            "Epoch 4 Batch 350 Loss 1.0450 Accuracy 0.6396\n",
            "Epoch 4 Batch 400 Loss 1.0451 Accuracy 0.6397\n",
            "Epoch 4 Batch 450 Loss 1.0447 Accuracy 0.6401\n",
            "Epoch 4 Batch 500 Loss 1.0448 Accuracy 0.6403\n",
            "discarded batch 505\n",
            "Epoch 4 Batch 550 Loss 1.0445 Accuracy 0.6403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 4 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.0447161>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.6403439>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss 1.0447 Accuracy 0.6403\n",
            "Time taken for 1 epoch: 111.87646007537842 secs\n",
            "\n",
            "epoch lasted: 111.88142418861389\n",
            "Epoch 5 Batch 0 Loss 1.0726 Accuracy 0.6416\n",
            "discarded batch 31\n",
            "Epoch 5 Batch 50 Loss 1.0428 Accuracy 0.6421\n",
            "Epoch 5 Batch 100 Loss 1.0408 Accuracy 0.6417\n",
            "Epoch 5 Batch 150 Loss 1.0386 Accuracy 0.6427\n",
            "Epoch 5 Batch 200 Loss 1.0395 Accuracy 0.6424\n",
            "Epoch 5 Batch 250 Loss 1.0389 Accuracy 0.6421\n",
            "Epoch 5 Batch 300 Loss 1.0387 Accuracy 0.6424\n",
            "Epoch 5 Batch 350 Loss 1.0385 Accuracy 0.6425\n",
            "Epoch 5 Batch 400 Loss 1.0386 Accuracy 0.6424\n",
            "Epoch 5 Batch 450 Loss 1.0385 Accuracy 0.6424\n",
            "Epoch 5 Batch 500 Loss 1.0379 Accuracy 0.6428\n",
            "Epoch 5 Batch 550 Loss 1.0373 Accuracy 0.6430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 5 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.0376738>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.6428009>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-5\n",
            "Epoch 5 Loss 1.0377 Accuracy 0.6428\n",
            "Time taken for 1 epoch: 112.14959907531738 secs\n",
            "\n",
            "epoch lasted: 112.15409517288208\n",
            "Epoch 6 Batch 0 Loss 1.0506 Accuracy 0.6385\n",
            "Epoch 6 Batch 50 Loss 1.0316 Accuracy 0.6456\n",
            "Epoch 6 Batch 100 Loss 1.0333 Accuracy 0.6445\n",
            "Epoch 6 Batch 150 Loss 1.0347 Accuracy 0.6440\n",
            "Epoch 6 Batch 200 Loss 1.0348 Accuracy 0.6437\n",
            "Epoch 6 Batch 250 Loss 1.0341 Accuracy 0.6441\n",
            "Epoch 6 Batch 300 Loss 1.0341 Accuracy 0.6441\n",
            "Epoch 6 Batch 350 Loss 1.0326 Accuracy 0.6446\n",
            "Epoch 6 Batch 400 Loss 1.0329 Accuracy 0.6442\n",
            "Epoch 6 Batch 450 Loss 1.0322 Accuracy 0.6441\n",
            "discarded batch 473\n",
            "Epoch 6 Batch 500 Loss 1.0318 Accuracy 0.6443\n",
            "Epoch 6 Batch 550 Loss 1.0309 Accuracy 0.6448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 6 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.0309633>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.6446956>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss 1.0310 Accuracy 0.6447\n",
            "Time taken for 1 epoch: 111.85958051681519 secs\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 6 < 21; dropping {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.98757744>, 'val_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.6579963>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "discarded batch 5\n",
            "Epoch 6 VALIDATION: Loss 0.9876 Accuracy 0.6580\n",
            "\n",
            "epoch lasted: 112.25320553779602\n",
            "Epoch 7 Batch 0 Loss 1.0072 Accuracy 0.6465\n",
            "Epoch 7 Batch 50 Loss 1.0247 Accuracy 0.6483\n",
            "Epoch 7 Batch 100 Loss 1.0264 Accuracy 0.6461\n",
            "discarded batch 122\n",
            "Epoch 7 Batch 150 Loss 1.0266 Accuracy 0.6468\n",
            "Epoch 7 Batch 200 Loss 1.0260 Accuracy 0.6471\n",
            "Epoch 7 Batch 250 Loss 1.0255 Accuracy 0.6474\n",
            "Epoch 7 Batch 300 Loss 1.0247 Accuracy 0.6474\n",
            "Epoch 7 Batch 350 Loss 1.0236 Accuracy 0.6477\n",
            "Epoch 7 Batch 400 Loss 1.0245 Accuracy 0.6475\n",
            "Epoch 7 Batch 450 Loss 1.0244 Accuracy 0.6474\n",
            "Epoch 7 Batch 500 Loss 1.0244 Accuracy 0.6471\n",
            "Epoch 7 Batch 550 Loss 1.0242 Accuracy 0.6473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 7 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.0240041>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.64737856>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss 1.0240 Accuracy 0.6474\n",
            "Time taken for 1 epoch: 111.84604954719543 secs\n",
            "\n",
            "epoch lasted: 111.85077905654907\n",
            "Epoch 8 Batch 0 Loss 0.9785 Accuracy 0.6677\n",
            "Epoch 8 Batch 50 Loss 1.0176 Accuracy 0.6501\n",
            "Epoch 8 Batch 100 Loss 1.0177 Accuracy 0.6502\n",
            "Epoch 8 Batch 150 Loss 1.0181 Accuracy 0.6497\n",
            "Epoch 8 Batch 200 Loss 1.0184 Accuracy 0.6490\n",
            "Epoch 8 Batch 250 Loss 1.0185 Accuracy 0.6485\n",
            "Epoch 8 Batch 300 Loss 1.0187 Accuracy 0.6483\n",
            "Epoch 8 Batch 350 Loss 1.0173 Accuracy 0.6491\n",
            "Epoch 8 Batch 400 Loss 1.0176 Accuracy 0.6492\n",
            "Epoch 8 Batch 450 Loss 1.0181 Accuracy 0.6489\n",
            "discarded batch 478\n",
            "Epoch 8 Batch 500 Loss 1.0179 Accuracy 0.6492\n",
            "Epoch 8 Batch 550 Loss 1.0176 Accuracy 0.6493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 8 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.0174501>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.6493182>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss 1.0175 Accuracy 0.6493\n",
            "Time taken for 1 epoch: 111.5713701248169 secs\n",
            "\n",
            "epoch lasted: 111.57617616653442\n",
            "Epoch 9 Batch 0 Loss 0.9999 Accuracy 0.6590\n",
            "Epoch 9 Batch 50 Loss 1.0091 Accuracy 0.6528\n",
            "Epoch 9 Batch 100 Loss 1.0095 Accuracy 0.6527\n",
            "Epoch 9 Batch 150 Loss 1.0084 Accuracy 0.6529\n",
            "Epoch 9 Batch 200 Loss 1.0096 Accuracy 0.6525\n",
            "Epoch 9 Batch 250 Loss 1.0102 Accuracy 0.6519\n",
            "Epoch 9 Batch 300 Loss 1.0104 Accuracy 0.6516\n",
            "Epoch 9 Batch 350 Loss 1.0113 Accuracy 0.6515\n",
            "Epoch 9 Batch 400 Loss 1.0109 Accuracy 0.6515\n",
            "Epoch 9 Batch 450 Loss 1.0117 Accuracy 0.6512\n",
            "Epoch 9 Batch 500 Loss 1.0116 Accuracy 0.6513\n",
            "discarded batch 530\n",
            "Epoch 9 Batch 550 Loss 1.0117 Accuracy 0.6513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 9 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.0118482>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.6513757>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss 1.0118 Accuracy 0.6514\n",
            "Time taken for 1 epoch: 111.37338423728943 secs\n",
            "\n",
            "epoch lasted: 111.37881636619568\n",
            "Epoch 10 Batch 0 Loss 0.9665 Accuracy 0.6658\n",
            "Epoch 10 Batch 50 Loss 1.0075 Accuracy 0.6519\n",
            "Epoch 10 Batch 100 Loss 1.0081 Accuracy 0.6523\n",
            "Epoch 10 Batch 150 Loss 1.0084 Accuracy 0.6527\n",
            "Epoch 10 Batch 200 Loss 1.0087 Accuracy 0.6524\n",
            "discarded batch 243\n",
            "Epoch 10 Batch 250 Loss 1.0075 Accuracy 0.6529\n",
            "Epoch 10 Batch 300 Loss 1.0068 Accuracy 0.6531\n",
            "Epoch 10 Batch 350 Loss 1.0061 Accuracy 0.6535\n",
            "Epoch 10 Batch 400 Loss 1.0061 Accuracy 0.6534\n",
            "Epoch 10 Batch 450 Loss 1.0057 Accuracy 0.6533\n",
            "Epoch 10 Batch 500 Loss 1.0056 Accuracy 0.6534\n",
            "Epoch 10 Batch 550 Loss 1.0059 Accuracy 0.6534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 10 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.0061003>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.6531997>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-6\n",
            "Epoch 10 Loss 1.0061 Accuracy 0.6532\n",
            "Time taken for 1 epoch: 111.52593111991882 secs\n",
            "\n",
            "epoch lasted: 111.53014802932739\n",
            "Epoch 11 Batch 0 Loss 0.9599 Accuracy 0.6665\n",
            "Epoch 11 Batch 50 Loss 1.0053 Accuracy 0.6531\n",
            "Epoch 11 Batch 100 Loss 1.0019 Accuracy 0.6552\n",
            "Epoch 11 Batch 150 Loss 1.0016 Accuracy 0.6549\n",
            "Epoch 11 Batch 200 Loss 1.0013 Accuracy 0.6552\n",
            "Epoch 11 Batch 250 Loss 1.0005 Accuracy 0.6550\n",
            "Epoch 11 Batch 300 Loss 0.9997 Accuracy 0.6555\n",
            "Epoch 11 Batch 350 Loss 0.9995 Accuracy 0.6556\n",
            "Epoch 11 Batch 400 Loss 1.0002 Accuracy 0.6557\n",
            "Epoch 11 Batch 450 Loss 1.0004 Accuracy 0.6555\n",
            "discarded batch 482\n",
            "Epoch 11 Batch 500 Loss 1.0006 Accuracy 0.6555\n",
            "Epoch 11 Batch 550 Loss 1.0001 Accuracy 0.6556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 11 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.0003735>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.6554971>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Loss 1.0004 Accuracy 0.6555\n",
            "Time taken for 1 epoch: 111.37558078765869 secs\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 11 < 21; dropping {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.95761967>, 'val_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.6694462>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "discarded batch 5\n",
            "Epoch 11 VALIDATION: Loss 0.9576 Accuracy 0.6694\n",
            "\n",
            "epoch lasted: 111.7622299194336\n",
            "Epoch 12 Batch 0 Loss 1.0062 Accuracy 0.6540\n",
            "Epoch 12 Batch 50 Loss 0.9967 Accuracy 0.6559\n",
            "Epoch 12 Batch 100 Loss 0.9923 Accuracy 0.6577\n",
            "Epoch 12 Batch 150 Loss 0.9947 Accuracy 0.6573\n",
            "Epoch 12 Batch 200 Loss 0.9958 Accuracy 0.6574\n",
            "Epoch 12 Batch 250 Loss 0.9966 Accuracy 0.6572\n",
            "Epoch 12 Batch 300 Loss 0.9967 Accuracy 0.6571\n",
            "Epoch 12 Batch 350 Loss 0.9957 Accuracy 0.6574\n",
            "Epoch 12 Batch 400 Loss 0.9960 Accuracy 0.6573\n",
            "discarded batch 427\n",
            "Epoch 12 Batch 450 Loss 0.9964 Accuracy 0.6569\n",
            "Epoch 12 Batch 500 Loss 0.9960 Accuracy 0.6571\n",
            "Epoch 12 Batch 550 Loss 0.9952 Accuracy 0.6572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 12 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.9947347>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.65735>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Loss 0.9947 Accuracy 0.6574\n",
            "Time taken for 1 epoch: 111.51752662658691 secs\n",
            "\n",
            "epoch lasted: 111.52249765396118\n",
            "Epoch 13 Batch 0 Loss 1.0138 Accuracy 0.6472\n",
            "Epoch 13 Batch 50 Loss 0.9926 Accuracy 0.6582\n",
            "Epoch 13 Batch 100 Loss 0.9909 Accuracy 0.6588\n",
            "Epoch 13 Batch 150 Loss 0.9926 Accuracy 0.6586\n",
            "Epoch 13 Batch 200 Loss 0.9933 Accuracy 0.6583\n",
            "Epoch 13 Batch 250 Loss 0.9933 Accuracy 0.6580\n",
            "Epoch 13 Batch 300 Loss 0.9933 Accuracy 0.6580\n",
            "discarded batch 323\n",
            "Epoch 13 Batch 350 Loss 0.9920 Accuracy 0.6586\n",
            "Epoch 13 Batch 400 Loss 0.9904 Accuracy 0.6592\n",
            "Epoch 13 Batch 450 Loss 0.9899 Accuracy 0.6593\n",
            "Epoch 13 Batch 500 Loss 0.9900 Accuracy 0.6593\n",
            "Epoch 13 Batch 550 Loss 0.9893 Accuracy 0.6596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 13 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.9889414>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.65959275>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Loss 0.9889 Accuracy 0.6596\n",
            "Time taken for 1 epoch: 111.50726675987244 secs\n",
            "\n",
            "epoch lasted: 111.5122561454773\n",
            "Epoch 14 Batch 0 Loss 1.0072 Accuracy 0.6553\n",
            "Epoch 14 Batch 50 Loss 0.9846 Accuracy 0.6611\n",
            "Epoch 14 Batch 100 Loss 0.9854 Accuracy 0.6603\n",
            "Epoch 14 Batch 150 Loss 0.9837 Accuracy 0.6607\n",
            "Epoch 14 Batch 200 Loss 0.9852 Accuracy 0.6599\n",
            "Epoch 14 Batch 250 Loss 0.9848 Accuracy 0.6603\n",
            "Epoch 14 Batch 300 Loss 0.9842 Accuracy 0.6604\n",
            "discarded batch 322\n",
            "Epoch 14 Batch 350 Loss 0.9851 Accuracy 0.6602\n",
            "Epoch 14 Batch 400 Loss 0.9851 Accuracy 0.6602\n",
            "Epoch 14 Batch 450 Loss 0.9848 Accuracy 0.6605\n",
            "Epoch 14 Batch 500 Loss 0.9845 Accuracy 0.6607\n",
            "Epoch 14 Batch 550 Loss 0.9842 Accuracy 0.6609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 14 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.9838472>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.6610269>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Loss 0.9838 Accuracy 0.6610\n",
            "Time taken for 1 epoch: 111.48043966293335 secs\n",
            "\n",
            "epoch lasted: 111.4853527545929\n",
            "Epoch 15 Batch 0 Loss 0.9659 Accuracy 0.6546\n",
            "Epoch 15 Batch 50 Loss 0.9822 Accuracy 0.6611\n",
            "Epoch 15 Batch 100 Loss 0.9835 Accuracy 0.6607\n",
            "Epoch 15 Batch 150 Loss 0.9796 Accuracy 0.6619\n",
            "Epoch 15 Batch 200 Loss 0.9811 Accuracy 0.6618\n",
            "Epoch 15 Batch 250 Loss 0.9800 Accuracy 0.6626\n",
            "Epoch 15 Batch 300 Loss 0.9802 Accuracy 0.6623\n",
            "Epoch 15 Batch 350 Loss 0.9800 Accuracy 0.6623\n",
            "Epoch 15 Batch 400 Loss 0.9794 Accuracy 0.6627\n",
            "Epoch 15 Batch 450 Loss 0.9796 Accuracy 0.6625\n",
            "Epoch 15 Batch 500 Loss 0.9789 Accuracy 0.6629\n",
            "discarded batch 540\n",
            "Epoch 15 Batch 550 Loss 0.9785 Accuracy 0.6632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 15 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.9785671>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.6633414>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-7\n",
            "Epoch 15 Loss 0.9786 Accuracy 0.6633\n",
            "Time taken for 1 epoch: 111.73923802375793 secs\n",
            "\n",
            "epoch lasted: 111.74346494674683\n",
            "Epoch 16 Batch 0 Loss 0.9936 Accuracy 0.6677\n",
            "Epoch 16 Batch 50 Loss 0.9762 Accuracy 0.6644\n",
            "Epoch 16 Batch 100 Loss 0.9761 Accuracy 0.6638\n",
            "Epoch 16 Batch 150 Loss 0.9763 Accuracy 0.6637\n",
            "Epoch 16 Batch 200 Loss 0.9739 Accuracy 0.6645\n",
            "Epoch 16 Batch 250 Loss 0.9744 Accuracy 0.6643\n",
            "Epoch 16 Batch 300 Loss 0.9734 Accuracy 0.6645\n",
            "Epoch 16 Batch 350 Loss 0.9720 Accuracy 0.6649\n",
            "Epoch 16 Batch 400 Loss 0.9731 Accuracy 0.6648\n",
            "discarded batch 419\n",
            "Epoch 16 Batch 450 Loss 0.9734 Accuracy 0.6650\n",
            "Epoch 16 Batch 500 Loss 0.9730 Accuracy 0.6651\n",
            "Epoch 16 Batch 550 Loss 0.9733 Accuracy 0.6649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 16 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.97342914>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.6647531>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 Loss 0.9734 Accuracy 0.6648\n",
            "Time taken for 1 epoch: 111.37622547149658 secs\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 16 < 21; dropping {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.9343449>, 'val_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.6785314>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "discarded batch 5\n",
            "Epoch 16 VALIDATION: Loss 0.9343 Accuracy 0.6785\n",
            "\n",
            "epoch lasted: 111.76774859428406\n",
            "Epoch 17 Batch 0 Loss 0.9578 Accuracy 0.6820\n",
            "Epoch 17 Batch 50 Loss 0.9669 Accuracy 0.6676\n",
            "Epoch 17 Batch 100 Loss 0.9676 Accuracy 0.6675\n",
            "Epoch 17 Batch 150 Loss 0.9670 Accuracy 0.6683\n",
            "Epoch 17 Batch 200 Loss 0.9677 Accuracy 0.6674\n",
            "Epoch 17 Batch 250 Loss 0.9677 Accuracy 0.6676\n",
            "Epoch 17 Batch 300 Loss 0.9682 Accuracy 0.6673\n",
            "Epoch 17 Batch 350 Loss 0.9686 Accuracy 0.6671\n",
            "Epoch 17 Batch 400 Loss 0.9677 Accuracy 0.6674\n",
            "Epoch 17 Batch 450 Loss 0.9681 Accuracy 0.6673\n",
            "Epoch 17 Batch 500 Loss 0.9676 Accuracy 0.6676\n",
            "Epoch 17 Batch 550 Loss 0.9684 Accuracy 0.6672\n",
            "discarded batch 557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 17 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.9685636>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.667149>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 Loss 0.9686 Accuracy 0.6671\n",
            "Time taken for 1 epoch: 111.366286277771 secs\n",
            "\n",
            "epoch lasted: 111.37135243415833\n",
            "Epoch 18 Batch 0 Loss 0.9735 Accuracy 0.6640\n",
            "Epoch 18 Batch 50 Loss 0.9605 Accuracy 0.6685\n",
            "Epoch 18 Batch 100 Loss 0.9612 Accuracy 0.6692\n",
            "Epoch 18 Batch 150 Loss 0.9628 Accuracy 0.6686\n",
            "Epoch 18 Batch 200 Loss 0.9636 Accuracy 0.6684\n",
            "Epoch 18 Batch 250 Loss 0.9637 Accuracy 0.6681\n",
            "Epoch 18 Batch 300 Loss 0.9644 Accuracy 0.6681\n",
            "Epoch 18 Batch 350 Loss 0.9647 Accuracy 0.6681\n",
            "Epoch 18 Batch 400 Loss 0.9646 Accuracy 0.6682\n",
            "Epoch 18 Batch 450 Loss 0.9638 Accuracy 0.6685\n",
            "Epoch 18 Batch 500 Loss 0.9642 Accuracy 0.6685\n",
            "discarded batch 521\n",
            "Epoch 18 Batch 550 Loss 0.9639 Accuracy 0.6686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 18 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.9635911>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.6688252>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 Loss 0.9636 Accuracy 0.6688\n",
            "Time taken for 1 epoch: 111.23070430755615 secs\n",
            "\n",
            "epoch lasted: 111.23532557487488\n",
            "Epoch 19 Batch 0 Loss 0.9415 Accuracy 0.6926\n",
            "discarded batch 7\n",
            "Epoch 19 Batch 50 Loss 0.9616 Accuracy 0.6712\n",
            "Epoch 19 Batch 100 Loss 0.9603 Accuracy 0.6714\n",
            "Epoch 19 Batch 150 Loss 0.9599 Accuracy 0.6711\n",
            "Epoch 19 Batch 200 Loss 0.9597 Accuracy 0.6704\n",
            "Epoch 19 Batch 250 Loss 0.9590 Accuracy 0.6708\n",
            "Epoch 19 Batch 300 Loss 0.9585 Accuracy 0.6708\n",
            "Epoch 19 Batch 350 Loss 0.9583 Accuracy 0.6711\n",
            "Epoch 19 Batch 400 Loss 0.9590 Accuracy 0.6709\n",
            "Epoch 19 Batch 450 Loss 0.9594 Accuracy 0.6708\n",
            "Epoch 19 Batch 500 Loss 0.9592 Accuracy 0.6709\n",
            "Epoch 19 Batch 550 Loss 0.9586 Accuracy 0.6713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 19 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.95821905>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.67143637>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 Loss 0.9582 Accuracy 0.6714\n",
            "Time taken for 1 epoch: 111.24335265159607 secs\n",
            "\n",
            "epoch lasted: 111.24864912033081\n",
            "Epoch 20 Batch 0 Loss 0.9670 Accuracy 0.6783\n",
            "Epoch 20 Batch 50 Loss 0.9560 Accuracy 0.6740\n",
            "Epoch 20 Batch 100 Loss 0.9562 Accuracy 0.6720\n",
            "Epoch 20 Batch 150 Loss 0.9528 Accuracy 0.6728\n",
            "Epoch 20 Batch 200 Loss 0.9523 Accuracy 0.6729\n",
            "Epoch 20 Batch 250 Loss 0.9530 Accuracy 0.6730\n",
            "Epoch 20 Batch 300 Loss 0.9534 Accuracy 0.6729\n",
            "Epoch 20 Batch 350 Loss 0.9541 Accuracy 0.6728\n",
            "discarded batch 364\n",
            "Epoch 20 Batch 400 Loss 0.9541 Accuracy 0.6729\n",
            "Epoch 20 Batch 450 Loss 0.9535 Accuracy 0.6732\n",
            "Epoch 20 Batch 500 Loss 0.9528 Accuracy 0.6733\n",
            "Epoch 20 Batch 550 Loss 0.9536 Accuracy 0.6732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 20 < 21; dropping {'train_loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.9535437>, 'train_accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.67328715>}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-8\n",
            "Epoch 20 Loss 0.9535 Accuracy 0.6733\n",
            "Time taken for 1 epoch: 111.53292512893677 secs\n",
            "\n",
            "epoch lasted: 111.53683090209961\n",
            "Epoch 21 Batch 0 Loss 0.9256 Accuracy 0.6970\n",
            "Epoch 21 Batch 50 Loss 0.9464 Accuracy 0.6762\n",
            "Epoch 21 Batch 100 Loss 0.9481 Accuracy 0.6753\n",
            "Epoch 21 Batch 150 Loss 0.9489 Accuracy 0.6750\n",
            "Epoch 21 Batch 200 Loss 0.9483 Accuracy 0.6755\n",
            "discarded batch 247\n",
            "Epoch 21 Batch 250 Loss 0.9500 Accuracy 0.6751\n",
            "Epoch 21 Batch 300 Loss 0.9494 Accuracy 0.6752\n",
            "Epoch 21 Batch 350 Loss 0.9493 Accuracy 0.6751\n",
            "Epoch 21 Batch 400 Loss 0.9493 Accuracy 0.6751\n",
            "Epoch 21 Batch 450 Loss 0.9493 Accuracy 0.6750\n",
            "Epoch 21 Batch 500 Loss 0.9492 Accuracy 0.6751\n",
            "Epoch 21 Batch 550 Loss 0.9489 Accuracy 0.6753\n",
            "Epoch 21 Loss 0.9485 Accuracy 0.6754\n",
            "Time taken for 1 epoch: 111.17384266853333 secs\n",
            "\n",
            "discarded batch 5\n",
            "Epoch 21 VALIDATION: Loss 0.9100 Accuracy 0.6879\n",
            "\n",
            "epoch lasted: 111.5509307384491\n",
            "Epoch 22 Batch 0 Loss 0.9889 Accuracy 0.6546\n",
            "Epoch 22 Batch 50 Loss 0.9365 Accuracy 0.6798\n",
            "Epoch 22 Batch 100 Loss 0.9435 Accuracy 0.6773\n",
            "Epoch 22 Batch 150 Loss 0.9432 Accuracy 0.6768\n",
            "Epoch 22 Batch 200 Loss 0.9440 Accuracy 0.6765\n",
            "Epoch 22 Batch 250 Loss 0.9432 Accuracy 0.6771\n",
            "Epoch 22 Batch 300 Loss 0.9428 Accuracy 0.6771\n",
            "Epoch 22 Batch 350 Loss 0.9433 Accuracy 0.6770\n",
            "Epoch 22 Batch 400 Loss 0.9435 Accuracy 0.6770\n",
            "Epoch 22 Batch 450 Loss 0.9430 Accuracy 0.6771\n",
            "Epoch 22 Batch 500 Loss 0.9428 Accuracy 0.6770\n",
            "discarded batch 509\n",
            "Epoch 22 Batch 550 Loss 0.9434 Accuracy 0.6770\n",
            "Epoch 22 Loss 0.9431 Accuracy 0.6770\n",
            "Time taken for 1 epoch: 111.13735818862915 secs\n",
            "\n",
            "epoch lasted: 111.14172673225403\n",
            "Epoch 23 Batch 0 Loss 0.9040 Accuracy 0.6907\n",
            "discarded batch 49\n",
            "Epoch 23 Batch 50 Loss 0.9425 Accuracy 0.6774\n",
            "Epoch 23 Batch 100 Loss 0.9396 Accuracy 0.6790\n",
            "Epoch 23 Batch 150 Loss 0.9431 Accuracy 0.6777\n",
            "Epoch 23 Batch 200 Loss 0.9421 Accuracy 0.6779\n",
            "Epoch 23 Batch 250 Loss 0.9419 Accuracy 0.6776\n",
            "Epoch 23 Batch 300 Loss 0.9413 Accuracy 0.6780\n",
            "Epoch 23 Batch 350 Loss 0.9415 Accuracy 0.6779\n",
            "Epoch 23 Batch 400 Loss 0.9409 Accuracy 0.6780\n",
            "Epoch 23 Batch 450 Loss 0.9402 Accuracy 0.6784\n",
            "Epoch 23 Batch 500 Loss 0.9398 Accuracy 0.6787\n",
            "Epoch 23 Batch 550 Loss 0.9395 Accuracy 0.6787\n",
            "Epoch 23 Loss 0.9393 Accuracy 0.6788\n",
            "Time taken for 1 epoch: 111.17243576049805 secs\n",
            "\n",
            "epoch lasted: 111.17616033554077\n",
            "Epoch 24 Batch 0 Loss 0.9563 Accuracy 0.6671\n",
            "Epoch 24 Batch 50 Loss 0.9388 Accuracy 0.6790\n",
            "Epoch 24 Batch 100 Loss 0.9349 Accuracy 0.6802\n",
            "Epoch 24 Batch 150 Loss 0.9342 Accuracy 0.6807\n",
            "discarded batch 160\n",
            "Epoch 24 Batch 200 Loss 0.9350 Accuracy 0.6802\n",
            "Epoch 24 Batch 250 Loss 0.9361 Accuracy 0.6799\n",
            "Epoch 24 Batch 300 Loss 0.9359 Accuracy 0.6798\n",
            "Epoch 24 Batch 350 Loss 0.9358 Accuracy 0.6799\n",
            "Epoch 24 Batch 400 Loss 0.9358 Accuracy 0.6798\n",
            "Epoch 24 Batch 450 Loss 0.9346 Accuracy 0.6803\n",
            "Epoch 24 Batch 500 Loss 0.9349 Accuracy 0.6800\n",
            "Epoch 24 Batch 550 Loss 0.9344 Accuracy 0.6804\n",
            "Epoch 24 Loss 0.9340 Accuracy 0.6805\n",
            "Time taken for 1 epoch: 111.15505838394165 secs\n",
            "\n",
            "epoch lasted: 111.159010887146\n",
            "Epoch 25 Batch 0 Loss 0.9797 Accuracy 0.6609\n",
            "Epoch 25 Batch 50 Loss 0.9251 Accuracy 0.6834\n",
            "Epoch 25 Batch 100 Loss 0.9267 Accuracy 0.6824\n",
            "Epoch 25 Batch 150 Loss 0.9256 Accuracy 0.6827\n",
            "Epoch 25 Batch 200 Loss 0.9280 Accuracy 0.6821\n",
            "Epoch 25 Batch 250 Loss 0.9277 Accuracy 0.6823\n",
            "Epoch 25 Batch 300 Loss 0.9279 Accuracy 0.6821\n",
            "Epoch 25 Batch 350 Loss 0.9283 Accuracy 0.6819\n",
            "Epoch 25 Batch 400 Loss 0.9289 Accuracy 0.6822\n",
            "Epoch 25 Batch 450 Loss 0.9299 Accuracy 0.6818\n",
            "Epoch 25 Batch 500 Loss 0.9295 Accuracy 0.6820\n",
            "discarded batch 514\n",
            "Epoch 25 Batch 550 Loss 0.9293 Accuracy 0.6823\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-9\n",
            "Epoch 25 Loss 0.9291 Accuracy 0.6824\n",
            "Time taken for 1 epoch: 111.47770237922668 secs\n",
            "\n",
            "epoch lasted: 111.4814932346344\n",
            "Epoch 26 Batch 0 Loss 0.9211 Accuracy 0.6789\n",
            "Epoch 26 Batch 50 Loss 0.9298 Accuracy 0.6828\n",
            "Epoch 26 Batch 100 Loss 0.9224 Accuracy 0.6857\n",
            "Epoch 26 Batch 150 Loss 0.9237 Accuracy 0.6843\n",
            "Epoch 26 Batch 200 Loss 0.9271 Accuracy 0.6833\n",
            "Epoch 26 Batch 250 Loss 0.9247 Accuracy 0.6840\n",
            "Epoch 26 Batch 300 Loss 0.9256 Accuracy 0.6838\n",
            "Epoch 26 Batch 350 Loss 0.9246 Accuracy 0.6840\n",
            "Epoch 26 Batch 400 Loss 0.9250 Accuracy 0.6841\n",
            "Epoch 26 Batch 450 Loss 0.9250 Accuracy 0.6840\n",
            "Epoch 26 Batch 500 Loss 0.9255 Accuracy 0.6839\n",
            "discarded batch 533\n",
            "Epoch 26 Batch 550 Loss 0.9250 Accuracy 0.6841\n",
            "Epoch 26 Loss 0.9246 Accuracy 0.6842\n",
            "Time taken for 1 epoch: 111.11041021347046 secs\n",
            "\n",
            "discarded batch 5\n",
            "Epoch 26 VALIDATION: Loss 0.8927 Accuracy 0.6952\n",
            "\n",
            "epoch lasted: 111.4910056591034\n",
            "Epoch 27 Batch 0 Loss 0.8995 Accuracy 0.6882\n",
            "Epoch 27 Batch 50 Loss 0.9169 Accuracy 0.6851\n",
            "Epoch 27 Batch 100 Loss 0.9166 Accuracy 0.6864\n",
            "Epoch 27 Batch 150 Loss 0.9172 Accuracy 0.6862\n",
            "Epoch 27 Batch 200 Loss 0.9168 Accuracy 0.6860\n",
            "Epoch 27 Batch 250 Loss 0.9191 Accuracy 0.6858\n",
            "Epoch 27 Batch 300 Loss 0.9198 Accuracy 0.6856\n",
            "discarded batch 344\n",
            "Epoch 27 Batch 350 Loss 0.9201 Accuracy 0.6857\n",
            "Epoch 27 Batch 400 Loss 0.9197 Accuracy 0.6857\n",
            "Epoch 27 Batch 450 Loss 0.9197 Accuracy 0.6859\n",
            "Epoch 27 Batch 500 Loss 0.9199 Accuracy 0.6859\n",
            "Epoch 27 Batch 550 Loss 0.9202 Accuracy 0.6857\n",
            "Epoch 27 Loss 0.9198 Accuracy 0.6858\n",
            "Time taken for 1 epoch: 111.08072519302368 secs\n",
            "\n",
            "epoch lasted: 111.08472657203674\n",
            "Epoch 28 Batch 0 Loss 0.8829 Accuracy 0.6901\n",
            "discarded batch 38\n",
            "Epoch 28 Batch 50 Loss 0.9172 Accuracy 0.6858\n",
            "Epoch 28 Batch 100 Loss 0.9175 Accuracy 0.6860\n",
            "Epoch 28 Batch 150 Loss 0.9146 Accuracy 0.6873\n",
            "Epoch 28 Batch 200 Loss 0.9158 Accuracy 0.6869\n",
            "Epoch 28 Batch 250 Loss 0.9164 Accuracy 0.6868\n",
            "Epoch 28 Batch 300 Loss 0.9166 Accuracy 0.6868\n",
            "Epoch 28 Batch 350 Loss 0.9157 Accuracy 0.6870\n",
            "Epoch 28 Batch 400 Loss 0.9161 Accuracy 0.6867\n",
            "Epoch 28 Batch 450 Loss 0.9163 Accuracy 0.6868\n",
            "Epoch 28 Batch 500 Loss 0.9161 Accuracy 0.6870\n",
            "Epoch 28 Batch 550 Loss 0.9156 Accuracy 0.6872\n",
            "Epoch 28 Loss 0.9160 Accuracy 0.6871\n",
            "Time taken for 1 epoch: 111.189199924469 secs\n",
            "\n",
            "epoch lasted: 111.19307136535645\n",
            "Epoch 29 Batch 0 Loss 0.9216 Accuracy 0.6839\n",
            "Epoch 29 Batch 50 Loss 0.9125 Accuracy 0.6886\n",
            "Epoch 29 Batch 100 Loss 0.9090 Accuracy 0.6900\n",
            "Epoch 29 Batch 150 Loss 0.9087 Accuracy 0.6903\n",
            "Epoch 29 Batch 200 Loss 0.9101 Accuracy 0.6894\n",
            "Epoch 29 Batch 250 Loss 0.9101 Accuracy 0.6892\n",
            "Epoch 29 Batch 300 Loss 0.9099 Accuracy 0.6894\n",
            "Epoch 29 Batch 350 Loss 0.9095 Accuracy 0.6896\n",
            "discarded batch 399\n",
            "Epoch 29 Batch 400 Loss 0.9099 Accuracy 0.6893\n",
            "Epoch 29 Batch 450 Loss 0.9102 Accuracy 0.6893\n",
            "Epoch 29 Batch 500 Loss 0.9099 Accuracy 0.6895\n",
            "Epoch 29 Batch 550 Loss 0.9104 Accuracy 0.6895\n",
            "Epoch 29 Loss 0.9111 Accuracy 0.6891\n",
            "Time taken for 1 epoch: 111.11069846153259 secs\n",
            "\n",
            "epoch lasted: 111.11534214019775\n",
            "Epoch 30 Batch 0 Loss 0.9334 Accuracy 0.6702\n",
            "Epoch 30 Batch 50 Loss 0.9003 Accuracy 0.6927\n",
            "Epoch 30 Batch 100 Loss 0.9059 Accuracy 0.6893\n",
            "Epoch 30 Batch 150 Loss 0.9075 Accuracy 0.6888\n",
            "Epoch 30 Batch 200 Loss 0.9080 Accuracy 0.6891\n",
            "Epoch 30 Batch 250 Loss 0.9071 Accuracy 0.6897\n",
            "Epoch 30 Batch 300 Loss 0.9074 Accuracy 0.6899\n",
            "Epoch 30 Batch 350 Loss 0.9078 Accuracy 0.6899\n",
            "Epoch 30 Batch 400 Loss 0.9077 Accuracy 0.6899\n",
            "Epoch 30 Batch 450 Loss 0.9077 Accuracy 0.6899\n",
            "discarded batch 459\n",
            "Epoch 30 Batch 500 Loss 0.9079 Accuracy 0.6899\n",
            "Epoch 30 Batch 550 Loss 0.9074 Accuracy 0.6901\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-10\n",
            "Epoch 30 Loss 0.9075 Accuracy 0.6901\n",
            "Time taken for 1 epoch: 111.39632630348206 secs\n",
            "\n",
            "epoch lasted: 111.39999437332153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "aHHI5hivp5T7",
        "outputId": "fcd272bd-e638-4c96-b3db-1c68947cac84"
      },
      "source": [
        "#@title Parameter persistence\n",
        "\n",
        "transformer.save_weights(\"./optimus_rhyme\")\n",
        "#transformer.load_weights(\"./optimus_rhyme\")\n",
        "\n",
        "\n",
        "#emb_enc_w = transformer.encoder.embedding.get_weights()[0]\n",
        "emb_enc_w = transformer.decoder.embedding.get_weights()[0]\n",
        "print(emb_enc_w.shape)\n",
        "\n",
        "out_v = open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = open('meta.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for num, word in enumerate(poetry_sy_lm_dataset.vocabulary.dictionary):\n",
        "  vec = emb_enc_w[num] # skip 0, it's padding.\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "\n",
        "\n",
        "'''\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir .\n",
        "'''"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1900, 128)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n%load_ext tensorboard\\n%tensorboard --logdir .\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7_wygFNRwgf"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQpP7oiF3iEE",
        "outputId": "23a98b47-4102-46db-9b35-264c59af7ad5"
      },
      "source": [
        "generate(1, k = 3, t = 0.9)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params: k=3, t=0.9\n",
            "e | io | ch’ al | fi| ne | di | tut| t’ i | di| sii|\n",
            "ap| pro| pin| qua| va | sì | com’ | io | do| vea|\n",
            "l’ ar| dor | del | de| si| de| rio in | me | fi| nii|\n",
            "                                          \n",
            " e | l’ al| tro | sie| gno| re e | la | suo | si | sta| schio|\n",
            "e | l’ om| bia| te | ch’ in| na| to | son | per | la| gio|\n",
            "e | la | sua | che | so| spen| te | so| sco| glia|\n",
            "                            \n",
            " ch’ e’ | io | che ’l | for| ciò | di| co| le e | po| sci|\n",
            "e | con | a| ma | che ’l | mio | con| ve| re | dio|\n",
            "che | tu | di| si| ra | con | so| vi| ta| me | che| ra|\n",
            "                                 \n",
            " che | l’ om| bi| glio a | li oc| chi | che i | fa| stros| so|\n",
            "di | mi| so| se | se | la | ca| tu| ra | chia| scoc| ci|\n",
            "con | l’ oc| cor | di | suo | so| lor | che | la | pas| so|\n",
            "                    \n",
            " e | di| ca| tu| ra il | ma| la| va| ta in | si | por| ta|\n",
            "ché | li oc| chi | con | l’ an| cor | si | sa| ra| to!|\n",
            "e | i’ | se’ | co| mi| sciò ’l | pos| se| ne in | sua | fos| so|\n",
            "                    \n",
            " el | cie| tro a| me | so| stro | ch’ i’ | mi | sot| tro|\n",
            "da | la | mon| tri | di| siei | se| gne | da | sco| ci|\n",
            "che | so| li o | di | ca| li al | sua | li a| mor | se| gno|\n",
            "                         \n",
            " sen| tri| ma | che | l’ oc| chi | del | mio | che ’l | mas| so|\n",
            "ch’ i’ | suo | di | ca| sta | se’ | si | fa| co| scos| so|\n",
            "che | l’ a| ve| ni| ta e | co| lui | son | si | fos| so|\n",
            "                   \n",
            " e | io | si | sa| gno a | quan| ci | ch’ i’ | fuo| to|\n",
            "el | co| lo a| vean | che | di | co| stra | che | schia| gi| ro|\n",
            "e | la | mia | dal | son | si | sua ’l | mon| do | scos| so|\n",
            " ch’ i’ | suo | com’ | el| la | che | da | la | mio | co| co|\n",
            "co| mi | con | li oc| chi | di| scor | la | mia | pie|\n",
            "più | di| co| me | dal | so| co | chi | san| gui| no|\n",
            "                              \n",
            " che | si | fa| cea| sto | che | li al| tro | chi | po| sta|\n",
            "e | quel | ch’ i’ | fuor | di | l’ al| la| te | più | chiu| so| la|\n",
            "di | sù | suo | che | più | co| lui | co| stan| ten| do|\n",
            "              \n",
            " che ’n| tel | che | la | co| me | che | li oc| chio| ve|\n",
            "e | che | se| cea| to | che | po| sì | sì | di| ve| re|\n",
            "con | a| scor| te | di | là | che | li oc| chi | mon| do|\n",
            "                            \n",
            " e | com’ | o| v’ io | chio| mi| na | del | cie| ston| do|\n",
            "e | in | per | l’ a| non | sia | per | com’ | io | co| co|\n",
            "e | la | scon| tan| da| min| ciò | che | li oc| co| co?|\n",
            "                         \n",
            " e in| te| ran | suo | da | la | mos| sa i | fac| con| so|\n",
            "ed | el| la | cor| ciò | che | tra | che | sa| co| gno|\n",
            "e | ch’ i’ | tan| to | di| sco| me in | che | fu | sa| to|\n",
            "                          \n",
            " e | la | gra| tan | si | sua | di| sco| sì | che | scon| do|\n",
            "e | dis| s’ a| ve| ra e | a | la| tor | co| sco| cor| so|\n",
            "e | l’ al| tro | san| zan| do | con| ven| do | can| no|\n",
            "                        \n",
            " più | che | la | suo | si | sua | ch’ i’ | fu | per| si|\n",
            "ed | el| li a | lui | ma| ce| re | ch’ al| tra | chiu| so|\n",
            "che | l’ oc| chi | ch’ i’ | mio | si | so| sciò | di| co|\n",
            " del | che | si | se’ | in| ciò | che | tu | so| spi| gli|\n",
            "po| sì ’l | pos| si | di| stan| ti in | per | li oc| ci| ta|\n",
            "e | l’ oc| chia| va | den| no | di | co| me | con| ta|\n",
            "                          \n",
            " di | lui | co| me | che | l’ al| tri| man| do a | ci| glio|\n",
            "el| li al | suoi | con| di | co| sì | si | si | sco| ca|\n",
            "più | a | li | so| cor | ch’ i’ | fac| cia | con | ton| no|\n",
            "                      \n",
            " co| sì | ch’ i’ | mi| co| me | che | si | si | po| sco| sta|\n",
            "co| me | se| gno in | cor| ch’ io | di | ca| spas| si| glio|\n",
            "che | di | la | mi | che | son | sot| tro | che | so| gnio|\n",
            "                 \n",
            " e | che | per| chi | con | son | che | la | sua | pa| sco|\n",
            "e | la | con | si | son | si | fu | son | si | par| ta|\n",
            "e | la | suo | se| cen| de | ch’ eio | co| me | fu | fa| sta|\n",
            "                      \n",
            " e | la | gen| dea | che | so| lo | di | la | mos| sa|\n",
            "che | la | mio | son | co| mi| stra | ch’ io | dis| se| sta|\n",
            "ch’ io | dis| ser| re e | co| me | dal | co| mi| ta| gna|\n",
            "                          \n",
            " che | se | chiu| di e | al| tro | che | sa| vi| so| stro|\n",
            "ch’ i’ | ma| e| stra | del | suo | son | se | so| stro| ta|\n",
            "che | l’ a| ve| ni | ch’ io | dal | sua | si | san| za!|\n",
            "                        \n",
            " che | sot| to | di | su | son | l’ a| mor | del | suo| na|\n",
            "se | che | tu | se’ | co| me | ch’ io | di | se| glio|\n",
            "e | ch’ al| tre e | che | più | che ’l | sol | con| te| so|\n",
            " e | quel| li | suo | ca| tu| ra | se | che | si | par| to|\n",
            "co| sì | che | la | son | sol | mio | se’ | co| lui|\n",
            "co| me | che ’l | ma| re| ga| gna | di | sua | chio|\n",
            "                                   \n",
            " e | la| ci| na | ch’ el| li oc| chi | ch’ i’ | si | pie| so|\n",
            "e | co| sì | con| di| min| ch’ io | io | son | li| ta|\n",
            "e | io | dis| si| na| tu| ra | che | la| sci| gno| gno|\n",
            "                           \n",
            " ch’ io | di | con| d’ io | di| vean | con | li| spos| so|\n",
            "e | che | si | suo | co| lui | che | li a | cas| sa| gno|\n",
            "per | l’ oc| chia| mi| nan| do a| spi| la | sua | san| to|\n",
            "                        \n",
            " e | di | son | l’ a| mos| so in | si | fa | che | so| la|\n",
            "com’ | e| ra | di| vo| lo | che | la | me| re al | par| so|\n",
            "di | fuos| so | che | tu | so| li | che ’l | sem| pa|\n",
            "                           \n",
            " che | l’ al| la | ma| e| gno in | pa| cea| la| ten| do|\n",
            "e ’l | mon| te | che | son’ | io | chi | si| mo | stan| chi|\n",
            "e | la | mon| tro | de’ | io | di | la | sua | fos| so!|\n",
            "                         \n",
            " ch’ i’ | mio | di| si| gri| ma | ciò | a | li al| tron| do|\n",
            "com| pen| sa | chi | fa | la | mi | con| de | car| chi|\n",
            "e | li al| la | mon| do | che | l’ al| tra in | fa| co|\n",
            "                           \n",
            " che | l’ o| gne | la | mi| na | sua | si | fo| cor| ti|\n",
            "se | so| lor | suo | chi | sot| to al | mi | par| ti| no|\n",
            "ciò | la | sua | con | l’ al| tre | di| stran| do| no|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30K9u1Y2A7l8",
        "outputId": "77a1cfc5-35e5-48b7-8766-692e87b7f1b0"
      },
      "source": [
        "generate(1, k = 3, t = 1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params: k=3, t=1\n",
            "e | io | ch’ al | fi| ne | di | tut| t’ i | di| sii|\n",
            "ap| pro| pin| qua| va | sì | com’ | io | do| vea|\n",
            "l’ ar| dor | del | de| si| de| rio in | me | fi| nii|\n",
            "                                          \n",
            " co| me | si | più | di| cea | che | so| sti| mo| gli|\n",
            "pen| si | che | si | so| le a | lui | se| ce| se | fi| co|\n",
            "e | la | con| tri| ma | co| lui a | men| tra | co| sto|\n",
            "                               \n",
            " del | ciel | co| me | se | li oc| chi | di | lun| go|\n",
            "che | se | so| li| co| me | di | la| gria| co| me|\n",
            "e | per | l’ al| tro al | con| te | la | mio | ca| lui | ca| so| sto|\n",
            "                        \n",
            " co| sì | dal | con| d’ i’ | mia | sua | ch’ io | fu | pa| schia|\n",
            "co| me | so| lor | di | la | sua | che in | chiu| so|\n",
            "e | che ’l | mi| che | di i | fuor | di| cel | pa| li| zia|\n",
            "                    \n",
            " e ’l | mio | di | sé | suo | che ’l | par| ti | ca| pos| si|\n",
            "co| mi | ch’ ei | ciò | da | li | fos| si| ra| ver| si|\n",
            "e | io | co| me | da| ve| re | che | si | fos| si|\n",
            "                               \n",
            " co| me | di | sé | lar | co| me | sia | co| sciu| ni!|\n",
            "che | so| la| va | del | cor | l’ al| tra | suo | fos| si|\n",
            "pe| rò | co| san| to | ch’ io | dis| s’ io | si | scos| si?|\n",
            "                        \n",
            " dis| sie| ti | che ’n | la | son| der | che | si | stas| si|\n",
            "el | po| cor | chi | pos| si | con | si | con | l’ ar| ti| nic| cian | si | co| min | ch’ i’ | fo| cia|\n",
            "                                  \n",
            " ci | si | co| me in | suo | di | la | ma | che | pa| sti|\n",
            "che | l’ a| ni| spe| re | di | si | fu | se| gni| ni|\n",
            "pos| sa | di | si | suo | da | quel | si | sot| ti| ni|\n",
            " che | li al| li oc| ciò | com’ | e’ | mo| te al | se| gno|\n",
            "che | l’ ac| ciò | che | se | l’ al| tri| mo in | te| sta|\n",
            "e | quan| do | com’ | è ’l | fac| ciò | co| mi | chiu| ca| sca|\n",
            "                  \n",
            " ch’ i’ | mi | fu | che | la| te | si | si | son| chio|\n",
            "per | che | sa| gio| le | co| me | che | li a| mor| so|\n",
            "ch’ al | fi| chio | di | l’ ac| ciò | da | li | mon| ta|\n",
            "                               \n",
            " pia| scia| vea| stro | che | si | fuor | ch’ al| tri|\n",
            "e | co| me | se| cor | con| tro | ch’ io | di | con| to|\n",
            "ch’ i’ | mon| ta| to in| gui| ta | ciò | la | men| do|\n",
            "                                 \n",
            " e | che | si | fuor | di | l’ a| vea | di | li | pos| so| sca|\n",
            "co| sì | di| vi| di | che | si | fos| so a | la | fi| ga|\n",
            "e ’l | mi | son | co| lui | com’ | io | sot| ta| ri|\n",
            "                         \n",
            " co| men| d’ io | mi | co| sì | ch’ el | fat| te | strat| to|\n",
            "che | sot| tro al | so| le | ciò | di| vi | sot| to|\n",
            "e | l’ al| tro | si | fui | co| stra| ni| sta| gna|\n",
            "                                 \n",
            " ch’ i’ | ch’ i’ | mia | co| me a | ch’ io | mi | po| ta|\n",
            "che | son | son | si | sa| va| sto | ch’ in | fo| ca| sta| ta|\n",
            "ché | ciò | di| stra| mi al | co| lui | che | san| dor | mio|\n",
            "                 \n",
            " con | la | ma| co| me in | se| ra| ti al| tron | sco| sca|\n",
            "che | l’ al| tra | suo| no i | dis| s’ i’ | mor | soc| ca| ta|\n",
            "che | son | l’ om| bri| stan| zo | che | si | fac| ca|\n",
            " e | che | par | li oc| che | l’ o| ver| ca in| ch’ a| scoc| cio|\n",
            "che | li al| li al | co| me | di| cel| la | schio|\n",
            "se | l’ oc| chia| stro| noc| ciò | di | qui ’l | mon| da|\n",
            "                          \n",
            " e | li | co| lui | com’ | i’ | mi | com’ | in| te | po| ro|\n",
            "che | di | son | la | sua | dis| sen | lor | si | so| la|\n",
            "e | co| sì | che | pos| si| ca | si | sua | co| sca|\n",
            "                            \n",
            " di | l’ oc| quei | di | li | sol | che | sia| van| da| to|\n",
            "e | da | ch’ el| la | di| scon| te | di | so| le| gna|\n",
            "co| lui | com’ | a| mo| le | che ’l | su| bi| na|\n",
            "                                   \n",
            " e ’l | so| spen| se| gno | co| me a | li an| co| lo|\n",
            "ch’ e| ra| mi al| tro | di | l’ a| ni| ma| ra | co| cor| to|\n",
            "e | chi | co| m’ e| ra | si | fa | co| min | co| sto|\n",
            "                               \n",
            " e | che | l’ o| ma| te | di| ce| re e | di | suo | sco| co|\n",
            "ch’ io | si | fu | ciò | dis| s’ al| li| man| da| ro|\n",
            "e | la | co| sa| re | di | lui | sia | chi | per| co|\n",
            "                               \n",
            " la| cor | la | giu| no in | lun| to | di| ciò | con| chia|\n",
            "e | li oc| ch’ in| na| ver| so | de | l’ al| to| stra| na|\n",
            "che | so| stra| mi| na| me in | sol | si | pas| sa| gna|\n",
            "                        \n",
            " po| sì ’l | man| da| va | ch’ io | so| stro | so| spor| si|\n",
            "e | del | co| sì | di | sù | se| gno | chi | fac| ci|\n",
            "che ’l | mos| si a | con | al | ma| e| stri | fu | pia| ta|\n",
            " el | mio | che | la | sof| fo| co | che | si ’l | sor| te|\n",
            "dis| se | l’ al| tro | del | mio | de | la | mo| scia|\n",
            "pos| sa| re | chi | po| ce| ra | di| sco| ra| mi| st|\n",
            "                                \n",
            " co| sì | co| me | che | l’ o| gne | so| scia| to|\n",
            "e | con | sa| vie|\n",
            "ch’ io | men| te il | mia | co| schia | che | son | fa| co | d’ io | in | for| ti | sa| scor| te|\n",
            "                                \n",
            " co| mi | di| ma| ri| ma | da | la | suo | ch’ i’ | fa| ce|\n",
            "che | la | sua | si | sua | la | mon| der | che | san| no|\n",
            "e | la | sel | che | l’ o| ven | l’ al| trit| ti| gna|\n",
            "                          \n",
            " ché | la | men| tro a| mor| te | con| d’ i’ | mo| le| tra!|\n",
            "e | la | mi| chi a| vo| lar | da | que| sta | chia| stra|\n",
            "e | con | la | mio | di| cen| tri| min | che | fac| ca|\n",
            "                         \n",
            " di| scor| na e | l’ al| ti| na| re a | la | scon| ta|\n",
            "e | di| cu| na e | del | con | si | son | so| la|\n",
            "co| me ’l | ma| cia il | suo | di| sci| so | di| sciu| no|\n",
            "                                    \n",
            " di | suo | si | ciò | che | l’ al| li | com’ | mio | fan| no|\n",
            "che ’l | pi| se | co| min| di | con | si | fos| sai|\n",
            "per| ché | di| sco| sì | l’ om| bi| ne | che | si| co|\n",
            "                             \n",
            " che | la | sua | che | so| scia| ri| men| do | ca| sco| sa|\n",
            "el| li | si | par| te in | pen| si | so| len| za|\n",
            "e | qual | so| stri | chi | san| zia| ro| to in | fuor| to|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63a9HOhJAhCL",
        "outputId": "48c16b98-786a-4c77-92e7-0da1fd497f18"
      },
      "source": [
        "generate(1, k = 5, t = 1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params: k=5, t=1\n",
            "e | io | ch’ al | fi| ne | di | tut| t’ i | di| sii|\n",
            "ap| pro| pin| qua| va | sì | com’ | io | do| vea|\n",
            "l’ ar| dor | del | de| si| de| rio in | me | fi| nii|\n",
            "                                          \n",
            " che | di | cu| no | può | dal | suo | la | sco| co| gni?|\n",
            "den| na| vi| tà | co| sì | che | di| man| tra | co| gliu| na|\n",
            "sì | com’ | i’ | più | l’ ac| ciò | dal | di| com’ | al| ma|\n",
            "                   \n",
            " se | che | pri| sti| rin | sì | ch’ el | mon| tro | col| le|\n",
            "e | l’ al| lor | co| men| si | sot| tri e ’l | san| do|\n",
            "e | che | si | pri| ma | co| me e | tu | di | si | fal| la|\n",
            "                      \n",
            " e | la | scas| si | die| co| lo | di| ce | sue | chian| ta|\n",
            "la | che | li oc| chi ’l | mi | col | si | pro| tel| la|\n",
            "che | de’ | la | sco| tro a | ma | sia | se | lui | fa| co| glia|\n",
            "                \n",
            " con| di| stre | che | si | pri| ta | ch’ i’ | ten | fio|\n",
            "e| ran| no in | fa| cian | d’ in| no| stra| noc| co| sco|\n",
            "che | la | per| che | lui | par| gi| so | sca| gnes| si|\n",
            "                           \n",
            " del | ca| la | scan| nar | di| ma e | ch’ el | fac| co|\n",
            "sì | se | l’ ac| ce| stu| ri a | col| lo | son| cor| ta|\n",
            "pur | di| cen| sie| na| rà | la | giù | chi | si | ri| glia|\n",
            "                         \n",
            " ch’ i’ | po| co| min | so| spec| chi | mi | ciòl| la| va|\n",
            "che | den| der | co| me | più | che | tan| te | pes| si|\n",
            "per | lo | son | con | poi | del | fuor | chiu| n’ es| si|\n",
            "                         \n",
            " di | quel | mio | del | cies| so i | ci| per | si | fu| ca|\n",
            "se | che | di| ma| gia e ’l | fuor | sue | si | fos| siei|\n",
            "col | sot| ti| na | dis| s’ i’ | ma| li al| trit| ta|\n",
            " che | se| gn’ io | sì | co| sì | che | per | che ’l | so| spar| te|\n",
            "de | lor | più | che | l’ or | si | ciò | la| vac| cia|\n",
            "che | tut| ti | mo| co e | la | ca| stre | lo | scri| gna|\n",
            "                 \n",
            " col | mol | mol| tra | si | fa| co an| co| me | chiu| so|\n",
            "e | lo | chi | fian| chi al | por| te | dal | cin| ci| to|\n",
            "e ’l | mol| tra | se| mo| ro| stro ’l | mi| so| spa| gna|\n",
            "                        \n",
            " ch’ io | sì | sa | ma | ché | sì | che ’l | cin| con| to|\n",
            "dis| se in| de| gno | di | ma| van| ne | li ’m| per| se|\n",
            "per | che ’l | mi | che | la | mon| te e | a | ca| gna!|\n",
            "                           \n",
            " ch’ el| li | pas| si| cor | par| tea | di| sciar | sor| to?|\n",
            "co| lan| di e | a | l’ un | sua | suo| ciò | chio | so| ra|\n",
            "com’ | in | sa| sco| sì | pos| se| gno in | tre| to| sto|\n",
            "                    \n",
            " e | san| to | li | fian| da| tor | suo | da | mo| te| gna|\n",
            "e | cui | co| me | ch’ io | co| men| so | di | vi| to|\n",
            "la | po| ta| ti ed | el| le| gno il | con | pos| si| gno|\n",
            "                           \n",
            " e | im| pen| si| ti| rio in | ciel | poi | di| sce| sto|\n",
            "che | li oc| chi | co| schi | sa| rin| go| se an | cuol| vel| la|\n",
            "di| spur | da | l’ om| bra| te | si | co| m’ in| cor| si?|\n",
            "                 \n",
            " di | mio | com’ | san| zan| do | se | lo| go| le | fo| co|\n",
            "che | si | mia | piè | la | mon| di| chi| nir| mi| to|\n",
            "sì | si | si | fen | don| cor | che ’l | car| de| ren| no|\n",
            " e ’r | suo| lo | com’ | i| spen| tro | più | a| mos| sa?|\n",
            "che | la | chiai | per | di | là | par| lar | sot| ta|\n",
            "di | con| d’ i’ | con| ve| dean | l’ uom | son | schia| sco| si|\n",
            "                      \n",
            " e | io ’n| co| sì | che | li | sua | mia | san| ten| ta|\n",
            "co| sì | per| che | sem| pre| se al | mo| schio | sai| va|\n",
            "e | do| giu| na | che | li oc| chi a | co| sì | ch’ a| scen| si?|\n",
            "                 \n",
            " e | la | sel| la e | piùi | che | sai | più | si | pas| si|\n",
            "che ’l | vol| ge| re i | che | sì | mos| si | scie| gno|\n",
            "che | più | a | con | tut| to| ran| di | con | so| lui|\n",
            "                          \n",
            " e | i’ | foc| cio | dol | che | tu | per | da | lui | co| stri| si|\n",
            "per | lo ’n| fac| cia al | por| ga | se| gno|\n",
            "e | im| mo| sciò | che | più | si | reg| gio | fac| co?|\n",
            "                            \n",
            " co| sì | pos| si | cin| ce| ne| scheg| gi| to | sen| no|\n",
            "e | là | di | suo | sof| fo| str’ om| bra| va| let| ta|\n",
            "co| sì | ch’ i’ | mi| no | sì | san| za| tra | scio| le|\n",
            "                             \n",
            " ed | io | più | l’ el | mi | san| zar | den| tra in| guar| se|\n",
            "di | ci| ven| ne| ri| co| min | con| ta | che ’n | li | pa| schia| ta| ta| tan | de ’l | mon| ce| se!|\n",
            "                                 \n",
            " el | son | pa| ren| de a | la | con| ciò | l’ in| gui| te|\n",
            "e | lo | giù | ca| se e | la| sco | se | san| cor| ta|\n",
            "e | io | da | me | che ’n| tra | di| ver | la | sce| sce|\n",
            " e | lo | ch’ e’ | ciel | co| lui | ch’ io ’ | ven| si| glia|\n",
            "di| cor | la | sol | per | ch’ io | noi | fac| cia|\n",
            "sì | che | di| si| co| se| ro | che ’l | fo| co al| tre| sto|\n",
            "                         \n",
            " e | dal | mi| co| me | si | fa| scio | si | che ’l | vie|\n",
            "den| d’ i’ | mia | me ’n| tren| der | la | suol | co| co|\n",
            "po| com’ | io | pro| ces| se | sof| fa | cor | s’ a| va| la|\n",
            "                      \n",
            " là | è | che | per | de| sco| lui | se| mar | soc| co|\n",
            "e | che | piè | per | cui | d’ io | di | son| der| so|\n",
            "del | mai | che ’l | se | dal | di | man| di| ciò ’ | so| co|\n",
            "                           \n",
            " e | in| ti| co | di | lo | con| d’ io | io | se| gno| gno|\n",
            "d’ i| ni| cu| lo ’l | ch’ io | cor | ta| le | sen| de|\n",
            "che | di | se’ | so| ve| ro e | di| cor| nar | se| so|\n",
            "                              \n",
            " di | com’ | el | fe| fes| sa e | la | fi| no | ch’ el| le|\n",
            "che | la | fat| tal| la | mia | per | ch’ è | trop| pra| ten| ta|\n",
            "por | su | sel | pos| se | sì | di | sac| quel | san| te|\n",
            "                \n",
            " e | an| cor | suo | del | mio | ch’ e’ | se | fu | pas| so?|\n",
            "el | che ’l | ver| ser | con| vie| de | sem | com’ | ste| n’ in| fi| glio | di | sem| pro|\n",
            "                                               \n",
            " co| me in | lui | con | la | son | con | pur | di | fat| to|\n",
            "ce| se | più | ch’ io | se’ | son| nan| ciò | chiul| le|\n",
            "e | quin| co | di | là | che | là | po| sì e | ch’ a| mo|\n",
            "\n"
          ]
        }
      ]
    }
  ]
}