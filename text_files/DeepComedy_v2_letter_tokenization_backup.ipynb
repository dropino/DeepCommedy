{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAwbk6Au6JCU",
    "outputId": "f6003b15-4137-4dc8-85e6-b58e924148d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPzlEKUbPIIE",
    "outputId": "287d9940-518e-4a19-e578-41967b8b9ab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "2.6.0\n",
      "Mon Sep 20 10:49:45 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "#@title Import & seed\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import requests\n",
    "import collections\n",
    "import pickle\n",
    "import copy, random\n",
    "import nltk as nl\n",
    "nl.download('punkt')\n",
    "from itertools import zip_longest\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Reshape, BatchNormalization, Dense, Dropout, concatenate,\n",
    "    Embedding, LSTM, Dense, GRU, Bidirectional, Add\n",
    ")\n",
    "from tensorflow.keras.activations import elu, relu, softmax, sigmoid\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "#Variables\n",
    "FILENAME = \"divineComedyProf.txt\"\n",
    "CLEANFILENAME = \"cleanComedyProf.txt\"\n",
    "INFERNO = \"inferno_syllnew.txt\"\n",
    "PURGATORIO = \"purgatorio_syllnew.txt\"\n",
    "PARADISO = \"paradiso_syllnew.txt\"\n",
    "PARADISOCLEAN = \"paradiso_clean.txt\"\n",
    "\n",
    "\n",
    "vocab_size = 1900\n",
    "terces_per_batch = 3\n",
    "terces_len = 200\n",
    "batch_len = terces_per_batch * (terces_len + 1)\n",
    "EPOCHS = 101\n",
    "\n",
    "\n",
    "#Setup\n",
    "print(tf.__version__)\n",
    "\n",
    "#np.random.seed(1234)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bkfS7t6vpYIJ",
    "outputId": "9176ddde-7b89-4414-a0a4-6d527bcce9b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.12.2-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 5.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
      "Collecting subprocess32>=3.5.3\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 3.8 MB/s \n",
      "\u001b[?25hCollecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
      "\u001b[K     |████████████████████████████████| 180 kB 42.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 48.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
      "Collecting configparser>=3.8.1\n",
      "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
      "Collecting yaspin>=1.0.0\n",
      "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
      "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
      "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
      "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
      "Building wheels for collected packages: subprocess32, pathtools\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=44bbafd893c88b272a944f80c6cd11dd1b0fe3ab3d11efcc27b8ee5f7434ee6f\n",
      "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=1f1ad9288cd8dcb8212451e4d8fdd8b9983c81ee5847c3d51c693a15f2a99d05\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
      "Successfully built subprocess32 pathtools\n",
      "Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
      "Successfully installed GitPython-3.1.24 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.12.2 yaspin-2.1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdropino\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">bananito96</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/dropino/DeepComedy\" target=\"_blank\">https://wandb.ai/dropino/DeepComedy</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/dropino/DeepComedy/runs/x48b4mi2\" target=\"_blank\">https://wandb.ai/dropino/DeepComedy/runs/x48b4mi2</a><br/>\n",
       "                Run data is saved locally in <code>/content/wandb/run-20210920_105000-x48b4mi2</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(x48b4mi2)</h1><iframe src=\"https://wandb.ai/dropino/DeepComedy/runs/x48b4mi2\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0669416650>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Setup wandb\n",
    "!pip install wandb\n",
    "!wandb login e6569c556c797b5bed38bc6cba40d24b68e8973d\n",
    "import wandb\n",
    "wandb.init(project=\"DeepComedy\", name=\"bananito96\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrEohsVOSEg2"
   },
   "source": [
    "#Clean files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "CczzJREJ0-Y4",
    "outputId": "d56d1bc5-3308-4f48-df98-1acf9d472404"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n#count chars\\nchar_list = []\\nwith open(FILENAME) as file:\\n  while True:\\n    char = file.read(1)\\n    if not char:\\n      print(\"End of file\")\\n      break\\n    char = char.lower()\\n\\n    #add good char to char list\\n    if char not in char_list:\\n      char_list.append(char)\\n\\nprint(char_list)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#count chars\n",
    "char_list = []\n",
    "with open(FILENAME) as file:\n",
    "  while True:\n",
    "    char = file.read(1)\n",
    "    if not char:\n",
    "      print(\"End of file\")\n",
    "      break\n",
    "    char = char.lower()\n",
    "\n",
    "    #add good char to char list\n",
    "    if char not in char_list:\n",
    "      char_list.append(char)\n",
    "\n",
    "print(char_list)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "_0tniPNkO1S4",
    "outputId": "797bf01a-8920-4692-a257-eace6c58c9b0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nremovable_chars = [\\'•\\', \\'—\\', \\'-\\', \\'(\\', \\')\\', \\',\\', \\'.\\', \\':\\', \\';\\', \\'“\\', \\'”\\', \\'«\\', \\'»\\', \\'\"\\',\\'0\\',\\'1\\',\\'2\\',\\'3\\',\\'4\\',\\'5\\',\\'6\\',\\'7\\',\\'8\\',\\'9\\']\\n#‘ e ’ usate per racchiudere parti parlate in latino\\n#“ e ” racchiude il parlato sia in italiano che latino\\n#idem « e »\\n\\n\\nclean_char_list = []\\nremovedCharsCounter = 0\\nfileOut = open(CLEANFILENAME, \"a\")\\n\\nwith open(FILENAME) as file:\\n  while True:\\n    char = file.read(1)\\n    if not char:\\n      print(\"End of file\")\\n      break\\n\\n    #transform upper case to lower case\\n    char = char.lower()\\n\\n    #remove unwanted chars\\n    if char not in removable_chars:\\n\\n      #add acceptable chars to char list\\n      if char not in clean_char_list:\\n        clean_char_list.append(char)\\n      \\n      #output in new file\\n      fileOut.write(char)\\n    else:\\n      removedCharsCounter += 1\\n\\n\\nprint(char_list)\\nprint(removedCharsCounter)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "removable_chars = ['•', '—', '-', '(', ')', ',', '.', ':', ';', '“', '”', '«', '»', '\"','0','1','2','3','4','5','6','7','8','9']\n",
    "#‘ e ’ usate per racchiudere parti parlate in latino\n",
    "#“ e ” racchiude il parlato sia in italiano che latino\n",
    "#idem « e »\n",
    "\n",
    "\n",
    "clean_char_list = []\n",
    "removedCharsCounter = 0\n",
    "fileOut = open(CLEANFILENAME, \"a\")\n",
    "\n",
    "with open(FILENAME) as file:\n",
    "  while True:\n",
    "    char = file.read(1)\n",
    "    if not char:\n",
    "      print(\"End of file\")\n",
    "      break\n",
    "\n",
    "    #transform upper case to lower case\n",
    "    char = char.lower()\n",
    "\n",
    "    #remove unwanted chars\n",
    "    if char not in removable_chars:\n",
    "\n",
    "      #add acceptable chars to char list\n",
    "      if char not in clean_char_list:\n",
    "        clean_char_list.append(char)\n",
    "      \n",
    "      #output in new file\n",
    "      fileOut.write(char)\n",
    "    else:\n",
    "      removedCharsCounter += 1\n",
    "\n",
    "\n",
    "print(char_list)\n",
    "print(removedCharsCounter)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "lxF6HYN5ANpF",
    "outputId": "541395e5-9115-4be7-baf4-7c128f310dc3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nf = open(CLEANFILENAME, \"r\")\\nlines = f.readlines()\\n\\nnumber_of_syllables = []\\nline_counter = 1\\n\\nfor line in lines:\\n  #excludes all empty lines and canto intros\\n  if \\'|\\' in line:\\n    syllables = line.count(\\'|\\')\\n    number_of_syllables.append(syllables)\\n    if syllables != 11:\\n      print(\"line number {} with text {} has {} syllables\".format(line_counter,line,syllables))\\n    line_counter += 1\\n\\nCounter(number_of_syllables)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "f = open(CLEANFILENAME, \"r\")\n",
    "lines = f.readlines()\n",
    "\n",
    "number_of_syllables = []\n",
    "line_counter = 1\n",
    "\n",
    "for line in lines:\n",
    "  #excludes all empty lines and canto intros\n",
    "  if '|' in line:\n",
    "    syllables = line.count('|')\n",
    "    number_of_syllables.append(syllables)\n",
    "    if syllables != 11:\n",
    "      print(\"line number {} with text {} has {} syllables\".format(line_counter,line,syllables))\n",
    "    line_counter += 1\n",
    "\n",
    "Counter(number_of_syllables)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnAp1FL5LbYO"
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "\"\"\"Once tokenized the dataset, a map of the syllables with the integer index is created. The final dimension\n",
    "of the vocabulary is 1874 tokens but it is limited to 1800 to remove the tail of infrequent syllables.\n",
    "\n",
    "We substituted every space between words with the special token < SEP > and inserted at the beginning\n",
    "of each verse the token < GO >. To make all verses the same lengths, we used the special character\n",
    "“< PAD >” to pad every terces to the length of 75 tokens. At the end of each verse we appended the\n",
    "symbol “< EOV > “, while at the end of each sentence “< EOS >”.\"\"\"\n",
    "\n",
    "number_of_syllables = []\n",
    "line_counter = 1\n",
    "\"\"\"\n",
    "space -> <SPA>\n",
    "verst start -> <VST>\n",
    "padding (up to 75) -> <PAD>\n",
    "end of verse -> <EOV>\n",
    "end of sentence -> <EOS>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A1mcovxWPadi",
    "outputId": "9022ae84-33f7-4c9d-8826-02fc05fcde24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating_vocabulary\n",
      "Train size: 4649\n",
      "Val size: 47\n",
      "Test size: 48\n",
      "[45, 8, 4, 10, 3, 1, 47, 17, 4, 26, 1, 47, 26, 7, 3, 1, 47, 14, 4, 10, 3, 1, 47, 13, 5, 17, 1, 47, 17, 6, 8, 3, 1, 47, 14, 6, 3, 1, 47, 8, 7, 1, 47, 12, 11, 9, 5, 3, 1, 47, 19, 6, 1, 47, 11, 5, 1, 49, 17, 6, 3, 1, 47, 9, 6, 1, 47, 11, 9, 7, 1, 47, 19, 5, 6, 3, 1, 47, 18, 4, 9, 3, 1, 47, 16, 1, 47, 8, 5, 3, 1, 47, 12, 4, 10, 1, 47, 19, 5, 3, 7, 1, 47, 12, 13, 16, 1, 47, 9, 5, 1, 49, 13, 22, 31, 3, 1, 47, 10, 5, 3, 1, 47, 14, 6, 1, 47, 9, 6, 11, 1, 47, 11, 5, 3, 1, 47, 19, 6, 5, 3, 1, 47, 4, 1, 47, 9, 5, 3, 1, 47, 12, 17, 5, 9, 1, 47, 9, 6, 1, 47, 11, 5, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49, 45, 5, 22, 6, 3, 1, 47, 24, 16, 5, 8, 1, 47, 11, 7, 3, 5, 3, 1, 47, 14, 6, 9, 3, 1, 47, 24, 16, 5, 10, 3, 1, 47, 4, 1, 47, 9, 5, 3, 30, 3, 1, 47, 13, 7, 1, 47, 12, 5, 3, 1, 47, 14, 16, 1, 47, 9, 5, 1, 49, 4, 1, 47, 12, 11, 5, 3, 1, 47, 12, 4, 10, 1, 47, 19, 5, 3, 1, 47, 12, 4, 10, 1, 47, 19, 5, 21, 1, 47, 21, 6, 5, 3, 4, 3, 1, 47, 5, 1, 47, 12, 18, 9, 5, 3, 4, 3, 1, 47, 23, 7, 9, 1, 47, 11, 4, 1, 49, 13, 22, 4, 3, 1, 47, 8, 4, 10, 3, 1, 47, 18, 4, 8, 1, 47, 12, 6, 4, 9, 3, 1, 47, 9, 6, 1, 47, 8, 7, 1, 47, 19, 5, 3, 1, 47, 10, 5, 3, 1, 47, 18, 5, 1, 47, 16, 1, 47, 9, 5, 35, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49, 45, 11, 5, 8, 1, 47, 11, 20, 3, 30, 3, 1, 47, 5, 1, 47, 17, 5, 1, 47, 9, 5, 3, 1, 47, 13, 22, 4, 3, 1, 47, 18, 7, 1, 47, 13, 7, 3, 30, 3, 1, 47, 18, 6, 28, 3, 1, 47, 17, 7, 9, 1, 47, 11, 4, 1, 49, 17, 5, 3, 1, 47, 18, 4, 9, 3, 1, 47, 11, 9, 5, 11, 1, 47, 11, 5, 9, 3, 1, 47, 14, 4, 10, 3, 1, 47, 25, 4, 8, 3, 1, 47, 13, 22, 20, 3, 6, 20, 3, 1, 47, 19, 6, 3, 1, 47, 11, 9, 7, 1, 47, 19, 5, 6, 1, 49, 14, 6, 1, 47, 9, 29, 3, 1, 47, 14, 4, 3, 1, 47, 10, 20, 3, 5, 10, 1, 47, 11, 9, 4, 3, 1, 47, 13, 7, 1, 47, 12, 4, 3, 1, 47, 13, 22, 20, 3, 6, 20, 3, 1, 47, 19, 20, 3, 22, 7, 3, 1, 47, 12, 13, 7, 9, 1, 47, 11, 4, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49]\n",
      "[45, 8, 4, 10, 3, 1, 47, 17, 4, 26, 1, 47, 26, 7, 3, 1, 47, 14, 4, 10, 3, 1, 47, 13, 5, 17, 1, 47, 17, 6, 8, 3, 1, 47, 14, 6, 3, 1, 47, 8, 7, 1, 47, 12, 11, 9, 5, 3, 1, 47, 19, 6, 1, 47, 11, 5, 1, 49, 17, 6, 3, 1, 47, 9, 6, 1, 47, 11, 9, 7, 1, 47, 19, 5, 6, 3, 1, 47, 18, 4, 9, 3, 1, 47, 16, 1, 47, 8, 5, 3, 1, 47, 12, 4, 10, 1, 47, 19, 5, 3, 7, 1, 47, 12, 13, 16, 1, 47, 9, 5, 1, 49, 13, 22, 31, 3, 1, 47, 10, 5, 3, 1, 47, 14, 6, 1, 47, 9, 6, 11, 1, 47, 11, 5, 3, 1, 47, 19, 6, 5, 3, 1, 47, 4, 1, 47, 9, 5, 3, 1, 47, 12, 17, 5, 9, 1, 47, 9, 6, 1, 47, 11, 5, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49, 45, 5, 22, 6, 3, 1, 47, 24, 16, 5, 8, 1, 47, 11, 7, 3, 5, 3, 1, 47, 14, 6, 9, 3, 1, 47, 24, 16, 5, 10, 3, 1, 47, 4, 1, 47, 9, 5, 3, 30, 3, 1, 47, 13, 7, 1, 47, 12, 5, 3, 1, 47, 14, 16, 1, 47, 9, 5, 1, 49, 4, 1, 47, 12, 11, 5, 3, 1, 47, 12, 4, 10, 1, 47, 19, 5, 3, 1, 47, 12, 4, 10, 1, 47, 19, 5, 21, 1, 47, 21, 6, 5, 3, 4, 3, 1, 47, 5, 1, 47, 12, 18, 9, 5, 3, 4, 3, 1, 47, 23, 7, 9, 1, 47, 11, 4, 1, 49, 13, 22, 4, 3, 1, 47, 8, 4, 10, 3, 1, 47, 18, 4, 8, 1, 47, 12, 6, 4, 9, 3, 1, 47, 9, 6, 1, 47, 8, 7, 1, 47, 19, 5, 3, 1, 47, 10, 5, 3, 1, 47, 18, 5, 1, 47, 16, 1, 47, 9, 5, 35, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49, 45, 11, 5, 8, 1, 47, 11, 20, 3, 30, 3, 1, 47, 5, 1, 47, 17, 5, 1, 47, 9, 5, 3, 1, 47, 13, 22, 4, 3, 1, 47, 18, 7, 1, 47, 13, 7, 3, 30, 3, 1, 47, 18, 6, 28, 3, 1, 47, 17, 7, 9, 1, 47, 11, 4, 1, 49, 17, 5, 3, 1, 47, 18, 4, 9, 3, 1, 47, 11, 9, 5, 11, 1, 47, 11, 5, 9, 3, 1, 47, 14, 4, 10, 3, 1, 47, 25, 4, 8, 3, 1, 47, 13, 22, 20, 3, 6, 20, 3, 1, 47, 19, 6, 3, 1, 47, 11, 9, 7, 1, 47, 19, 5, 6, 1, 49, 14, 6, 1, 47, 9, 29, 3, 1, 47, 14, 4, 3, 1, 47, 10, 20, 3, 5, 10, 1, 47, 11, 9, 4, 3, 1, 47, 13, 7, 1, 47, 12, 4, 3, 1, 47, 13, 22, 20, 3, 6, 20, 3, 1, 47, 19, 20, 3, 22, 7, 3, 1, 47, 12, 13, 7, 9, 1, 47, 11, 4, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49]\n",
      "603\n",
      "[45, 19, 4, 9, 1, 47, 21, 6, 1, 47, 8, 4, 3, 1, 47, 17, 5, 1, 47, 14, 9, 4, 3, 1, 47, 23, 6, 1, 47, 21, 10, 6, 5, 3, 1, 47, 14, 4, 10, 3, 1, 47, 11, 16, 7, 3, 1, 47, 23, 6, 1, 47, 21, 10, 6, 7, 1, 49, 16, 1, 47, 17, 6, 1, 47, 10, 4, 3, 4, 3, 1, 47, 5, 10, 1, 47, 11, 5, 3, 1, 47, 18, 6, 28, 3, 1, 47, 13, 22, 4, 3, 1, 47, 13, 9, 4, 1, 47, 5, 1, 47, 11, 16, 1, 47, 9, 5, 1, 49, 11, 4, 9, 1, 47, 17, 6, 1, 47, 8, 4, 3, 1, 47, 23, 6, 12, 1, 47, 12, 7, 3, 1, 47, 14, 20, 3, 4, 11, 1, 47, 11, 4, 9, 1, 47, 8, 7, 3, 1, 47, 13, 7, 8, 1, 47, 12, 6, 1, 47, 21, 10, 6, 7, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49, 45, 11, 16, 3, 1, 47, 12, 4, 20, 3, 1, 47, 13, 7, 1, 47, 10, 4, 6, 3, 1, 47, 13, 22, 4, 3, 1, 47, 10, 20, 3, 16, 1, 47, 17, 5, 1, 47, 8, 5, 3, 1, 47, 8, 5, 1, 47, 11, 16, 1, 47, 9, 5, 1, 49, 8, 7, 1, 47, 25, 6, 1, 47, 10, 6, 1, 47, 11, 5, 1, 47, 12, 11, 6, 3, 1, 47, 12, 27, 3, 1, 47, 13, 22, 4, 3, 20, 10, 3, 1, 47, 12, 16, 7, 3, 1, 47, 23, 5, 11, 1, 47, 11, 7, 1, 47, 9, 4, 1, 49, 8, 7, 8, 3, 1, 47, 14, 6, 1, 47, 12, 14, 4, 21, 1, 47, 8, 29, 3, 1, 47, 14, 6, 3, 1, 47, 23, 5, 9, 1, 47, 12, 6, 3, 1, 47, 12, 16, 5, 3, 1, 47, 23, 5, 11, 1, 47, 11, 16, 1, 47, 9, 5, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49, 45, 8, 4, 10, 3, 1, 47, 19, 4, 8, 1, 47, 11, 9, 4, 3, 1, 47, 11, 16, 7, 3, 1, 47, 12, 6, 3, 1, 47, 9, 5, 13, 1, 47, 13, 4, 1, 47, 12, 4, 3, 1, 47, 10, 20, 3, 5, 1, 47, 17, 7, 1, 47, 9, 4, 1, 49, 18, 4, 9, 3, 1, 47, 10, 7, 3, 1, 47, 13, 16, 6, 3, 1, 47, 13, 5, 10, 1, 47, 14, 7, 3, 1, 47, 8, 4, 3, 1, 47, 10, 20, 3, 4, 11, 1, 47, 11, 4, 9, 1, 47, 8, 5, 3, 1, 47, 18, 5, 1, 47, 13, 4, 1, 49, 13, 7, 1, 47, 12, 27, 3, 1, 47, 30, 3, 1, 47, 21, 4, 9, 1, 47, 17, 6, 1, 47, 8, 5, 1, 47, 11, 7, 3, 1, 47, 24, 16, 4, 1, 47, 12, 11, 7, 3, 1, 47, 23, 6, 7, 1, 47, 9, 4, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49]\n",
      "[45, 19, 4, 9, 1, 47, 21, 6, 1, 47, 8, 4, 3, 1, 47, 17, 5, 1, 47, 14, 9, 4, 3, 1, 47, 23, 6, 1, 47, 21, 10, 6, 5, 3, 1, 47, 14, 4, 10, 3, 1, 47, 11, 16, 7, 3, 1, 47, 23, 6, 1, 47, 21, 10, 6, 7, 1, 49, 16, 1, 47, 17, 6, 1, 47, 10, 4, 3, 4, 3, 1, 47, 5, 10, 1, 47, 11, 5, 3, 1, 47, 18, 6, 28, 3, 1, 47, 13, 22, 4, 3, 1, 47, 13, 9, 4, 1, 47, 5, 1, 47, 11, 16, 1, 47, 9, 5, 1, 49, 11, 4, 9, 1, 47, 17, 6, 1, 47, 8, 4, 3, 1, 47, 23, 6, 12, 1, 47, 12, 7, 3, 1, 47, 14, 20, 3, 4, 11, 1, 47, 11, 4, 9, 1, 47, 8, 7, 3, 1, 47, 13, 7, 8, 1, 47, 12, 6, 1, 47, 21, 10, 6, 7, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49, 45, 11, 16, 3, 1, 47, 12, 4, 20, 3, 1, 47, 13, 7, 1, 47, 10, 4, 6, 3, 1, 47, 13, 22, 4, 3, 1, 47, 10, 20, 3, 16, 1, 47, 17, 5, 1, 47, 8, 5, 3, 1, 47, 8, 5, 1, 47, 11, 16, 1, 47, 9, 5, 1, 49, 8, 7, 1, 47, 25, 6, 1, 47, 10, 6, 1, 47, 11, 5, 1, 47, 12, 11, 6, 3, 1, 47, 12, 27, 3, 1, 47, 13, 22, 4, 3, 20, 10, 3, 1, 47, 12, 16, 7, 3, 1, 47, 23, 5, 11, 1, 47, 11, 7, 1, 47, 9, 4, 1, 49, 8, 7, 8, 3, 1, 47, 14, 6, 1, 47, 12, 14, 4, 21, 1, 47, 8, 29, 3, 1, 47, 14, 6, 3, 1, 47, 23, 5, 9, 1, 47, 12, 6, 3, 1, 47, 12, 16, 5, 3, 1, 47, 23, 5, 11, 1, 47, 11, 16, 1, 47, 9, 5, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49, 45, 8, 4, 10, 3, 1, 47, 19, 4, 8, 1, 47, 11, 9, 4, 3, 1, 47, 11, 16, 7, 3, 1, 47, 12, 6, 3, 1, 47, 9, 5, 13, 1, 47, 13, 4, 1, 47, 12, 4, 3, 1, 47, 10, 20, 3, 5, 1, 47, 17, 7, 1, 47, 9, 4, 1, 49, 18, 4, 9, 3, 1, 47, 10, 7, 3, 1, 47, 13, 16, 6, 3, 1, 47, 13, 5, 10, 1, 47, 14, 7, 3, 1, 47, 8, 4, 3, 1, 47, 10, 20, 3, 4, 11, 1, 47, 11, 4, 9, 1, 47, 8, 5, 3, 1, 47, 18, 5, 1, 47, 13, 4, 1, 49, 13, 7, 1, 47, 12, 27, 3, 1, 47, 30, 3, 1, 47, 21, 4, 9, 1, 47, 17, 6, 1, 47, 8, 5, 1, 47, 11, 7, 3, 1, 47, 24, 16, 4, 1, 47, 12, 11, 7, 3, 1, 47, 23, 6, 7, 1, 47, 9, 4, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49]\n",
      "603\n",
      "[45, 13, 22, 4, 3, 1, 47, 23, 16, 3, 1, 47, 25, 6, 1, 47, 12, 5, 1, 47, 19, 5, 3, 5, 10, 3, 1, 47, 13, 5, 8, 1, 47, 11, 7, 9, 3, 1, 47, 13, 22, 4, 3, 1, 47, 18, 4, 9, 3, 1, 47, 14, 7, 1, 47, 21, 10, 6, 5, 1, 49, 14, 4, 10, 3, 1, 47, 23, 5, 10, 1, 47, 10, 7, 3, 1, 47, 14, 6, 12, 1, 47, 12, 4, 3, 1, 47, 17, 6, 1, 47, 12, 4, 1, 47, 9, 4, 1, 47, 9, 4, 3, 1, 47, 17, 4, 6, 1, 49, 18, 16, 7, 6, 3, 1, 47, 11, 16, 3, 1, 47, 19, 4, 1, 47, 14, 4, 9, 3, 1, 47, 13, 7, 1, 47, 12, 27, 3, 1, 47, 14, 6, 3, 1, 47, 12, 7, 1, 47, 21, 10, 6, 5, 3, 6, 8, 3, 1, 47, 12, 7, 1, 47, 21, 10, 6, 5, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49, 45, 21, 6, 28, 3, 1, 47, 14, 6, 1, 47, 21, 9, 5, 1, 47, 14, 5, 9, 3, 1, 47, 13, 7, 17, 20, 3, 1, 47, 6, 7, 3, 1, 47, 13, 22, 20, 3, 5, 3, 1, 47, 18, 9, 7, 1, 47, 18, 9, 6, 7, 3, 1, 47, 8, 7, 1, 47, 17, 4, 1, 49, 19, 7, 3, 1, 47, 18, 4, 9, 3, 1, 47, 10, 5, 3, 1, 47, 9, 7, 1, 47, 12, 5, 3, 1, 47, 21, 6, 28, 3, 1, 47, 14, 6, 3, 1, 47, 23, 7, 1, 47, 21, 10, 6, 5, 3, 6, 8, 3, 1, 47, 23, 7, 1, 47, 21, 10, 6, 5, 1, 49, 4, 3, 1, 47, 14, 5, 10, 3, 1, 47, 12, 4, 11, 1, 47, 11, 6, 1, 47, 17, 7, 3, 1, 47, 21, 9, 5, 1, 47, 14, 7, 3, 6, 8, 3, 1, 47, 21, 6, 28, 3, 1, 47, 12, 27, 3, 1, 47, 13, 7, 1, 47, 17, 4, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49, 45, 6, 8, 1, 47, 23, 6, 1, 47, 8, 7, 3, 5, 14, 3, 1, 47, 4, 12, 1, 47, 12, 7, 3, 1, 47, 12, 16, 13, 1, 47, 13, 4, 1, 47, 14, 7, 1, 47, 8, 7, 3, 4, 1, 47, 25, 9, 4, 4, 1, 49, 14, 6, 1, 47, 9, 6, 1, 47, 17, 4, 8, 1, 47, 14, 7, 3, 1, 47, 14, 4, 10, 3, 1, 47, 23, 6, 7, 9, 3, 1, 47, 11, 16, 11, 1, 47, 11, 4, 3, 1, 47, 10, 4, 3, 1, 47, 13, 22, 6, 7, 1, 47, 17, 4, 1, 49, 18, 4, 9, 1, 47, 13, 22, 31, 3, 1, 47, 12, 4, 1, 47, 13, 7, 8, 1, 47, 14, 7, 3, 1, 47, 10, 7, 3, 1, 47, 12, 21, 16, 5, 9, 1, 47, 14, 7, 3, 1, 47, 13, 22, 4, 3, 1, 47, 23, 31, 4, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49]\n",
      "[45, 13, 22, 4, 3, 1, 47, 23, 16, 3, 1, 47, 25, 6, 1, 47, 12, 5, 1, 47, 19, 5, 3, 5, 10, 3, 1, 47, 13, 5, 8, 1, 47, 11, 7, 9, 3, 1, 47, 13, 22, 4, 3, 1, 47, 18, 4, 9, 3, 1, 47, 14, 7, 1, 47, 21, 10, 6, 5, 1, 49, 14, 4, 10, 3, 1, 47, 23, 5, 10, 1, 47, 10, 7, 3, 1, 47, 14, 6, 12, 1, 47, 12, 4, 3, 1, 47, 17, 6, 1, 47, 12, 4, 1, 47, 9, 4, 1, 47, 9, 4, 3, 1, 47, 17, 4, 6, 1, 49, 18, 16, 7, 6, 3, 1, 47, 11, 16, 3, 1, 47, 19, 4, 1, 47, 14, 4, 9, 3, 1, 47, 13, 7, 1, 47, 12, 27, 3, 1, 47, 14, 6, 3, 1, 47, 12, 7, 1, 47, 21, 10, 6, 5, 3, 6, 8, 3, 1, 47, 12, 7, 1, 47, 21, 10, 6, 5, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49, 45, 21, 6, 28, 3, 1, 47, 14, 6, 1, 47, 21, 9, 5, 1, 47, 14, 5, 9, 3, 1, 47, 13, 7, 17, 20, 3, 1, 47, 6, 7, 3, 1, 47, 13, 22, 20, 3, 5, 3, 1, 47, 18, 9, 7, 1, 47, 18, 9, 6, 7, 3, 1, 47, 8, 7, 1, 47, 17, 4, 1, 49, 19, 7, 3, 1, 47, 18, 4, 9, 3, 1, 47, 10, 5, 3, 1, 47, 9, 7, 1, 47, 12, 5, 3, 1, 47, 21, 6, 28, 3, 1, 47, 14, 6, 3, 1, 47, 23, 7, 1, 47, 21, 10, 6, 5, 3, 6, 8, 3, 1, 47, 23, 7, 1, 47, 21, 10, 6, 5, 1, 49, 4, 3, 1, 47, 14, 5, 10, 3, 1, 47, 12, 4, 11, 1, 47, 11, 6, 1, 47, 17, 7, 3, 1, 47, 21, 9, 5, 1, 47, 14, 7, 3, 6, 8, 3, 1, 47, 21, 6, 28, 3, 1, 47, 12, 27, 3, 1, 47, 13, 7, 1, 47, 17, 4, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49, 45, 6, 8, 1, 47, 23, 6, 1, 47, 8, 7, 3, 5, 14, 3, 1, 47, 4, 12, 1, 47, 12, 7, 3, 1, 47, 12, 16, 13, 1, 47, 13, 4, 1, 47, 14, 7, 1, 47, 8, 7, 3, 4, 1, 47, 25, 9, 4, 4, 1, 49, 14, 6, 1, 47, 9, 6, 1, 47, 17, 4, 8, 1, 47, 14, 7, 3, 1, 47, 14, 4, 10, 3, 1, 47, 23, 6, 7, 9, 3, 1, 47, 11, 16, 11, 1, 47, 11, 4, 3, 1, 47, 10, 4, 3, 1, 47, 13, 22, 6, 7, 1, 47, 17, 4, 1, 49, 18, 4, 9, 1, 47, 13, 22, 31, 3, 1, 47, 12, 4, 1, 47, 13, 7, 8, 1, 47, 14, 7, 3, 1, 47, 10, 7, 3, 1, 47, 12, 21, 16, 5, 9, 1, 47, 14, 7, 3, 1, 47, 13, 22, 4, 3, 1, 47, 23, 31, 4, 1, 49, 48, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 49]\n",
      "603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_hyp_lm_tercets(tercets):\n",
    "    new_tercets = []\n",
    "    for tercet in tercets:\n",
    "        new_tercets.append([])\n",
    "        for verse in tercet:\n",
    "            new_tercets[-1].append([])\n",
    "            for hyp_w in verse:\n",
    "                new_tercets[-1][-1].extend(hyp_w)\n",
    "                new_tercets[-1][-1].append('<SEP>')\n",
    "            new_tercets[-1][-1] = new_tercets[-1][-1][:-1]\n",
    "\n",
    "    return new_tercets\n",
    "\n",
    "def hyphenation(verse):\n",
    "    \"\"\"\n",
    "    Split word in syllables\n",
    "    :param verse: input string\n",
    "    :return: a list containing syllables of the word\n",
    "    \"\"\"\n",
    "    syllables = []\n",
    "    syllable = \"\"\n",
    "\n",
    "    verse = verse[1:]\n",
    "    verse = verse+\"|\" \n",
    "\n",
    "    for letter in verse:\n",
    "        syllable += letter\n",
    "        if letter == '|':\n",
    "            syllables.append(syllable)\n",
    "            syllable = \"\"\n",
    "               \n",
    "    return syllables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_dc_hyphenation(canti):\n",
    "    hyp_canti, hyp_tokens = [], []\n",
    "    for canto in canti:\n",
    "        hyp_canti.append([])\n",
    "        for verso in canto:\n",
    "            syllables = hyphenation(verso)\n",
    "            hyp_canti[-1].append(syllables)\n",
    "            for syllable in syllables:\n",
    "                hyp_tokens.extend(syllable)\n",
    "\n",
    "    return hyp_canti, hyp_tokens\n",
    "    \n",
    "\n",
    "def get_dc_cantos(filename, encoding=None):\n",
    "    # raw_data = read_words(filename=filename)\n",
    "    cantos = []\n",
    "    cantoCounter =  0\n",
    "    cantos.append([])\n",
    "    with open(filename, \"r\", encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            sentence = line.strip()\n",
    "            if 'canto' not in sentence and len(sentence) > 2:\n",
    "                if len(sentence) > 2:\n",
    "                    cantos[cantoCounter].append(sentence)\n",
    "                    \"\"\"\n",
    "                    if len(cantos[cantoCounter]) == 3:\n",
    "                      cantoCounter += 1\n",
    "                      cantos.append([])\n",
    "                    \"\"\"\n",
    "    return cantos\n",
    "\n",
    "\n",
    "def create_tercets(cantos):\n",
    "    tercets = []\n",
    "    for i,canto in enumerate(cantos):\n",
    "        for v,verse in enumerate(canto):\n",
    "            if v%3 == 0:\n",
    "                tercets.append([])\n",
    "\n",
    "            tercets[-1].append(verse)\n",
    "        tercets = tercets[:-1]  # removes the last malformed tercets (only 2 verses)\n",
    "\n",
    "    return tercets\n",
    "\n",
    "def pad_list(l, pad_token, max_l_size, keep_lasts=False, pad_right=True):\n",
    "    \"\"\"\n",
    "    Adds a padding token to a list\n",
    "    inputs:\n",
    "    :param l: input list to pad.\n",
    "    :param pad_token: value to add as padding.\n",
    "    :param max_l_size: length of the new padded list to return,\n",
    "    it truncates lists longer that 'max_l_size' without adding\n",
    "    padding values.\n",
    "    :param keep_lasts: If True, preserves the max_l_size last elements\n",
    "    of a sequence (by keeping the same order).  E.g.:\n",
    "    if keep_lasts is True and max_l_size=3 [1,2,3,4] becomes [2,3,4].\n",
    "\n",
    "\n",
    "    :return: the list padded or truncated.\n",
    "    \"\"\"\n",
    "    to_pad = []\n",
    "    max_l = min(max_l_size, len(l))  # maximum len\n",
    "    l_init = len(l) - max_l if len(l) > max_l and keep_lasts else 0  # initial position where to sample from the list\n",
    "    l_end = len(l) if len(l) > max_l and keep_lasts else max_l\n",
    "    for i in range(l_init, l_end):\n",
    "        to_pad.append(l[i])\n",
    "\n",
    "    # for j in range(len(l), max_l_size):\n",
    "    #     to_pad.append(pad_token)\n",
    "    pad_tokens = [pad_token] * (max_l_size-len(l))\n",
    "    padded_l = to_pad + pad_tokens if pad_right else pad_tokens + to_pad\n",
    "\n",
    "    return padded_l\n",
    "\n",
    "\n",
    "def save_data(data, file):\n",
    "    with open(file, 'wb') as output:\n",
    "        pickle.dump(data, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_data(file):\n",
    "    with open(file, 'rb') as obj:\n",
    "        return pickle.load(obj)\n",
    "\n",
    "def print_and_write(file, s):\n",
    "    print(s)\n",
    "    file.write(s)\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    def __init__(self, vocab_size=None):\n",
    "        self.dictionary = dict()\n",
    "        self.rev_dictionary = dict()\n",
    "        self.count = []\n",
    "        self.special_tokens = []\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def build_vocabulary_from_counts(self, count, special_tokens=[]):\n",
    "        \"\"\"\n",
    "        Sets all the attributes of the Vocabulary object.\n",
    "        :param count: a list of lists as follows: [['token', number_of_occurrences],...]\n",
    "        :param special_tokens: a list of strings. E.g. ['<EOS>', '<PAD>',...]\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        dictionary = dict()\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = len(dictionary)\n",
    "\n",
    "        # adding eventual special tokens to the dictionary (e.g. <EOS>,<PAD> etc..)\n",
    "        d = len(dictionary)\n",
    "        for i, token in enumerate(special_tokens):\n",
    "            dictionary[token] = d + i\n",
    "\n",
    "        self.count = count\n",
    "        self.dictionary = dictionary\n",
    "        self.rev_dictionary = dict(zip(self.dictionary.values(), self.dictionary.keys()))\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab_size = len(dictionary)\n",
    "\n",
    "    def build_vocabulary_from_tokens(self, tokens, vocabulary_size=None, special_tokens=[]):\n",
    "        \"\"\"\n",
    "        Given a list of tokens, it sets the Vocabulary object attributes by constructing\n",
    "        a dictionary mapping each token to a unique id.\n",
    "        :param tokens: a list of strings.\n",
    "         E.g. [\"the\", \"cat\", \"is\", ... \".\", \"the\", \"house\" ,\"is\" ...].\n",
    "         NB: Here you should put all your token instances of the corpus.\n",
    "        :param vocabulary_size: The number of elements of your vocabulary. If there are more\n",
    "        than 'vocabulary_size' elements on tokens, it considers only the 'vocabulary_size'\n",
    "        most frequent ones.\n",
    "        :param special_tokens: Optional. A list of strings. Useful to add special tokens in vocabulary.\n",
    "        If you don't have any, keep it empty.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        vocabulary_size = vocabulary_size if vocabulary_size is not None else self.vocab_size\n",
    "        vocabulary_size = vocabulary_size - (len(special_tokens) + 1) if vocabulary_size else None\n",
    "        # counts occurrences of each token\n",
    "        count = [['<UNK>', -1]]\n",
    "        count.extend(collections.Counter(tokens).most_common(vocabulary_size))  # takes only the most frequent ones, if size is None takes them all\n",
    "        self.build_vocabulary_from_counts(count, special_tokens)  # actually build the vocabulary\n",
    "        self._set_unk_count(tokens)  # set the number of OOV instances\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_vocabulary(vocab0, vocab1, vocabulary_size=-1):\n",
    "        \"\"\"\n",
    "        Merge two Vocabulary objects into a new one.\n",
    "        :param vocab0: first Vocabulary object\n",
    "        :param vocab1: second Vocabulary object\n",
    "        :param vocabulary_size: parameter to decide the merged vocabulary size.\n",
    "        With default value -1, all the words of both vocabularies are preserved.\n",
    "        When set to 0, the size of the vocabulary is set to the size of vocab0,\n",
    "        when set to 1 it is kept the size of vocab1.\n",
    "        :return: a new vocabulary\n",
    "        \"\"\"\n",
    "        # get size of the new vocabulary\n",
    "        vocab_size = vocab0.vocab_size + vocab1.vocab_size if vocabulary_size == -1 else vocabulary_size\n",
    "        merged_special_tokens = list(set(vocab0.special_tokens) | set(vocab1.special_tokens))\n",
    "\n",
    "        # merge the counts from the two vocabularies and then selects the most_common tokens\n",
    "        merged_counts = collections.Counter(dict(vocab0.count)) + collections.Counter(dict(vocab1.count))\n",
    "        merged_counts = merged_counts.most_common(vocab_size)\n",
    "        count = [['<UNK>', -1]]\n",
    "        count.extend(merged_counts)\n",
    "\n",
    "        # create the new vocabulary\n",
    "        merged_vocab = Vocabulary(vocab_size)\n",
    "        merged_vocab.build_vocabulary_from_counts(count, merged_special_tokens)\n",
    "        return merged_vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_vocabularies(vocab_list, vocab_size=None):\n",
    "        \"\"\"\n",
    "        Join a list of vocabularies into a new one.\n",
    "        :param vocab_list: a list of Vocabulary objects\n",
    "        :param vocab_size: the maximum size of the merged vocabulary.\n",
    "        :return: a vocabulary merging them all.\n",
    "        \"\"\"\n",
    "        vocab_size = vocab_size if vocab_size else sum([v.vocab_size for v in vocab_list])\n",
    "        merged_vocab = Vocabulary(vocab_size)\n",
    "        for voc in vocab_list:\n",
    "            merged_vocab = Vocabulary.merge_vocabulary(merged_vocab, voc, vocab_size)\n",
    "        return merged_vocab\n",
    "\n",
    "    def string2id(self, dataset):\n",
    "        \"\"\"\n",
    "        Converts a dataset of strings into a dataset of ids according to the object dictionary.\n",
    "        :param dataset: any string-based dataset with any nested lists.\n",
    "        :return: a new dataset, with the same shape of dataset, where each string is mapped into its\n",
    "        corresponding id associated in the dictionary (0 for unknown tokens).\n",
    "        \"\"\"\n",
    "\n",
    "        def _recursive_call(items):\n",
    "            new_items = []\n",
    "            for item in items:\n",
    "                if isinstance(item, str) or isinstance(item, int) or isinstance(item, float):\n",
    "                    new_items.append(self.word2id(item))\n",
    "                else:\n",
    "                    new_items.append(_recursive_call(item))\n",
    "            return new_items\n",
    "\n",
    "        return _recursive_call(dataset)\n",
    "\n",
    "    def id2string(self, dataset):\n",
    "        \"\"\"\n",
    "        Converts a dataset of integer ids into a dataset of string according to the reverse dictionary.\n",
    "        :param dataset: any int-based dataset with any nested lists. Allowed types are int, np.int32, np.int64.\n",
    "        :return: a new dataset, with the same shape of dataset, where each token is mapped into its\n",
    "        corresponding string associated in the reverse dictionary.\n",
    "        \"\"\"\n",
    "        def _recursive_call(items):\n",
    "            new_items = []\n",
    "            for item in items:\n",
    "                if isinstance(item, int) or isinstance(item, np.int) or isinstance(item, np.int32) or isinstance(item, np.int64):\n",
    "                    new_items.append(self.id2word(item))\n",
    "                else:\n",
    "                    new_items.append(_recursive_call(item))\n",
    "            return new_items\n",
    "\n",
    "        return _recursive_call(dataset)\n",
    "\n",
    "    def word2id(self, item):\n",
    "        \"\"\"\n",
    "        Maps a string token to its corresponding id.\n",
    "        :param item: a string.\n",
    "        :return: If the token belongs to the vocabulary, it returns an integer id > 0, otherwise\n",
    "        it returns the value associated to the unknown symbol, that is typically 0.\n",
    "        \"\"\"\n",
    "        return self.dictionary[item] if item in self.dictionary else self.dictionary['<UNK>']\n",
    "\n",
    "    def id2word(self, token_id):\n",
    "        \"\"\"\n",
    "        Maps an integer token to its corresponding string.\n",
    "        :param token_id: an integer.\n",
    "        :return: If the id belongs to the vocabulary, it returns the string\n",
    "        associated to it, otherwise it returns the string associated\n",
    "        to the unknown symbol, that is '<UNK>'.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.rev_dictionary[token_id] if token_id in self.rev_dictionary else self.rev_dictionary[self.dictionary['<UNK>']]\n",
    "\n",
    "    def get_unk_count(self):\n",
    "        return self.count[0][1]\n",
    "\n",
    "    def _set_unk_count(self, tokens):\n",
    "        \"\"\"\n",
    "        Sets the number of OOV instances in the tokens provided\n",
    "        :param tokens: a list of tokens\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        data = list()\n",
    "        unk_count = 0\n",
    "        for word in tokens:\n",
    "            if word in self.dictionary:\n",
    "                index = self.dictionary[word]\n",
    "            else:\n",
    "                index = 0  # dictionary['<UNK>']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "        self.count[0][1] = unk_count\n",
    "\n",
    "    def add_element(self, name, is_special_token=False):\n",
    "        if name not in self.dictionary:\n",
    "            self.vocab_size += 1\n",
    "            self.dictionary[name] = self.vocab_size\n",
    "            self.rev_dictionary[self.vocab_size] = name\n",
    "\n",
    "            if is_special_token:\n",
    "                self.special_tokens = list(self.special_tokens)\n",
    "                self.special_tokens.append(name)\n",
    "\n",
    "            self.count.append([name, 1])\n",
    "\n",
    "    def set_vocabulary(self, dictionary, rev_dictionary, special_tokens, vocab_size):\n",
    "        self.dictionary = dictionary,\n",
    "        self.rev_dictionary = rev_dictionary\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocabulary(filename):\n",
    "        return load_data(filename)\n",
    "\n",
    "    def save_vocabulary(self, filename):\n",
    "        save_data(self, filename)\n",
    "\n",
    "class SyLMDataset(object):\n",
    "    def __init__(self, config, sy_vocab=None):\n",
    "        self.config = config\n",
    "        self.vocabulary = sy_vocab\n",
    "\n",
    "        self.raw_train_x = []\n",
    "        self.raw_val_x = []\n",
    "        self.raw_test_x = []\n",
    "        self.raw_x = []\n",
    "\n",
    "        self.train_x, self.train_y = [], []\n",
    "        self.val_x, self.val_y = [], []\n",
    "        self.test_x, self.test_y = [], []\n",
    "        self.x, self.y = [], []\n",
    "\n",
    "    def initialize(self, sess):\n",
    "        pass\n",
    "\n",
    "    def load(self, sources):\n",
    "        \"\"\"\n",
    "        Extract raw texts form sources and gather them all together.\n",
    "        :param sources: a string or an iterable of strings containing the file(s)\n",
    "        to process in order to build the dataset.\n",
    "        :return: a list of raw strings.\n",
    "        \"\"\"\n",
    "        return NotImplementedError\n",
    "\n",
    "    def build(self, sources, split_size=0.8):\n",
    "        \"\"\"\n",
    "        :param sources: a string or an iterable of strings containing the file(s)\n",
    "        to process in order to build the dataset.\n",
    "        :param split_size: the size to split the dataset, set >=1.0 to not split.\n",
    "        \"\"\"\n",
    "\n",
    "        raw_x = self.load(sources)\n",
    "        # raw_x = self.tokenize([self.preprocess(ex) for ex in raw_x])  # fixme\n",
    "        # splitting data\n",
    "        self.raw_x = raw_x\n",
    "        if split_size < 1.0:\n",
    "            self.raw_train_x, self.raw_test_x = self.split(self.raw_x, train_size=split_size)\n",
    "            self.raw_train_x, self.raw_val_x = self.split(self.raw_train_x, train_size=split_size)\n",
    "        else:\n",
    "            self.raw_train_x = self.raw_x\n",
    "\n",
    "        if self.vocabulary is None:\n",
    "            # creates vocabulary\n",
    "            tokens = [item for sublist in self.raw_train_x for item in sublist]  # get tokens\n",
    "            special_tokens = (\"<GO>\", \"<PAD>\", \"<SEP>\", \"<EOS>\", \"<EOV>\")\n",
    "            self._create_vocab(tokens, special_tokens=special_tokens)\n",
    "\n",
    "        # creates x,y for train\n",
    "        self.train_x = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
    "        self.train_y = self._build_dataset(self.raw_train_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
    "\n",
    "        # creates x,y for validation\n",
    "        self.val_x = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
    "        self.val_y = self._build_dataset(self.raw_val_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
    "\n",
    "        # creates x,y for testing\n",
    "        self.test_x = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
    "        self.test_y = self._build_dataset(self.raw_test_x, insert_go=True, max_len=self.config.sentence_max_len, shuffle=False)\n",
    "\n",
    "    def _create_vocab(self, tokens, special_tokens=(\"<PAD>\", \"<GO>\", \"<SEP>\", \"<EOV>\", \"<EOS>\")):\n",
    "        \"\"\"\n",
    "        Create the vocabulary. Special tokens can be added to the tokens obtained from\n",
    "        the corpus.\n",
    "        :param tokens: a list of all the tokens in the corpus. Each token is a string.\n",
    "        :param special_tokens: a list of strings.\n",
    "        \"\"\"\n",
    "        print(\"creating_vocabulary\") \n",
    "\n",
    "        vocab = Vocabulary(vocab_size=self.config.input_vocab_size)\n",
    "        vocab.build_vocabulary_from_tokens(tokens, special_tokens=special_tokens)\n",
    "        self.vocabulary = vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def split(raw_data, train_size=0.8):\n",
    "        size = math.floor(len(raw_data)*train_size)\n",
    "        return raw_data[:size], raw_data[size:]\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(txt):\n",
    "        return txt\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle(x):\n",
    "        return random.sample(x, len(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(txt):\n",
    "        return txt\n",
    "\n",
    "    def _build_dataset(self, raw_data, max_len=200, insert_go=True, keep_lasts=False, pad_right=True, shuffle=True):\n",
    "        \"\"\"\n",
    "        Converts all the tokens in e1_raw_data by mapping each token with its corresponding\n",
    "        value in the dictionary. In case of token not in the dictionary, they are assigned to\n",
    "        a specific id. Each sequence is padded up to the seq_max_len setup in the config.\n",
    "\n",
    "        :param raw_data: list of sequences, each sequence is a list of tokens (strings).\n",
    "        :param max_len: max length of a sequence, crop longer and pad smaller ones.\n",
    "        :param insert_go: True to insert <GO>, False otherwise.\n",
    "        :param keep_lasts: True to truncate initial elements of a sequence.\n",
    "        :param pad_right: pad to the right (default value True), otherwise pads to left.\n",
    "        :param shuffle: Optional. If True data are shuffled.\n",
    "        :return: A list of sequences where each token in each sequence is an int id.\n",
    "        \"\"\"\n",
    "        dataset = []\n",
    "        for sentence in raw_data:\n",
    "            sentence_ids = [self.vocabulary.word2id(\"<GO>\")] if insert_go else []\n",
    "            sentence_ids.extend([self.vocabulary.word2id(w) for w in sentence])\n",
    "            sentence_ids.append(self.vocabulary.word2id(\"<EOS>\"))\n",
    "            sentence_ids = pad_list(sentence_ids, self.vocabulary.word2id(\"<PAD>\"), max_len, keep_lasts=keep_lasts, pad_right=pad_right)\n",
    "\n",
    "            dataset.append(sentence_ids)\n",
    "\n",
    "        if shuffle:\n",
    "            return random.sample(dataset, len(dataset))\n",
    "        else:\n",
    "            return dataset\n",
    "\n",
    "    def get_batches(self, batch_size=32, split_sel='train'):\n",
    "        \"\"\"\n",
    "        Iterator over the training set. Useful method to run experiments.\n",
    "        :param batch_size: size of the mini_batch\n",
    "        :return: input and target.\n",
    "        \"\"\"\n",
    "        if split_sel == 'train':\n",
    "            x, y = self.train_x, self.train_y\n",
    "        elif split_sel == 'val':\n",
    "            x, y = self.val_x, self.val_y\n",
    "        else:\n",
    "            x, y = self.test_x, self.test_y\n",
    "        \n",
    "        i = 0 #random.randint(0, batch_size)\n",
    "        batches = []\n",
    "        eov = self.vocabulary.word2id(\"<EOV>\")\n",
    "        go = self.vocabulary.word2id(\"<GO>\")\n",
    "        # prepare batches\n",
    "        while i < len(x):\n",
    "            j = 0\n",
    "            batch_x, batch_y = [], []\n",
    "            while j < batch_size and i+j<len(x):\n",
    "                for c in x[i+j]:\n",
    "                  batch_x.append(c)\n",
    "                batch_x.append(eov)\n",
    "                for c in y[i+j]:\n",
    "                  batch_y.append(c)\n",
    "                batch_y.append(eov)\n",
    "                j += 1\n",
    "            i += batch_size\n",
    "            batches.append((batch_x, batch_y))\n",
    "\n",
    "        # supply\n",
    "        i = 0\n",
    "        while i < len(batches):\n",
    "            yield batches[i][0], batches[i][1]\n",
    "            i += 1\n",
    "\n",
    "class DanteSyLMDataset(SyLMDataset):\n",
    "    def __init__(self, config, sy_vocab=None):\n",
    "        \"\"\"\n",
    "        Class to create a dataset from Dante Alighieri's Divine Comedy.\n",
    "        :param config: a Config object\n",
    "        :param sy_vocab: (optional) a Vocabulary object where tokens of the dictionary\n",
    "        are syllables. If None, the vocabulary is create automatically from the source.\n",
    "        \"\"\"\n",
    "        super().__init__(config, sy_vocab)\n",
    "\n",
    "    def load(self, sources):\n",
    "        \"\"\"\n",
    "        Load examples from dataset\n",
    "        :param sources: data filepath.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        canti = get_dc_cantos(filename=sources)  # get raw data from file\n",
    "        canti, tokens = get_dc_hyphenation(canti)  # converts each\n",
    "\n",
    "        tercets = create_tercets(canti)\n",
    "        tercets = get_hyp_lm_tercets(tercets)\n",
    "        x = []\n",
    "        for tercet in tercets:\n",
    "            x.append([])\n",
    "            for verse in tercet:\n",
    "                x[-1].extend(verse)\n",
    "                x[-1].append(\"<EOV>\")\n",
    "\n",
    "        #x = self.shuffle(x)\n",
    "        return x\n",
    "\n",
    "def seq2str(seq):\n",
    "    def output2string(batch, rev_vocabulary, special_tokens, end_of_tokens):\n",
    "        to_print = ''\n",
    "        for token in batch:\n",
    "            if token in special_tokens:\n",
    "                to_print += ' '\n",
    "            elif end_of_tokens and token in end_of_tokens:\n",
    "                to_print += '\\n'\n",
    "            elif token in rev_vocabulary:\n",
    "                to_print += rev_vocabulary[token]\n",
    "            else:\n",
    "                to_print += '<UNK>'\n",
    "        return to_print\n",
    "\n",
    "    return output2string(seq, poetry_sy_lm_dataset.vocabulary.rev_dictionary,\n",
    "      special_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\"), 0, poetry_sy_lm_dataset.vocabulary.word2id(\"<SEP>\"),\n",
    "                      poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\"), poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")],\n",
    "      end_of_tokens=[poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")])\n",
    "\n",
    "class cnfg:\n",
    "  vocab_size = vocab_size\n",
    "  input_vocab_size = vocab_size\n",
    "  sentence_max_len = terces_len\n",
    "\n",
    "config = cnfg()\n",
    "poetry_sy_lm_dataset = DanteSyLMDataset(config, sy_vocab=None)\n",
    "\n",
    "data_path = '/content/gdrive/MyDrive/cleanComedyProf.txt'  # dataset location, here just the name of the source file\n",
    "\n",
    "poetry_sy_lm_dataset.build(data_path, split_size=0.99)  # actual creation of  vocabulary (if not provided) and dataset\n",
    "print(\"Train size: \" + str(len(poetry_sy_lm_dataset.train_y)))\n",
    "print(\"Val size: \" + str(len(poetry_sy_lm_dataset.val_y)))\n",
    "print(\"Test size: \" + str(len(poetry_sy_lm_dataset.test_y)))\n",
    "\n",
    "eov = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOV>\")\n",
    "pad = poetry_sy_lm_dataset.vocabulary.word2id(\"<PAD>\")\n",
    "go = poetry_sy_lm_dataset.vocabulary.word2id(\"<GO>\")\n",
    "eos = poetry_sy_lm_dataset.vocabulary.word2id(\"<EOS>\")\n",
    "\n",
    "\n",
    "batches = [b for b in poetry_sy_lm_dataset.get_batches(terces_per_batch)]\n",
    "print(batches[0][0])\n",
    "print(batches[0][1])\n",
    "print(len(batches[0][0]))\n",
    "test_b = [b for b in poetry_sy_lm_dataset.get_batches(terces_per_batch, split_sel='test')]\n",
    "print(test_b[0][0])\n",
    "print(test_b[0][1])\n",
    "print(len(test_b[0][0]))\n",
    "val_b = [b for b in poetry_sy_lm_dataset.get_batches(terces_per_batch, split_sel='val')]\n",
    "print(val_b[0][0])\n",
    "print(val_b[0][1])\n",
    "print(len(val_b[0][0]))\n",
    "len(poetry_sy_lm_dataset.vocabulary.dictionary.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_6n273Y1I18",
    "outputId": "b68028f0-c135-4fc7-84b2-9056b50be34f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 3,\n",
       " '!': 35,\n",
       " '<EOS>': 48,\n",
       " '<EOV>': 49,\n",
       " '<GO>': 45,\n",
       " '<PAD>': 46,\n",
       " '<SEP>': 47,\n",
       " '<UNK>': 0,\n",
       " '?': 34,\n",
       " 'a': 5,\n",
       " 'b': 25,\n",
       " 'c': 13,\n",
       " 'd': 14,\n",
       " 'e': 4,\n",
       " 'f': 23,\n",
       " 'g': 21,\n",
       " 'h': 22,\n",
       " 'i': 6,\n",
       " 'j': 42,\n",
       " 'l': 10,\n",
       " 'm': 17,\n",
       " 'n': 8,\n",
       " 'o': 7,\n",
       " 'p': 18,\n",
       " 'q': 24,\n",
       " 'r': 9,\n",
       " 's': 12,\n",
       " 't': 11,\n",
       " 'u': 16,\n",
       " 'v': 19,\n",
       " 'x': 41,\n",
       " 'y': 44,\n",
       " 'z': 26,\n",
       " '|': 1,\n",
       " 'à': 32,\n",
       " 'ä': 39,\n",
       " 'è': 30,\n",
       " 'é': 31,\n",
       " 'ë': 36,\n",
       " 'ì': 27,\n",
       " 'ï': 33,\n",
       " 'ò': 29,\n",
       " 'ó': 38,\n",
       " 'ö': 43,\n",
       " 'ù': 28,\n",
       " 'ü': 37,\n",
       " '‘': 40,\n",
       " '’': 20}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poetry_sy_lm_dataset.vocabulary.dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xURM0fNiOIKj",
    "outputId": "3987828d-b210-485b-c6be-36863e2d1f29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in | tut| te | par| ti im| pe| ra e | qui| vi | reg| ge|\n",
      "qui| vi è | la | sua | cit| tà | e | l’ al| to | seg| gio|\n",
      "oh | fe| li| ce | co| lui | cu’ | i| vi e| leg| ge!|\n",
      "                              \n",
      " e | io | a | lui | po| e| ta io | ti | ri| cheg| gio|\n",
      "per | quel| lo | dio | che | tu | non | co| no| sce| sti|\n",
      "ac| ciò | ch’ io | fug| ga | que| sto | ma| le e | peg| gio|\n",
      "                          \n",
      " che | tu | mi | me| ni | là | do| v’ or | di| ce| sti|\n",
      "sì | ch’ io | veg| gia | la | por| ta | di | san | pie| tro|\n",
      "e | co| lor | cui | tu | fai | co| tan| to | me| sti|\n",
      "                             \n",
      "\n",
      " al| lor | si | mos| se e | io | li | ten| ni | die| tro|\n",
      "lo | gior| no | se | n’ an| da| va e | l’ ae| re | bru| no|\n",
      "to| glie| va | li a| ni| mai | che | so| no in | ter| ra|\n",
      "                        \n",
      " da | le | fa| ti| che | lo| ro e | io | sol | u| no|\n",
      "m’ ap| pa| rec| chia| va a | so| ste| ner | la | guer| ra|\n",
      "sì | del | cam| mi| no e | sì | de | la | pie| ta| te|\n",
      "                                \n",
      " che | ri| trar| rà | la | men| te | che | non | er| ra|\n",
      "o | mu| se o | al| to in| ge| gno or | m’ a| iu| ta| te|\n",
      "o | men| te | che | scri| ve| sti | ciò | ch’ io | vi| di|\n",
      "                           \n",
      "\n",
      " qui | si | par| rà | la | tua | no| bi| li| ta| te|\n",
      "io | co| min| ciai | po| e| ta | che | mi | gui| di|\n",
      "guar| da | la | mia | vir| tù | s’ el| l’ è | pos| sen| te|\n",
      "                                  \n",
      " pri| ma | ch’ a | l’ al| to | pas| so | tu | mi | fi| di|\n",
      "tu | di| ci | che | di | sil| vï| o il | pa| ren| te|\n",
      "cor| rut| ti| bi| le an| co| ra ad | im| mor| ta| le|\n",
      "                                 \n",
      " se| co| lo an| dò | e | fu | sen| si| bil| men| te|\n",
      "pe| rò | se | l’ av| ver| sa| rio | d’ o| gne | ma| le|\n",
      "cor| te| se i | fu | pen| san| do | l’ al| to ef| fet| to|\n",
      "                                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(seq2str(batches[14][0]))\n",
    "print(seq2str(batches[15][0]))\n",
    "print(seq2str(batches[16][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmSJj8BMqVY_"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICqI4vK-pC4k"
   },
   "outputs": [],
   "source": [
    "wandb.config.num_layers = 4\n",
    "wandb.config.d_model = 128\n",
    "wandb.config.dff = 256\n",
    "wandb.config.num_heads = 4\n",
    "wandb.config.dropout = 0.1\n",
    "wandb.config.learning_rate = 2e-4 \n",
    "\n",
    "generate_at = [10,20,30,40,50,60,70,80,90,100,110,120,130,140,150]\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def create_padding_mask(seq):   \n",
    "    seq = tf.cast(tf.math.equal(seq, pad), tf.float32)\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "    \n",
    "    Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    return output, attention_weights\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "            \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        return output, attention_weights\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def __call__(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        return out2\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "    \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "        \n",
    "    def __call__(self, x, enc_output, training, \n",
    "            look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        \n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                        for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "            \n",
    "    def __call__(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                        for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def __call__(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                look_ahead_mask, padding_mask)\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "        \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "                target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                            input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                            target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "        \n",
    "    def __call__(self, inp, tar, training, enc_padding_mask, \n",
    "            look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        \n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        \n",
    "        return final_output, attention_weights\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(wandb.config.learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "input_vocab_size = vocab_size\n",
    "target_vocab_size = vocab_size\n",
    "max_len = batch_len\n",
    "\n",
    "transformer = Transformer(wandb.config.num_layers, wandb.config.d_model, \n",
    "                          wandb.config.num_heads, wandb.config.dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=max_len, \n",
    "                          pe_target=max_len,\n",
    "                          rate=wandb.config.dropout)\n",
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')\n",
    "\n",
    "@tf.function()\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                    True, \n",
    "                                    enc_padding_mask, \n",
    "                                    combined_mask, \n",
    "                                    dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)\n",
    "\n",
    "@tf.function()\n",
    "def val_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    \n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                False, \n",
    "                                enc_padding_mask, \n",
    "                                combined_mask, \n",
    "                                dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "    \n",
    "    val_loss(loss)\n",
    "    val_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XaxwE-zBpx4S"
   },
   "outputs": [],
   "source": [
    "#@title Generation\n",
    "def generate(index=0, k=1, t=1):\n",
    "\n",
    "    def evaluate_greedy(inp_sentence, decoder_input):\n",
    "        inp_sentence = inp_sentence\n",
    "        encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "        \n",
    "        output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "        terces = 0\n",
    "        for i in range(batch_len):\n",
    "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "                encoder_input, output)\n",
    "        \n",
    "            # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "            predictions, attention_weights = transformer(encoder_input, \n",
    "                                                        output,\n",
    "                                                        False,\n",
    "                                                        enc_padding_mask,\n",
    "                                                        combined_mask,\n",
    "                                                        dec_padding_mask)\n",
    "            \n",
    "            # select the last word from the seq_len dimension\n",
    "            predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "            predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "            # return the result if the predicted_id is equal to the end token\n",
    "            if predicted_id == eos:\n",
    "                terces += 1\n",
    "                if terces == terces_per_batch-1:\n",
    "                    return tf.squeeze(output, axis=0), attention_weights\n",
    "            # concatentate the predicted_id to the output which is given to the decoder\n",
    "            # as its input.\n",
    "            output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "        return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "\n",
    "    def evaluate_topk(inp_sentence, decoder_input, k=5, temperature=0.5):\n",
    "        inp_sentence = inp_sentence\n",
    "        encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "        \n",
    "        output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "        def scale(tensor):\n",
    "            tensor = tf.math.divide(\n",
    "                tf.subtract(\n",
    "                    tensor, \n",
    "                    tf.reduce_min(tensor)\n",
    "                ), \n",
    "                tf.subtract(\n",
    "                    tf.reduce_max(tensor), \n",
    "                    tf.reduce_min(tensor))\n",
    "                )\n",
    "            return tensor\n",
    "\n",
    "        terces = 0\n",
    "        for i in range(batch_len):\n",
    "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "                encoder_input, output)\n",
    "        \n",
    "            # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "            predictions, attention_weights = transformer(encoder_input, \n",
    "                                                        output,\n",
    "                                                        False,\n",
    "                                                        enc_padding_mask,\n",
    "                                                        combined_mask,\n",
    "                                                        dec_padding_mask)\n",
    "            # select the last word from the seq_len dimension\n",
    "            predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "            predictions, indices = tf.math.top_k(predictions,k=k)\n",
    "            predictions /= temperature\n",
    "            #predictions = scale(predictions)\n",
    "            predictions = np.squeeze(predictions, axis=0)\n",
    "            indices = np.squeeze(indices, axis=0)\n",
    "            indices = np.squeeze(indices, axis=0)\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "            predicted_id = indices[predicted_id]\n",
    "\n",
    "            # return the result if the predicted_id is equal to the end token\n",
    "            if predicted_id == eos:\n",
    "                terces += 1\n",
    "                if terces == terces_per_batch-1:\n",
    "                    return tf.squeeze(output, axis=0), attention_weights\n",
    "            # concatentate the predicted_id to the output which is given to the decoder\n",
    "            # as its input.\n",
    "            predicted_id = tf.expand_dims(predicted_id, 0)\n",
    "            predicted_id = tf.expand_dims(predicted_id, 0)\n",
    "            output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "        return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "    out_list = test_b[index][0]\n",
    "    offset = terces_len # a tercet\n",
    "    txt_gen = seq2str(out_list[-offset:])\n",
    "\n",
    "    print(\"params: k={}, t={}\".format(k,t))\n",
    "    for i in range(32//(terces_per_batch-1)): # 30 terces = cantica\n",
    "        out, att_w = evaluate_topk([pad], out_list[-offset:], k, t)\n",
    "        out_list = out.numpy().tolist()\n",
    "        out_str = seq2str(out_list[offset:])\n",
    "        txt_gen += out_str\n",
    "\n",
    "    print(txt_gen)\n",
    "    wandb.log({\"generated\":\n",
    "            wandb.Html(\"k=\"+str(k)+\" t=\"+str(t)+\n",
    "                       \"<pre>\"+txt_gen+\"</pre>\", inject=False)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jv5bxDVqR0zv"
   },
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-izCTWvp23H",
    "outputId": "82258ae0-b314-4d0a-84d8-b5ec29256d33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      " che | si | fa| ce| sti | se| gui| ta | si | stri| sta|\n",
      "co| me | si | fa| vi| glia | di | sé | la | sua|\n",
      "co| me | si | fa| ce | se| con| ti | si | sta| vi|\n",
      "  \n",
      " che | si | fa| ce| sti | si | ri| spuo| se | stre| me|\n",
      "co| me | si | ve| der | la | sua | per | la | stra| da|\n",
      "co| me | si | ri| spuo| se | che | si | ri| spo| sta|\n",
      "  \n",
      " che | si | fa| ce| se | di | sé | la | sua | scri| se|\n",
      "co| me | si | fa| ce| sta | se| gui| ta | stra| va|\n",
      "co| me | si | fa| ce| sti | se| gui| ta | stri| se|\n",
      "  \n",
      " che | si | fa| ce| sti | si | ri| spuo| se | stra| di|\n",
      "co| me | si | ve| der | la | sua | per | la | stra| di|\n",
      "che | si | ri| spuo| se | di | sé | la | sua | stra| di|\n",
      "  \n",
      " che | si | fa| ce| sti | si | ri| spuo| se | stra|\n",
      "co| me | si | fa | se| gui| ta | se| con| ti| ca|\n",
      "che | si | ve| de | la | sua | di| stra | se| gui|\n",
      "  \n",
      " co| me | si | fa| ce | sì | che | si | ri| spuo|\n",
      "che | la | sua | per | la | sua | par| te | si | spi| ri|\n",
      "co| me | si | fa| ce| sta | se| gui| ta | stri| stra|\n",
      "  \n",
      " che | si | fa| ce| sti | si | ri| spuo| se | stra| sti|\n",
      "co| me | si | fa| ce | se| gui| ta | se| gui| ta|\n",
      "co| me | si | fa| ce| sti | si | fa| ce| sti| ta|\n",
      "  \n",
      " che | la | sua | di | sé | di | sé | di| stra | sta|\n",
      "co| me | si | fa| ce| sta | se| con| da | sua|\n",
      "co| me | si | ve| de | la | sua | per | la | sua|\n",
      "  \n",
      " che | si | fa| ce| se | di | sé | la | sua | si| ri| sta|\n",
      "co| me | si | fa| ce| sta | se| gui| ta | sua | stra|\n",
      "co| me | si | fa| ce| sta | se| gui| ta | stri| sta|\n",
      "                                   \n",
      " che | si | ri| spuo| se | sì | che | la | sua | stri| sta|\n",
      "co| me | si | fa| ce | si | fa| ce| se | sta| vi|\n",
      "che | si | fa| ce| sti | se| con| do | si | ri| spo|\n",
      "  \n",
      "  \n",
      "co| me | si | fa| ce | di | sé | la | sua | sca| ri|\n",
      "che | si | fa| ri| ce | si | fa| ce| sti| ca | sta|\n",
      "co| me | si | fa| ce| sti | se| con| da | sua|\n",
      "  \n",
      " co| me | si | ve| de | la | sua | per | la | stra| ce|\n",
      "che | si | fa | sua | di| stra| ti | si | ri| spuo| se|\n",
      "co| me | si | ri| spuo| se | di | sé | la | sua|\n",
      "  \n",
      " che | si | fa| ce| se | di | sé | la | sua | scri| se|\n",
      "co| me | si | fa| ce| sta | se| gui| ta | stra| va|\n",
      "co| me | si | fa| ce| sti | se| gui| ta | stri| se|\n",
      "  \n",
      " che | si | fa| ce| sti | si | ri| spuo| se | stra| di|\n",
      "co| me | si | fa| ce | se| gui| ta | se| con| ti|\n",
      "che | la | sua | per | la | sua | per | la | sua|\n",
      "  \n",
      "\n",
      "epoch lasted: 370.9951264858246\n",
      "Epoch 42 Batch 0 Loss 0.7443 Accuracy 0.7475\n",
      "Epoch 42 Batch 50 Loss 0.8104 Accuracy 0.7254\n",
      "Epoch 42 Batch 100 Loss 0.8073 Accuracy 0.7273\n",
      "Epoch 42 Batch 150 Loss 0.8050 Accuracy 0.7273\n",
      "Epoch 42 Batch 200 Loss 0.8054 Accuracy 0.7271\n",
      "Epoch 42 Batch 250 Loss 0.8038 Accuracy 0.7282\n",
      "Epoch 42 Batch 300 Loss 0.8042 Accuracy 0.7280\n",
      "Epoch 42 Batch 350 Loss 0.8029 Accuracy 0.7280\n",
      "Epoch 42 Batch 400 Loss 0.8032 Accuracy 0.7278\n",
      "discarded batch 417\n",
      "Epoch 42 Batch 450 Loss 0.8021 Accuracy 0.7280\n",
      "Epoch 42 Batch 500 Loss 0.8024 Accuracy 0.7281\n",
      "Epoch 42 Batch 550 Loss 0.8026 Accuracy 0.7279\n",
      "Epoch 42 Batch 600 Loss 0.8025 Accuracy 0.7280\n",
      "Epoch 42 Batch 650 Loss 0.8028 Accuracy 0.7279\n",
      "Epoch 42 Batch 700 Loss 0.8024 Accuracy 0.7278\n",
      "Epoch 42 Batch 750 Loss 0.8018 Accuracy 0.7279\n",
      "Epoch 42 Batch 800 Loss 0.8024 Accuracy 0.7278\n",
      "Epoch 42 Batch 850 Loss 0.8026 Accuracy 0.7278\n",
      "Epoch 42 Batch 900 Loss 0.8023 Accuracy 0.7280\n",
      "Epoch 42 Batch 950 Loss 0.8025 Accuracy 0.7278\n",
      "Epoch 42 Batch 1000 Loss 0.8024 Accuracy 0.7278\n",
      "Epoch 42 Batch 1050 Loss 0.8032 Accuracy 0.7277\n",
      "Epoch 42 Batch 1100 Loss 0.8040 Accuracy 0.7275\n",
      "Epoch 42 Batch 1150 Loss 0.8040 Accuracy 0.7274\n",
      "Epoch 42 Batch 1200 Loss 0.8035 Accuracy 0.7277\n",
      "Epoch 42 Batch 1250 Loss 0.8034 Accuracy 0.7278\n",
      "Epoch 42 Batch 1300 Loss 0.8038 Accuracy 0.7277\n",
      "Epoch 42 Batch 1350 Loss 0.8038 Accuracy 0.7277\n",
      "Epoch 42 Batch 1400 Loss 0.8036 Accuracy 0.7279\n",
      "Epoch 42 Batch 1450 Loss 0.8035 Accuracy 0.7279\n",
      "Epoch 42 Batch 1500 Loss 0.8034 Accuracy 0.7280\n",
      "Epoch 42 Loss 0.8034 Accuracy 0.7280\n",
      "Time taken for 1 epoch: 37.04751420021057 secs\n",
      "\n",
      "epoch lasted: 37.0517942905426\n",
      "Epoch 43 Batch 0 Loss 0.8368 Accuracy 0.7076\n",
      "Epoch 43 Batch 50 Loss 0.7920 Accuracy 0.7288\n",
      "Epoch 43 Batch 100 Loss 0.7905 Accuracy 0.7311\n",
      "Epoch 43 Batch 150 Loss 0.7899 Accuracy 0.7315\n",
      "Epoch 43 Batch 200 Loss 0.7904 Accuracy 0.7312\n",
      "Epoch 43 Batch 250 Loss 0.7903 Accuracy 0.7314\n",
      "Epoch 43 Batch 300 Loss 0.7922 Accuracy 0.7310\n",
      "Epoch 43 Batch 350 Loss 0.7941 Accuracy 0.7305\n",
      "Epoch 43 Batch 400 Loss 0.7939 Accuracy 0.7305\n",
      "Epoch 43 Batch 450 Loss 0.7963 Accuracy 0.7300\n",
      "Epoch 43 Batch 500 Loss 0.7975 Accuracy 0.7296\n",
      "Epoch 43 Batch 550 Loss 0.7972 Accuracy 0.7298\n",
      "Epoch 43 Batch 600 Loss 0.7978 Accuracy 0.7297\n",
      "Epoch 43 Batch 650 Loss 0.7996 Accuracy 0.7289\n",
      "Epoch 43 Batch 700 Loss 0.7995 Accuracy 0.7289\n",
      "Epoch 43 Batch 750 Loss 0.8001 Accuracy 0.7287\n",
      "Epoch 43 Batch 800 Loss 0.8008 Accuracy 0.7284\n",
      "Epoch 43 Batch 850 Loss 0.8001 Accuracy 0.7287\n",
      "Epoch 43 Batch 900 Loss 0.8005 Accuracy 0.7285\n",
      "discarded batch 928\n",
      "Epoch 43 Batch 950 Loss 0.8003 Accuracy 0.7285\n",
      "Epoch 43 Batch 1000 Loss 0.8001 Accuracy 0.7286\n",
      "Epoch 43 Batch 1050 Loss 0.7994 Accuracy 0.7289\n",
      "Epoch 43 Batch 1100 Loss 0.7993 Accuracy 0.7289\n",
      "Epoch 43 Batch 1150 Loss 0.7996 Accuracy 0.7290\n",
      "Epoch 43 Batch 1200 Loss 0.8000 Accuracy 0.7289\n",
      "Epoch 43 Batch 1250 Loss 0.8007 Accuracy 0.7287\n",
      "Epoch 43 Batch 1300 Loss 0.8001 Accuracy 0.7288\n",
      "Epoch 43 Batch 1350 Loss 0.8004 Accuracy 0.7287\n",
      "Epoch 43 Batch 1400 Loss 0.8005 Accuracy 0.7287\n",
      "Epoch 43 Batch 1450 Loss 0.8005 Accuracy 0.7287\n",
      "Epoch 43 Batch 1500 Loss 0.8004 Accuracy 0.7288\n",
      "Epoch 43 Loss 0.8003 Accuracy 0.7288\n",
      "Time taken for 1 epoch: 37.03187561035156 secs\n",
      "\n",
      "epoch lasted: 37.03550338745117\n",
      "Epoch 44 Batch 0 Loss 0.7858 Accuracy 0.7292\n",
      "Epoch 44 Batch 50 Loss 0.7827 Accuracy 0.7345\n",
      "Epoch 44 Batch 100 Loss 0.7897 Accuracy 0.7308\n",
      "Epoch 44 Batch 150 Loss 0.7880 Accuracy 0.7325\n",
      "Epoch 44 Batch 200 Loss 0.7894 Accuracy 0.7322\n",
      "Epoch 44 Batch 250 Loss 0.7931 Accuracy 0.7312\n",
      "Epoch 44 Batch 300 Loss 0.7926 Accuracy 0.7310\n",
      "discarded batch 301\n",
      "Epoch 44 Batch 350 Loss 0.7939 Accuracy 0.7306\n",
      "Epoch 44 Batch 400 Loss 0.7943 Accuracy 0.7307\n",
      "Epoch 44 Batch 450 Loss 0.7952 Accuracy 0.7306\n",
      "Epoch 44 Batch 500 Loss 0.7948 Accuracy 0.7309\n",
      "Epoch 44 Batch 550 Loss 0.7952 Accuracy 0.7306\n",
      "Epoch 44 Batch 600 Loss 0.7957 Accuracy 0.7302\n",
      "Epoch 44 Batch 650 Loss 0.7962 Accuracy 0.7301\n",
      "Epoch 44 Batch 700 Loss 0.7966 Accuracy 0.7299\n",
      "Epoch 44 Batch 750 Loss 0.7965 Accuracy 0.7300\n",
      "Epoch 44 Batch 800 Loss 0.7968 Accuracy 0.7297\n",
      "Epoch 44 Batch 850 Loss 0.7960 Accuracy 0.7300\n",
      "Epoch 44 Batch 900 Loss 0.7967 Accuracy 0.7297\n",
      "Epoch 44 Batch 950 Loss 0.7968 Accuracy 0.7298\n",
      "Epoch 44 Batch 1000 Loss 0.7965 Accuracy 0.7299\n",
      "Epoch 44 Batch 1050 Loss 0.7964 Accuracy 0.7301\n",
      "Epoch 44 Batch 1100 Loss 0.7957 Accuracy 0.7302\n",
      "Epoch 44 Batch 1150 Loss 0.7960 Accuracy 0.7301\n",
      "Epoch 44 Batch 1200 Loss 0.7964 Accuracy 0.7300\n",
      "Epoch 44 Batch 1250 Loss 0.7966 Accuracy 0.7299\n",
      "Epoch 44 Batch 1300 Loss 0.7963 Accuracy 0.7299\n",
      "Epoch 44 Batch 1350 Loss 0.7962 Accuracy 0.7300\n",
      "Epoch 44 Batch 1400 Loss 0.7969 Accuracy 0.7298\n",
      "Epoch 44 Batch 1450 Loss 0.7966 Accuracy 0.7299\n",
      "Epoch 44 Batch 1500 Loss 0.7967 Accuracy 0.7299\n",
      "Epoch 44 Loss 0.7966 Accuracy 0.7299\n",
      "Time taken for 1 epoch: 37.01825833320618 secs\n",
      "\n",
      "epoch lasted: 37.02264881134033\n",
      "Epoch 45 Batch 0 Loss 0.7829 Accuracy 0.7475\n",
      "Epoch 45 Batch 50 Loss 0.7966 Accuracy 0.7330\n",
      "Epoch 45 Batch 100 Loss 0.7937 Accuracy 0.7324\n",
      "Epoch 45 Batch 150 Loss 0.7944 Accuracy 0.7314\n",
      "Epoch 45 Batch 200 Loss 0.7927 Accuracy 0.7312\n",
      "Epoch 45 Batch 250 Loss 0.7921 Accuracy 0.7314\n",
      "Epoch 45 Batch 300 Loss 0.7919 Accuracy 0.7311\n",
      "Epoch 45 Batch 350 Loss 0.7917 Accuracy 0.7310\n",
      "Epoch 45 Batch 400 Loss 0.7919 Accuracy 0.7306\n",
      "Epoch 45 Batch 450 Loss 0.7926 Accuracy 0.7303\n",
      "Epoch 45 Batch 500 Loss 0.7933 Accuracy 0.7303\n",
      "Epoch 45 Batch 550 Loss 0.7914 Accuracy 0.7310\n",
      "Epoch 45 Batch 600 Loss 0.7919 Accuracy 0.7307\n",
      "Epoch 45 Batch 650 Loss 0.7915 Accuracy 0.7309\n",
      "Epoch 45 Batch 700 Loss 0.7920 Accuracy 0.7309\n",
      "Epoch 45 Batch 750 Loss 0.7921 Accuracy 0.7309\n",
      "Epoch 45 Batch 800 Loss 0.7928 Accuracy 0.7306\n",
      "Epoch 45 Batch 850 Loss 0.7924 Accuracy 0.7309\n",
      "Epoch 45 Batch 900 Loss 0.7932 Accuracy 0.7307\n",
      "Epoch 45 Batch 950 Loss 0.7931 Accuracy 0.7308\n",
      "Epoch 45 Batch 1000 Loss 0.7926 Accuracy 0.7311\n",
      "Epoch 45 Batch 1050 Loss 0.7930 Accuracy 0.7310\n",
      "Epoch 45 Batch 1100 Loss 0.7929 Accuracy 0.7311\n",
      "Epoch 45 Batch 1150 Loss 0.7929 Accuracy 0.7311\n",
      "Epoch 45 Batch 1200 Loss 0.7928 Accuracy 0.7312\n",
      "Epoch 45 Batch 1250 Loss 0.7929 Accuracy 0.7312\n",
      "Epoch 45 Batch 1300 Loss 0.7929 Accuracy 0.7313\n",
      "discarded batch 1315\n",
      "Epoch 45 Batch 1350 Loss 0.7930 Accuracy 0.7313\n",
      "Epoch 45 Batch 1400 Loss 0.7929 Accuracy 0.7313\n",
      "Epoch 45 Batch 1450 Loss 0.7931 Accuracy 0.7313\n",
      "Epoch 45 Batch 1500 Loss 0.7932 Accuracy 0.7312\n",
      "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
      "Epoch 45 Loss 0.7938 Accuracy 0.7311\n",
      "Time taken for 1 epoch: 37.3083815574646 secs\n",
      "\n",
      "epoch lasted: 37.312058210372925\n",
      "Epoch 46 Batch 0 Loss 0.7812 Accuracy 0.7342\n",
      "Epoch 46 Batch 50 Loss 0.7774 Accuracy 0.7354\n",
      "Epoch 46 Batch 100 Loss 0.7826 Accuracy 0.7335\n",
      "Epoch 46 Batch 150 Loss 0.7855 Accuracy 0.7334\n",
      "Epoch 46 Batch 200 Loss 0.7870 Accuracy 0.7335\n",
      "Epoch 46 Batch 250 Loss 0.7865 Accuracy 0.7338\n",
      "Epoch 46 Batch 300 Loss 0.7854 Accuracy 0.7343\n",
      "Epoch 46 Batch 350 Loss 0.7860 Accuracy 0.7340\n",
      "Epoch 46 Batch 400 Loss 0.7882 Accuracy 0.7333\n",
      "Epoch 46 Batch 450 Loss 0.7880 Accuracy 0.7335\n",
      "Epoch 46 Batch 500 Loss 0.7881 Accuracy 0.7336\n",
      "Epoch 46 Batch 550 Loss 0.7883 Accuracy 0.7332\n",
      "Epoch 46 Batch 600 Loss 0.7901 Accuracy 0.7325\n",
      "Epoch 46 Batch 650 Loss 0.7904 Accuracy 0.7323\n",
      "Epoch 46 Batch 700 Loss 0.7907 Accuracy 0.7323\n",
      "Epoch 46 Batch 750 Loss 0.7904 Accuracy 0.7324\n",
      "Epoch 46 Batch 800 Loss 0.7902 Accuracy 0.7325\n",
      "Epoch 46 Batch 850 Loss 0.7897 Accuracy 0.7326\n",
      "Epoch 46 Batch 900 Loss 0.7899 Accuracy 0.7324\n",
      "discarded batch 926\n",
      "Epoch 46 Batch 950 Loss 0.7899 Accuracy 0.7324\n",
      "Epoch 46 Batch 1000 Loss 0.7901 Accuracy 0.7323\n",
      "Epoch 46 Batch 1050 Loss 0.7902 Accuracy 0.7322\n",
      "Epoch 46 Batch 1100 Loss 0.7905 Accuracy 0.7322\n",
      "Epoch 46 Batch 1150 Loss 0.7907 Accuracy 0.7321\n",
      "Epoch 46 Batch 1200 Loss 0.7906 Accuracy 0.7322\n",
      "Epoch 46 Batch 1250 Loss 0.7901 Accuracy 0.7323\n",
      "Epoch 46 Batch 1300 Loss 0.7909 Accuracy 0.7321\n",
      "Epoch 46 Batch 1350 Loss 0.7906 Accuracy 0.7323\n",
      "Epoch 46 Batch 1400 Loss 0.7907 Accuracy 0.7322\n",
      "Epoch 46 Batch 1450 Loss 0.7906 Accuracy 0.7322\n",
      "Epoch 46 Batch 1500 Loss 0.7907 Accuracy 0.7321\n",
      "Epoch 46 Loss 0.7906 Accuracy 0.7322\n",
      "Time taken for 1 epoch: 36.9450523853302 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 46 VALIDATION: Loss 0.7715 Accuracy 0.7362\n",
      "\n",
      "epoch lasted: 37.09462070465088\n",
      "Epoch 47 Batch 0 Loss 0.7885 Accuracy 0.7292\n",
      "Epoch 47 Batch 50 Loss 0.7757 Accuracy 0.7370\n",
      "Epoch 47 Batch 100 Loss 0.7761 Accuracy 0.7364\n",
      "Epoch 47 Batch 150 Loss 0.7749 Accuracy 0.7366\n",
      "Epoch 47 Batch 200 Loss 0.7775 Accuracy 0.7359\n",
      "Epoch 47 Batch 250 Loss 0.7798 Accuracy 0.7354\n",
      "Epoch 47 Batch 300 Loss 0.7813 Accuracy 0.7351\n",
      "Epoch 47 Batch 350 Loss 0.7842 Accuracy 0.7346\n",
      "Epoch 47 Batch 400 Loss 0.7842 Accuracy 0.7349\n",
      "Epoch 47 Batch 450 Loss 0.7839 Accuracy 0.7348\n",
      "Epoch 47 Batch 500 Loss 0.7840 Accuracy 0.7349\n",
      "Epoch 47 Batch 550 Loss 0.7848 Accuracy 0.7345\n",
      "Epoch 47 Batch 600 Loss 0.7857 Accuracy 0.7343\n",
      "Epoch 47 Batch 650 Loss 0.7850 Accuracy 0.7345\n",
      "Epoch 47 Batch 700 Loss 0.7848 Accuracy 0.7345\n",
      "Epoch 47 Batch 750 Loss 0.7857 Accuracy 0.7343\n",
      "discarded batch 790\n",
      "Epoch 47 Batch 800 Loss 0.7854 Accuracy 0.7344\n",
      "Epoch 47 Batch 850 Loss 0.7851 Accuracy 0.7345\n",
      "Epoch 47 Batch 900 Loss 0.7849 Accuracy 0.7346\n",
      "Epoch 47 Batch 950 Loss 0.7857 Accuracy 0.7343\n",
      "Epoch 47 Batch 1000 Loss 0.7860 Accuracy 0.7342\n",
      "Epoch 47 Batch 1050 Loss 0.7867 Accuracy 0.7340\n",
      "Epoch 47 Batch 1100 Loss 0.7869 Accuracy 0.7339\n",
      "Epoch 47 Batch 1150 Loss 0.7869 Accuracy 0.7339\n",
      "Epoch 47 Batch 1200 Loss 0.7866 Accuracy 0.7339\n",
      "Epoch 47 Batch 1250 Loss 0.7864 Accuracy 0.7339\n",
      "Epoch 47 Batch 1300 Loss 0.7867 Accuracy 0.7338\n",
      "Epoch 47 Batch 1350 Loss 0.7872 Accuracy 0.7337\n",
      "Epoch 47 Batch 1400 Loss 0.7869 Accuracy 0.7338\n",
      "Epoch 47 Batch 1450 Loss 0.7872 Accuracy 0.7337\n",
      "Epoch 47 Batch 1500 Loss 0.7876 Accuracy 0.7335\n",
      "Epoch 47 Loss 0.7878 Accuracy 0.7335\n",
      "Time taken for 1 epoch: 36.96800184249878 secs\n",
      "\n",
      "epoch lasted: 36.97186827659607\n",
      "Epoch 48 Batch 0 Loss 0.8443 Accuracy 0.7076\n",
      "Epoch 48 Batch 50 Loss 0.7816 Accuracy 0.7350\n",
      "Epoch 48 Batch 100 Loss 0.7877 Accuracy 0.7326\n",
      "Epoch 48 Batch 150 Loss 0.7838 Accuracy 0.7341\n",
      "Epoch 48 Batch 200 Loss 0.7820 Accuracy 0.7345\n",
      "discarded batch 213\n",
      "Epoch 48 Batch 250 Loss 0.7815 Accuracy 0.7345\n",
      "Epoch 48 Batch 300 Loss 0.7834 Accuracy 0.7341\n",
      "Epoch 48 Batch 350 Loss 0.7827 Accuracy 0.7348\n",
      "Epoch 48 Batch 400 Loss 0.7835 Accuracy 0.7346\n",
      "Epoch 48 Batch 450 Loss 0.7842 Accuracy 0.7344\n",
      "Epoch 48 Batch 500 Loss 0.7840 Accuracy 0.7345\n",
      "Epoch 48 Batch 550 Loss 0.7839 Accuracy 0.7345\n",
      "Epoch 48 Batch 600 Loss 0.7842 Accuracy 0.7344\n",
      "Epoch 48 Batch 650 Loss 0.7840 Accuracy 0.7343\n",
      "Epoch 48 Batch 700 Loss 0.7842 Accuracy 0.7343\n",
      "Epoch 48 Batch 750 Loss 0.7842 Accuracy 0.7345\n",
      "Epoch 48 Batch 800 Loss 0.7844 Accuracy 0.7344\n",
      "Epoch 48 Batch 850 Loss 0.7841 Accuracy 0.7345\n",
      "Epoch 48 Batch 900 Loss 0.7843 Accuracy 0.7346\n",
      "Epoch 48 Batch 950 Loss 0.7841 Accuracy 0.7347\n",
      "Epoch 48 Batch 1000 Loss 0.7840 Accuracy 0.7347\n",
      "Epoch 48 Batch 1050 Loss 0.7840 Accuracy 0.7347\n",
      "Epoch 48 Batch 1100 Loss 0.7846 Accuracy 0.7345\n",
      "Epoch 48 Batch 1150 Loss 0.7847 Accuracy 0.7345\n",
      "Epoch 48 Batch 1200 Loss 0.7846 Accuracy 0.7344\n",
      "Epoch 48 Batch 1250 Loss 0.7847 Accuracy 0.7343\n",
      "Epoch 48 Batch 1300 Loss 0.7850 Accuracy 0.7342\n",
      "Epoch 48 Batch 1350 Loss 0.7855 Accuracy 0.7339\n",
      "Epoch 48 Batch 1400 Loss 0.7858 Accuracy 0.7338\n",
      "Epoch 48 Batch 1450 Loss 0.7859 Accuracy 0.7338\n",
      "Epoch 48 Batch 1500 Loss 0.7857 Accuracy 0.7340\n",
      "Epoch 48 Loss 0.7857 Accuracy 0.7340\n",
      "Time taken for 1 epoch: 36.87163972854614 secs\n",
      "\n",
      "epoch lasted: 36.87583112716675\n",
      "Epoch 49 Batch 0 Loss 0.6917 Accuracy 0.7757\n",
      "discarded batch 40\n",
      "Epoch 49 Batch 50 Loss 0.7898 Accuracy 0.7320\n",
      "Epoch 49 Batch 100 Loss 0.7746 Accuracy 0.7380\n",
      "Epoch 49 Batch 150 Loss 0.7701 Accuracy 0.7392\n",
      "Epoch 49 Batch 200 Loss 0.7711 Accuracy 0.7387\n",
      "Epoch 49 Batch 250 Loss 0.7742 Accuracy 0.7374\n",
      "Epoch 49 Batch 300 Loss 0.7754 Accuracy 0.7373\n",
      "Epoch 49 Batch 350 Loss 0.7752 Accuracy 0.7369\n",
      "Epoch 49 Batch 400 Loss 0.7764 Accuracy 0.7369\n",
      "Epoch 49 Batch 450 Loss 0.7768 Accuracy 0.7365\n",
      "Epoch 49 Batch 500 Loss 0.7773 Accuracy 0.7364\n",
      "Epoch 49 Batch 550 Loss 0.7774 Accuracy 0.7367\n",
      "Epoch 49 Batch 600 Loss 0.7793 Accuracy 0.7361\n",
      "Epoch 49 Batch 650 Loss 0.7804 Accuracy 0.7360\n",
      "Epoch 49 Batch 700 Loss 0.7804 Accuracy 0.7358\n",
      "Epoch 49 Batch 750 Loss 0.7806 Accuracy 0.7357\n",
      "Epoch 49 Batch 800 Loss 0.7803 Accuracy 0.7359\n",
      "Epoch 49 Batch 850 Loss 0.7808 Accuracy 0.7357\n",
      "Epoch 49 Batch 900 Loss 0.7814 Accuracy 0.7355\n",
      "Epoch 49 Batch 950 Loss 0.7818 Accuracy 0.7353\n",
      "Epoch 49 Batch 1000 Loss 0.7816 Accuracy 0.7355\n",
      "Epoch 49 Batch 1050 Loss 0.7821 Accuracy 0.7352\n",
      "Epoch 49 Batch 1100 Loss 0.7818 Accuracy 0.7353\n",
      "Epoch 49 Batch 1150 Loss 0.7821 Accuracy 0.7352\n",
      "Epoch 49 Batch 1200 Loss 0.7824 Accuracy 0.7352\n",
      "Epoch 49 Batch 1250 Loss 0.7825 Accuracy 0.7351\n",
      "Epoch 49 Batch 1300 Loss 0.7828 Accuracy 0.7351\n",
      "Epoch 49 Batch 1350 Loss 0.7828 Accuracy 0.7350\n",
      "Epoch 49 Batch 1400 Loss 0.7832 Accuracy 0.7349\n",
      "Epoch 49 Batch 1450 Loss 0.7834 Accuracy 0.7348\n",
      "Epoch 49 Batch 1500 Loss 0.7830 Accuracy 0.7350\n",
      "Epoch 49 Loss 0.7827 Accuracy 0.7351\n",
      "Time taken for 1 epoch: 37.26556134223938 secs\n",
      "\n",
      "epoch lasted: 37.270485162734985\n",
      "Epoch 50 Batch 0 Loss 0.7235 Accuracy 0.7492\n",
      "Epoch 50 Batch 50 Loss 0.7872 Accuracy 0.7343\n",
      "Epoch 50 Batch 100 Loss 0.7785 Accuracy 0.7377\n",
      "Epoch 50 Batch 150 Loss 0.7817 Accuracy 0.7358\n",
      "Epoch 50 Batch 200 Loss 0.7826 Accuracy 0.7353\n",
      "Epoch 50 Batch 250 Loss 0.7815 Accuracy 0.7352\n",
      "Epoch 50 Batch 300 Loss 0.7822 Accuracy 0.7348\n",
      "Epoch 50 Batch 350 Loss 0.7816 Accuracy 0.7349\n",
      "Epoch 50 Batch 400 Loss 0.7812 Accuracy 0.7353\n",
      "Epoch 50 Batch 450 Loss 0.7807 Accuracy 0.7353\n",
      "Epoch 50 Batch 500 Loss 0.7800 Accuracy 0.7352\n",
      "Epoch 50 Batch 550 Loss 0.7797 Accuracy 0.7355\n",
      "Epoch 50 Batch 600 Loss 0.7802 Accuracy 0.7355\n",
      "Epoch 50 Batch 650 Loss 0.7799 Accuracy 0.7355\n",
      "Epoch 50 Batch 700 Loss 0.7798 Accuracy 0.7356\n",
      "Epoch 50 Batch 750 Loss 0.7797 Accuracy 0.7357\n",
      "Epoch 50 Batch 800 Loss 0.7794 Accuracy 0.7359\n",
      "discarded batch 820\n",
      "Epoch 50 Batch 850 Loss 0.7795 Accuracy 0.7359\n",
      "Epoch 50 Batch 900 Loss 0.7796 Accuracy 0.7359\n",
      "Epoch 50 Batch 950 Loss 0.7795 Accuracy 0.7359\n",
      "Epoch 50 Batch 1000 Loss 0.7791 Accuracy 0.7360\n",
      "Epoch 50 Batch 1050 Loss 0.7788 Accuracy 0.7360\n",
      "Epoch 50 Batch 1100 Loss 0.7791 Accuracy 0.7359\n",
      "Epoch 50 Batch 1150 Loss 0.7793 Accuracy 0.7358\n",
      "Epoch 50 Batch 1200 Loss 0.7790 Accuracy 0.7359\n",
      "Epoch 50 Batch 1250 Loss 0.7786 Accuracy 0.7360\n",
      "Epoch 50 Batch 1300 Loss 0.7786 Accuracy 0.7360\n",
      "Epoch 50 Batch 1350 Loss 0.7788 Accuracy 0.7359\n",
      "Epoch 50 Batch 1400 Loss 0.7796 Accuracy 0.7357\n",
      "Epoch 50 Batch 1450 Loss 0.7797 Accuracy 0.7357\n",
      "Epoch 50 Batch 1500 Loss 0.7794 Accuracy 0.7359\n",
      "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
      "Epoch 50 Loss 0.7800 Accuracy 0.7357\n",
      "Time taken for 1 epoch: 37.27370047569275 secs\n",
      "\n",
      "epoch lasted: 37.27851581573486\n",
      "Epoch 51 Batch 0 Loss 0.8247 Accuracy 0.7243\n",
      "Epoch 51 Batch 50 Loss 0.7744 Accuracy 0.7374\n",
      "Epoch 51 Batch 100 Loss 0.7701 Accuracy 0.7386\n",
      "Epoch 51 Batch 150 Loss 0.7716 Accuracy 0.7391\n",
      "Epoch 51 Batch 200 Loss 0.7702 Accuracy 0.7393\n",
      "Epoch 51 Batch 250 Loss 0.7718 Accuracy 0.7388\n",
      "Epoch 51 Batch 300 Loss 0.7724 Accuracy 0.7385\n",
      "Epoch 51 Batch 350 Loss 0.7736 Accuracy 0.7384\n",
      "Epoch 51 Batch 400 Loss 0.7728 Accuracy 0.7387\n",
      "Epoch 51 Batch 450 Loss 0.7722 Accuracy 0.7386\n",
      "Epoch 51 Batch 500 Loss 0.7725 Accuracy 0.7384\n",
      "Epoch 51 Batch 550 Loss 0.7730 Accuracy 0.7381\n",
      "Epoch 51 Batch 600 Loss 0.7731 Accuracy 0.7382\n",
      "discarded batch 624\n",
      "Epoch 51 Batch 650 Loss 0.7733 Accuracy 0.7381\n",
      "Epoch 51 Batch 700 Loss 0.7734 Accuracy 0.7380\n",
      "Epoch 51 Batch 750 Loss 0.7739 Accuracy 0.7379\n",
      "Epoch 51 Batch 800 Loss 0.7744 Accuracy 0.7378\n",
      "Epoch 51 Batch 850 Loss 0.7750 Accuracy 0.7377\n",
      "Epoch 51 Batch 900 Loss 0.7753 Accuracy 0.7376\n",
      "Epoch 51 Batch 950 Loss 0.7755 Accuracy 0.7376\n",
      "Epoch 51 Batch 1000 Loss 0.7761 Accuracy 0.7374\n",
      "Epoch 51 Batch 1050 Loss 0.7766 Accuracy 0.7371\n",
      "Epoch 51 Batch 1100 Loss 0.7771 Accuracy 0.7368\n",
      "Epoch 51 Batch 1150 Loss 0.7771 Accuracy 0.7368\n",
      "Epoch 51 Batch 1200 Loss 0.7766 Accuracy 0.7370\n",
      "Epoch 51 Batch 1250 Loss 0.7773 Accuracy 0.7368\n",
      "Epoch 51 Batch 1300 Loss 0.7772 Accuracy 0.7367\n",
      "Epoch 51 Batch 1350 Loss 0.7771 Accuracy 0.7367\n",
      "Epoch 51 Batch 1400 Loss 0.7773 Accuracy 0.7366\n",
      "Epoch 51 Batch 1450 Loss 0.7776 Accuracy 0.7365\n",
      "Epoch 51 Batch 1500 Loss 0.7777 Accuracy 0.7365\n",
      "Epoch 51 Loss 0.7777 Accuracy 0.7365\n",
      "Time taken for 1 epoch: 37.01610255241394 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 51 VALIDATION: Loss 0.7600 Accuracy 0.7436\n",
      "\n",
      "la | tua | be| ni| gni| tà | non | pur | soc| cor| re|\n",
      "a | chi | do| man| da | ma | mol| te | fï| a| te|\n",
      "li| be| ra| men| te al | di| man| dar | pre| cor| re|\n",
      "                                        \n",
      " che | la | mia | di| man| da | la | mia | di| si| ri|\n",
      "che | si | fa | di| stin| zi | di | so| lea | stra| te|\n",
      "co| sì | che | si | fa | di | so| le | sua | pa| te|\n",
      "                                     \n",
      " co| sì | di| scer| na | se| gui| ta | si | spi| ri|\n",
      "che | si | fa | co| sì | di | sua | per | la | si| ri|\n",
      "che | si | fa | se| gui| ta | se| gui| ta | sta| te|\n",
      "  \n",
      " che | si | fa| ti| ca | di | so| le | si | ri| sta|\n",
      "co| me | si | ri| spuo| se | di | so| le | sta| te|\n",
      "che | si | fa| vi| glia | co| sì | che | si | sta| te|\n",
      "  \n",
      " co| me | si | fa | che | si | ri| spuo| se | sta| ti|\n",
      "che | la | spi| ri| ta | se| con| da | si | ri| spuo| se|\n",
      "co| me | si | ri| spuo| se | di | so| lo| sa | sta| ti|\n",
      "  \n",
      " che | si | fa| vi| glia | di | so| le | si | sta| vi|\n",
      "co| me | si | vi| vi | si | fa | se| con| da | sta| vi|\n",
      "che | la | sua | per | la | sua | co| sa | si| mi| glia|\n",
      "  \n",
      " che | si | fa| cea | di | so| le | se| gue | sa| li|\n",
      "co| me | si | fa | che | si | fa | si | ri| spo| glia|\n",
      "co| sì | co| me | si | ri| spuo| se | si | sta| li|\n",
      "  \n",
      " che | si | ve| de| re a | la | sua | la | sua | pi| ri|\n",
      "che | si | ri| spuo| se | di | so| le | si | ri| spu| se|\n",
      "co| me | si | ri| spuo| se | di | so| le | sta| vi|\n",
      "  \n",
      " che | si | fa| cea | di | so| le| va | si | sta| li|\n",
      "co| me | si | fa | che | si | fa | se| con| de| re|\n",
      "co| sì | co| me | si | ve| de| re a | la | spi| ri|\n",
      "  \n",
      " co| me | si | fa | che | si | ri| spuo| se | sta| vi|\n",
      "che | la | mia | di| stin| gen| do | si | ri| spuo| se|\n",
      "co| sì | co| me | si | fa | che | si | fa | sca| vi|\n",
      " che | si | fa| vi| glia | di | so| lo| re in | su| so|\n",
      "che | la | mia | di| stin| gen| do | si | ri| spuo| se|\n",
      "co| sì | co| me | si | fa | che | si | fa| vil| la|\n",
      "                                     \n",
      " co| sì | si | ri| spuo| se | di | so| le | sca| li|\n",
      "che | si | fa | se| gui| ta | se| gui| ta | sta| li|\n",
      "co| me | si | fa | se| con| do | li | spi| ri| ti|\n",
      "  \n",
      " che | si | fa| cea | di | so| lo| ri | si | sco| sta|\n",
      "co| me | si | ve| de| re a | la | sua | per| de| ci|\n",
      "che | si | fa | co| sì | co| sì | co| sì | sa| li|\n",
      "  \n",
      " che | si | fa | co| sì | co| me | si | ri| spo| sta|\n",
      "co| me | si | ve| de| re a | la | sua | per| ché | sta|\n",
      "che | si | ri| spuo| se | di | so| lo| re a | mi| ra|\n",
      "  \n",
      " che | si | fa| vi| glia | di | so| lo| le | sco| lo|\n",
      "co| me | si | fa | che | si | fa | se| con| de| re|\n",
      "co| me | si | ri| spuo| se | di | so| lo| re a| mi|\n",
      "  \n",
      " che | si | fa| vi| glia | di | so| le | si | sta| di|\n",
      "co| me | si | ve| de| re a | la | sua | per| de| se|\n",
      "che | si | fa | co| sì | co| me | si | fa| vi| glia|\n",
      "  \n",
      " che | si | fa | co| sì | co| me | si | ri| spuo| se|\n",
      "co| me | si | ve| de| re a | la | sua | per| de| re|\n",
      "che | si | ri| spuo| se | di | so| lo| sa | sci| gli|\n",
      "  \n",
      " che | si | fa| cea | di | là | do| ve | si | ri| so|\n",
      "co| me | si | ve| de| re a | la | sua | per| ché | so| la|\n",
      "che | si | ri| spuo| se | di | so| lo| re a| mi| glia|\n",
      "  \n",
      " co| me | si | fa | che | si | ri| spuo| se | sta| vi|\n",
      "che | la | mia | di| stin| gue | di | so| le | spi| gli|\n",
      "che | si | ri| spuo| se | di | so| lo| re a| mi| gli|\n",
      "\n",
      "epoch lasted: 441.1632025241852\n",
      "Epoch 52 Batch 0 Loss 0.7673 Accuracy 0.7508\n",
      "Epoch 52 Batch 50 Loss 0.7788 Accuracy 0.7373\n",
      "Epoch 52 Batch 100 Loss 0.7756 Accuracy 0.7381\n",
      "Epoch 52 Batch 150 Loss 0.7742 Accuracy 0.7385\n",
      "Epoch 52 Batch 200 Loss 0.7751 Accuracy 0.7385\n",
      "Epoch 52 Batch 250 Loss 0.7748 Accuracy 0.7385\n",
      "Epoch 52 Batch 300 Loss 0.7753 Accuracy 0.7386\n",
      "Epoch 52 Batch 350 Loss 0.7744 Accuracy 0.7383\n",
      "Epoch 52 Batch 400 Loss 0.7737 Accuracy 0.7383\n",
      "Epoch 52 Batch 450 Loss 0.7729 Accuracy 0.7386\n",
      "Epoch 52 Batch 500 Loss 0.7726 Accuracy 0.7386\n",
      "Epoch 52 Batch 550 Loss 0.7723 Accuracy 0.7387\n",
      "Epoch 52 Batch 600 Loss 0.7724 Accuracy 0.7387\n",
      "Epoch 52 Batch 650 Loss 0.7727 Accuracy 0.7385\n",
      "discarded batch 658\n",
      "Epoch 52 Batch 700 Loss 0.7733 Accuracy 0.7383\n",
      "Epoch 52 Batch 750 Loss 0.7732 Accuracy 0.7383\n",
      "Epoch 52 Batch 800 Loss 0.7733 Accuracy 0.7381\n",
      "Epoch 52 Batch 850 Loss 0.7740 Accuracy 0.7380\n",
      "Epoch 52 Batch 900 Loss 0.7745 Accuracy 0.7377\n",
      "Epoch 52 Batch 950 Loss 0.7752 Accuracy 0.7376\n",
      "Epoch 52 Batch 1000 Loss 0.7751 Accuracy 0.7377\n",
      "Epoch 52 Batch 1050 Loss 0.7755 Accuracy 0.7376\n",
      "Epoch 52 Batch 1100 Loss 0.7755 Accuracy 0.7376\n",
      "Epoch 52 Batch 1150 Loss 0.7754 Accuracy 0.7374\n",
      "Epoch 52 Batch 1200 Loss 0.7754 Accuracy 0.7374\n",
      "Epoch 52 Batch 1250 Loss 0.7753 Accuracy 0.7374\n",
      "Epoch 52 Batch 1300 Loss 0.7750 Accuracy 0.7375\n",
      "Epoch 52 Batch 1350 Loss 0.7752 Accuracy 0.7374\n",
      "Epoch 52 Batch 1400 Loss 0.7752 Accuracy 0.7374\n",
      "Epoch 52 Batch 1450 Loss 0.7753 Accuracy 0.7374\n",
      "Epoch 52 Batch 1500 Loss 0.7756 Accuracy 0.7373\n",
      "Epoch 52 Loss 0.7755 Accuracy 0.7373\n",
      "Time taken for 1 epoch: 36.448516845703125 secs\n",
      "\n",
      "epoch lasted: 36.452584743499756\n",
      "Epoch 53 Batch 0 Loss 0.7505 Accuracy 0.7658\n",
      "Epoch 53 Batch 50 Loss 0.7682 Accuracy 0.7373\n",
      "Epoch 53 Batch 100 Loss 0.7728 Accuracy 0.7372\n",
      "Epoch 53 Batch 150 Loss 0.7684 Accuracy 0.7393\n",
      "Epoch 53 Batch 200 Loss 0.7694 Accuracy 0.7387\n",
      "Epoch 53 Batch 250 Loss 0.7694 Accuracy 0.7386\n",
      "Epoch 53 Batch 300 Loss 0.7712 Accuracy 0.7382\n",
      "Epoch 53 Batch 350 Loss 0.7726 Accuracy 0.7377\n",
      "Epoch 53 Batch 400 Loss 0.7730 Accuracy 0.7371\n",
      "Epoch 53 Batch 450 Loss 0.7724 Accuracy 0.7378\n",
      "Epoch 53 Batch 500 Loss 0.7726 Accuracy 0.7381\n",
      "Epoch 53 Batch 550 Loss 0.7732 Accuracy 0.7380\n",
      "Epoch 53 Batch 600 Loss 0.7740 Accuracy 0.7380\n",
      "Epoch 53 Batch 650 Loss 0.7740 Accuracy 0.7380\n",
      "Epoch 53 Batch 700 Loss 0.7735 Accuracy 0.7381\n",
      "Epoch 53 Batch 750 Loss 0.7737 Accuracy 0.7380\n",
      "Epoch 53 Batch 800 Loss 0.7738 Accuracy 0.7379\n",
      "Epoch 53 Batch 850 Loss 0.7728 Accuracy 0.7382\n",
      "Epoch 53 Batch 900 Loss 0.7732 Accuracy 0.7381\n",
      "Epoch 53 Batch 950 Loss 0.7731 Accuracy 0.7380\n",
      "Epoch 53 Batch 1000 Loss 0.7727 Accuracy 0.7381\n",
      "Epoch 53 Batch 1050 Loss 0.7723 Accuracy 0.7383\n",
      "Epoch 53 Batch 1100 Loss 0.7729 Accuracy 0.7381\n",
      "Epoch 53 Batch 1150 Loss 0.7730 Accuracy 0.7381\n",
      "Epoch 53 Batch 1200 Loss 0.7729 Accuracy 0.7381\n",
      "Epoch 53 Batch 1250 Loss 0.7728 Accuracy 0.7382\n",
      "Epoch 53 Batch 1300 Loss 0.7725 Accuracy 0.7382\n",
      "discarded batch 1342\n",
      "Epoch 53 Batch 1350 Loss 0.7727 Accuracy 0.7382\n",
      "Epoch 53 Batch 1400 Loss 0.7727 Accuracy 0.7381\n",
      "Epoch 53 Batch 1450 Loss 0.7731 Accuracy 0.7381\n",
      "Epoch 53 Batch 1500 Loss 0.7730 Accuracy 0.7381\n",
      "Epoch 53 Loss 0.7729 Accuracy 0.7381\n",
      "Time taken for 1 epoch: 36.47673964500427 secs\n",
      "\n",
      "epoch lasted: 36.480313777923584\n",
      "Epoch 54 Batch 0 Loss 0.8645 Accuracy 0.7259\n",
      "Epoch 54 Batch 50 Loss 0.7647 Accuracy 0.7420\n",
      "Epoch 54 Batch 100 Loss 0.7656 Accuracy 0.7420\n",
      "Epoch 54 Batch 150 Loss 0.7635 Accuracy 0.7417\n",
      "Epoch 54 Batch 200 Loss 0.7648 Accuracy 0.7420\n",
      "Epoch 54 Batch 250 Loss 0.7662 Accuracy 0.7412\n",
      "Epoch 54 Batch 300 Loss 0.7658 Accuracy 0.7409\n",
      "Epoch 54 Batch 350 Loss 0.7656 Accuracy 0.7414\n",
      "Epoch 54 Batch 400 Loss 0.7675 Accuracy 0.7406\n",
      "Epoch 54 Batch 450 Loss 0.7678 Accuracy 0.7407\n",
      "Epoch 54 Batch 500 Loss 0.7682 Accuracy 0.7403\n",
      "Epoch 54 Batch 550 Loss 0.7690 Accuracy 0.7401\n",
      "Epoch 54 Batch 600 Loss 0.7696 Accuracy 0.7398\n",
      "Epoch 54 Batch 650 Loss 0.7695 Accuracy 0.7395\n",
      "Epoch 54 Batch 700 Loss 0.7692 Accuracy 0.7395\n",
      "Epoch 54 Batch 750 Loss 0.7691 Accuracy 0.7397\n",
      "Epoch 54 Batch 800 Loss 0.7695 Accuracy 0.7394\n",
      "Epoch 54 Batch 850 Loss 0.7700 Accuracy 0.7392\n",
      "Epoch 54 Batch 900 Loss 0.7703 Accuracy 0.7390\n",
      "Epoch 54 Batch 950 Loss 0.7703 Accuracy 0.7390\n",
      "Epoch 54 Batch 1000 Loss 0.7704 Accuracy 0.7389\n",
      "Epoch 54 Batch 1050 Loss 0.7700 Accuracy 0.7391\n",
      "Epoch 54 Batch 1100 Loss 0.7702 Accuracy 0.7390\n",
      "Epoch 54 Batch 1150 Loss 0.7703 Accuracy 0.7389\n",
      "Epoch 54 Batch 1200 Loss 0.7700 Accuracy 0.7390\n",
      "discarded batch 1229\n",
      "Epoch 54 Batch 1250 Loss 0.7696 Accuracy 0.7391\n",
      "Epoch 54 Batch 1300 Loss 0.7701 Accuracy 0.7389\n",
      "Epoch 54 Batch 1350 Loss 0.7701 Accuracy 0.7388\n",
      "Epoch 54 Batch 1400 Loss 0.7703 Accuracy 0.7388\n",
      "Epoch 54 Batch 1450 Loss 0.7700 Accuracy 0.7389\n",
      "Epoch 54 Batch 1500 Loss 0.7704 Accuracy 0.7388\n",
      "Epoch 54 Loss 0.7708 Accuracy 0.7386\n",
      "Time taken for 1 epoch: 36.76075720787048 secs\n",
      "\n",
      "epoch lasted: 36.76572012901306\n",
      "Epoch 55 Batch 0 Loss 0.8261 Accuracy 0.7143\n",
      "Epoch 55 Batch 50 Loss 0.7899 Accuracy 0.7323\n",
      "Epoch 55 Batch 100 Loss 0.7763 Accuracy 0.7363\n",
      "Epoch 55 Batch 150 Loss 0.7728 Accuracy 0.7370\n",
      "Epoch 55 Batch 200 Loss 0.7720 Accuracy 0.7375\n",
      "Epoch 55 Batch 250 Loss 0.7683 Accuracy 0.7384\n",
      "Epoch 55 Batch 300 Loss 0.7669 Accuracy 0.7387\n",
      "discarded batch 316\n",
      "Epoch 55 Batch 350 Loss 0.7662 Accuracy 0.7389\n",
      "Epoch 55 Batch 400 Loss 0.7662 Accuracy 0.7388\n",
      "Epoch 55 Batch 450 Loss 0.7669 Accuracy 0.7387\n",
      "Epoch 55 Batch 500 Loss 0.7672 Accuracy 0.7388\n",
      "Epoch 55 Batch 550 Loss 0.7675 Accuracy 0.7389\n",
      "Epoch 55 Batch 600 Loss 0.7684 Accuracy 0.7386\n",
      "Epoch 55 Batch 650 Loss 0.7679 Accuracy 0.7389\n",
      "Epoch 55 Batch 700 Loss 0.7678 Accuracy 0.7392\n",
      "Epoch 55 Batch 750 Loss 0.7692 Accuracy 0.7388\n",
      "Epoch 55 Batch 800 Loss 0.7690 Accuracy 0.7387\n",
      "Epoch 55 Batch 850 Loss 0.7692 Accuracy 0.7388\n",
      "Epoch 55 Batch 900 Loss 0.7698 Accuracy 0.7387\n",
      "Epoch 55 Batch 950 Loss 0.7697 Accuracy 0.7387\n",
      "Epoch 55 Batch 1000 Loss 0.7694 Accuracy 0.7389\n",
      "Epoch 55 Batch 1050 Loss 0.7685 Accuracy 0.7391\n",
      "Epoch 55 Batch 1100 Loss 0.7685 Accuracy 0.7391\n",
      "Epoch 55 Batch 1150 Loss 0.7690 Accuracy 0.7390\n",
      "Epoch 55 Batch 1200 Loss 0.7691 Accuracy 0.7389\n",
      "Epoch 55 Batch 1250 Loss 0.7687 Accuracy 0.7390\n",
      "Epoch 55 Batch 1300 Loss 0.7689 Accuracy 0.7390\n",
      "Epoch 55 Batch 1350 Loss 0.7691 Accuracy 0.7389\n",
      "Epoch 55 Batch 1400 Loss 0.7690 Accuracy 0.7389\n",
      "Epoch 55 Batch 1450 Loss 0.7693 Accuracy 0.7389\n",
      "Epoch 55 Batch 1500 Loss 0.7693 Accuracy 0.7389\n",
      "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-11\n",
      "Epoch 55 Loss 0.7692 Accuracy 0.7389\n",
      "Time taken for 1 epoch: 36.767093896865845 secs\n",
      "\n",
      "epoch lasted: 36.77155423164368\n",
      "Epoch 56 Batch 0 Loss 0.7720 Accuracy 0.7326\n",
      "Epoch 56 Batch 50 Loss 0.7534 Accuracy 0.7442\n",
      "Epoch 56 Batch 100 Loss 0.7562 Accuracy 0.7442\n",
      "Epoch 56 Batch 150 Loss 0.7607 Accuracy 0.7430\n",
      "Epoch 56 Batch 200 Loss 0.7612 Accuracy 0.7426\n",
      "Epoch 56 Batch 250 Loss 0.7626 Accuracy 0.7417\n",
      "Epoch 56 Batch 300 Loss 0.7632 Accuracy 0.7417\n",
      "Epoch 56 Batch 350 Loss 0.7648 Accuracy 0.7411\n",
      "Epoch 56 Batch 400 Loss 0.7641 Accuracy 0.7412\n",
      "Epoch 56 Batch 450 Loss 0.7656 Accuracy 0.7405\n",
      "Epoch 56 Batch 500 Loss 0.7651 Accuracy 0.7406\n",
      "Epoch 56 Batch 550 Loss 0.7631 Accuracy 0.7412\n",
      "Epoch 56 Batch 600 Loss 0.7635 Accuracy 0.7410\n",
      "Epoch 56 Batch 650 Loss 0.7641 Accuracy 0.7408\n",
      "Epoch 56 Batch 700 Loss 0.7639 Accuracy 0.7410\n",
      "Epoch 56 Batch 750 Loss 0.7645 Accuracy 0.7407\n",
      "discarded batch 767\n",
      "Epoch 56 Batch 800 Loss 0.7647 Accuracy 0.7408\n",
      "Epoch 56 Batch 850 Loss 0.7648 Accuracy 0.7407\n",
      "Epoch 56 Batch 900 Loss 0.7648 Accuracy 0.7407\n",
      "Epoch 56 Batch 950 Loss 0.7654 Accuracy 0.7404\n",
      "Epoch 56 Batch 1000 Loss 0.7658 Accuracy 0.7403\n",
      "Epoch 56 Batch 1050 Loss 0.7656 Accuracy 0.7404\n",
      "Epoch 56 Batch 1100 Loss 0.7656 Accuracy 0.7403\n",
      "Epoch 56 Batch 1150 Loss 0.7658 Accuracy 0.7403\n",
      "Epoch 56 Batch 1200 Loss 0.7659 Accuracy 0.7402\n",
      "Epoch 56 Batch 1250 Loss 0.7661 Accuracy 0.7401\n",
      "Epoch 56 Batch 1300 Loss 0.7657 Accuracy 0.7403\n",
      "Epoch 56 Batch 1350 Loss 0.7664 Accuracy 0.7402\n",
      "Epoch 56 Batch 1400 Loss 0.7664 Accuracy 0.7402\n",
      "Epoch 56 Batch 1450 Loss 0.7668 Accuracy 0.7401\n",
      "Epoch 56 Batch 1500 Loss 0.7664 Accuracy 0.7402\n",
      "Epoch 56 Loss 0.7667 Accuracy 0.7401\n",
      "Time taken for 1 epoch: 36.594385862350464 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 56 VALIDATION: Loss 0.7487 Accuracy 0.7494\n",
      "\n",
      "epoch lasted: 36.74283790588379\n",
      "Epoch 57 Batch 0 Loss 0.8275 Accuracy 0.7143\n",
      "Epoch 57 Batch 50 Loss 0.7646 Accuracy 0.7404\n",
      "Epoch 57 Batch 100 Loss 0.7582 Accuracy 0.7419\n",
      "Epoch 57 Batch 150 Loss 0.7577 Accuracy 0.7426\n",
      "Epoch 57 Batch 200 Loss 0.7620 Accuracy 0.7417\n",
      "Epoch 57 Batch 250 Loss 0.7621 Accuracy 0.7414\n",
      "Epoch 57 Batch 300 Loss 0.7615 Accuracy 0.7419\n",
      "Epoch 57 Batch 350 Loss 0.7627 Accuracy 0.7415\n",
      "Epoch 57 Batch 400 Loss 0.7632 Accuracy 0.7412\n",
      "Epoch 57 Batch 450 Loss 0.7636 Accuracy 0.7412\n",
      "Epoch 57 Batch 500 Loss 0.7633 Accuracy 0.7412\n",
      "Epoch 57 Batch 550 Loss 0.7624 Accuracy 0.7416\n",
      "Epoch 57 Batch 600 Loss 0.7627 Accuracy 0.7416\n",
      "Epoch 57 Batch 650 Loss 0.7626 Accuracy 0.7416\n",
      "Epoch 57 Batch 700 Loss 0.7626 Accuracy 0.7416\n",
      "Epoch 57 Batch 750 Loss 0.7627 Accuracy 0.7416\n",
      "Epoch 57 Batch 800 Loss 0.7634 Accuracy 0.7414\n",
      "Epoch 57 Batch 850 Loss 0.7640 Accuracy 0.7410\n",
      "Epoch 57 Batch 900 Loss 0.7644 Accuracy 0.7410\n",
      "Epoch 57 Batch 950 Loss 0.7642 Accuracy 0.7411\n",
      "Epoch 57 Batch 1000 Loss 0.7645 Accuracy 0.7410\n",
      "discarded batch 1035\n",
      "Epoch 57 Batch 1050 Loss 0.7645 Accuracy 0.7410\n",
      "Epoch 57 Batch 1100 Loss 0.7646 Accuracy 0.7409\n",
      "Epoch 57 Batch 1150 Loss 0.7648 Accuracy 0.7408\n",
      "Epoch 57 Batch 1200 Loss 0.7649 Accuracy 0.7408\n",
      "Epoch 57 Batch 1250 Loss 0.7651 Accuracy 0.7409\n",
      "Epoch 57 Batch 1300 Loss 0.7654 Accuracy 0.7407\n",
      "Epoch 57 Batch 1350 Loss 0.7657 Accuracy 0.7406\n",
      "Epoch 57 Batch 1400 Loss 0.7658 Accuracy 0.7405\n",
      "Epoch 57 Batch 1450 Loss 0.7657 Accuracy 0.7406\n",
      "Epoch 57 Batch 1500 Loss 0.7656 Accuracy 0.7406\n",
      "Epoch 57 Loss 0.7655 Accuracy 0.7406\n",
      "Time taken for 1 epoch: 36.6374351978302 secs\n",
      "\n",
      "epoch lasted: 36.64114475250244\n",
      "Epoch 58 Batch 0 Loss 0.6970 Accuracy 0.7558\n",
      "Epoch 58 Batch 50 Loss 0.7624 Accuracy 0.7403\n",
      "Epoch 58 Batch 100 Loss 0.7564 Accuracy 0.7418\n",
      "Epoch 58 Batch 150 Loss 0.7544 Accuracy 0.7425\n",
      "Epoch 58 Batch 200 Loss 0.7523 Accuracy 0.7435\n",
      "Epoch 58 Batch 250 Loss 0.7538 Accuracy 0.7441\n",
      "Epoch 58 Batch 300 Loss 0.7562 Accuracy 0.7434\n",
      "Epoch 58 Batch 350 Loss 0.7578 Accuracy 0.7425\n",
      "Epoch 58 Batch 400 Loss 0.7586 Accuracy 0.7425\n",
      "Epoch 58 Batch 450 Loss 0.7584 Accuracy 0.7425\n",
      "Epoch 58 Batch 500 Loss 0.7589 Accuracy 0.7425\n",
      "Epoch 58 Batch 550 Loss 0.7581 Accuracy 0.7429\n",
      "discarded batch 555\n",
      "Epoch 58 Batch 600 Loss 0.7580 Accuracy 0.7430\n",
      "Epoch 58 Batch 650 Loss 0.7582 Accuracy 0.7428\n",
      "Epoch 58 Batch 700 Loss 0.7589 Accuracy 0.7425\n",
      "Epoch 58 Batch 750 Loss 0.7587 Accuracy 0.7425\n",
      "Epoch 58 Batch 800 Loss 0.7589 Accuracy 0.7424\n",
      "Epoch 58 Batch 850 Loss 0.7588 Accuracy 0.7425\n",
      "Epoch 58 Batch 900 Loss 0.7595 Accuracy 0.7423\n",
      "Epoch 58 Batch 950 Loss 0.7598 Accuracy 0.7421\n",
      "Epoch 58 Batch 1000 Loss 0.7598 Accuracy 0.7421\n",
      "Epoch 58 Batch 1050 Loss 0.7603 Accuracy 0.7419\n",
      "Epoch 58 Batch 1100 Loss 0.7610 Accuracy 0.7418\n",
      "Epoch 58 Batch 1150 Loss 0.7611 Accuracy 0.7418\n",
      "Epoch 58 Batch 1200 Loss 0.7608 Accuracy 0.7419\n",
      "Epoch 58 Batch 1250 Loss 0.7614 Accuracy 0.7417\n",
      "Epoch 58 Batch 1300 Loss 0.7615 Accuracy 0.7417\n",
      "Epoch 58 Batch 1350 Loss 0.7619 Accuracy 0.7416\n",
      "Epoch 58 Batch 1400 Loss 0.7618 Accuracy 0.7416\n",
      "Epoch 58 Batch 1450 Loss 0.7619 Accuracy 0.7416\n",
      "Epoch 58 Batch 1500 Loss 0.7624 Accuracy 0.7415\n",
      "Epoch 58 Loss 0.7627 Accuracy 0.7414\n",
      "Time taken for 1 epoch: 36.54978680610657 secs\n",
      "\n",
      "epoch lasted: 36.55384111404419\n",
      "Epoch 59 Batch 0 Loss 0.7732 Accuracy 0.7492\n",
      "Epoch 59 Batch 50 Loss 0.7501 Accuracy 0.7457\n",
      "discarded batch 80\n",
      "Epoch 59 Batch 100 Loss 0.7547 Accuracy 0.7442\n",
      "Epoch 59 Batch 150 Loss 0.7531 Accuracy 0.7447\n",
      "Epoch 59 Batch 200 Loss 0.7552 Accuracy 0.7444\n",
      "Epoch 59 Batch 250 Loss 0.7551 Accuracy 0.7443\n",
      "Epoch 59 Batch 300 Loss 0.7550 Accuracy 0.7441\n",
      "Epoch 59 Batch 350 Loss 0.7552 Accuracy 0.7439\n",
      "Epoch 59 Batch 400 Loss 0.7550 Accuracy 0.7439\n",
      "Epoch 59 Batch 450 Loss 0.7549 Accuracy 0.7437\n",
      "Epoch 59 Batch 500 Loss 0.7549 Accuracy 0.7435\n",
      "Epoch 59 Batch 550 Loss 0.7553 Accuracy 0.7434\n",
      "Epoch 59 Batch 600 Loss 0.7562 Accuracy 0.7431\n",
      "Epoch 59 Batch 650 Loss 0.7572 Accuracy 0.7429\n",
      "Epoch 59 Batch 700 Loss 0.7573 Accuracy 0.7430\n",
      "Epoch 59 Batch 750 Loss 0.7578 Accuracy 0.7428\n",
      "Epoch 59 Batch 800 Loss 0.7584 Accuracy 0.7424\n",
      "Epoch 59 Batch 850 Loss 0.7585 Accuracy 0.7425\n",
      "Epoch 59 Batch 900 Loss 0.7588 Accuracy 0.7424\n",
      "Epoch 59 Batch 950 Loss 0.7592 Accuracy 0.7422\n",
      "Epoch 59 Batch 1000 Loss 0.7597 Accuracy 0.7421\n",
      "Epoch 59 Batch 1050 Loss 0.7597 Accuracy 0.7422\n",
      "Epoch 59 Batch 1100 Loss 0.7598 Accuracy 0.7422\n",
      "Epoch 59 Batch 1150 Loss 0.7595 Accuracy 0.7422\n",
      "Epoch 59 Batch 1200 Loss 0.7596 Accuracy 0.7424\n",
      "Epoch 59 Batch 1250 Loss 0.7600 Accuracy 0.7422\n",
      "Epoch 59 Batch 1300 Loss 0.7606 Accuracy 0.7420\n",
      "Epoch 59 Batch 1350 Loss 0.7610 Accuracy 0.7418\n",
      "Epoch 59 Batch 1400 Loss 0.7614 Accuracy 0.7417\n",
      "Epoch 59 Batch 1450 Loss 0.7615 Accuracy 0.7416\n",
      "Epoch 59 Batch 1500 Loss 0.7614 Accuracy 0.7417\n",
      "Epoch 59 Loss 0.7610 Accuracy 0.7419\n",
      "Time taken for 1 epoch: 36.52712154388428 secs\n",
      "\n",
      "epoch lasted: 36.53168511390686\n",
      "Epoch 60 Batch 0 Loss 0.6955 Accuracy 0.7641\n",
      "Epoch 60 Batch 50 Loss 0.7544 Accuracy 0.7454\n",
      "Epoch 60 Batch 100 Loss 0.7495 Accuracy 0.7474\n",
      "Epoch 60 Batch 150 Loss 0.7533 Accuracy 0.7456\n",
      "Epoch 60 Batch 200 Loss 0.7553 Accuracy 0.7443\n",
      "Epoch 60 Batch 250 Loss 0.7557 Accuracy 0.7438\n",
      "Epoch 60 Batch 300 Loss 0.7553 Accuracy 0.7441\n",
      "Epoch 60 Batch 350 Loss 0.7555 Accuracy 0.7440\n",
      "Epoch 60 Batch 400 Loss 0.7558 Accuracy 0.7438\n",
      "Epoch 60 Batch 450 Loss 0.7557 Accuracy 0.7436\n",
      "discarded batch 487\n",
      "Epoch 60 Batch 500 Loss 0.7550 Accuracy 0.7439\n",
      "Epoch 60 Batch 550 Loss 0.7552 Accuracy 0.7438\n",
      "Epoch 60 Batch 600 Loss 0.7551 Accuracy 0.7439\n",
      "Epoch 60 Batch 650 Loss 0.7544 Accuracy 0.7440\n",
      "Epoch 60 Batch 700 Loss 0.7549 Accuracy 0.7439\n",
      "Epoch 60 Batch 750 Loss 0.7553 Accuracy 0.7438\n",
      "Epoch 60 Batch 800 Loss 0.7556 Accuracy 0.7435\n",
      "Epoch 60 Batch 850 Loss 0.7561 Accuracy 0.7434\n",
      "Epoch 60 Batch 900 Loss 0.7570 Accuracy 0.7432\n",
      "Epoch 60 Batch 950 Loss 0.7571 Accuracy 0.7431\n",
      "Epoch 60 Batch 1000 Loss 0.7572 Accuracy 0.7431\n",
      "Epoch 60 Batch 1050 Loss 0.7575 Accuracy 0.7430\n",
      "Epoch 60 Batch 1100 Loss 0.7576 Accuracy 0.7429\n",
      "Epoch 60 Batch 1150 Loss 0.7577 Accuracy 0.7429\n",
      "Epoch 60 Batch 1200 Loss 0.7581 Accuracy 0.7429\n",
      "Epoch 60 Batch 1250 Loss 0.7586 Accuracy 0.7428\n",
      "Epoch 60 Batch 1300 Loss 0.7587 Accuracy 0.7428\n",
      "Epoch 60 Batch 1350 Loss 0.7587 Accuracy 0.7428\n",
      "Epoch 60 Batch 1400 Loss 0.7588 Accuracy 0.7427\n",
      "Epoch 60 Batch 1450 Loss 0.7593 Accuracy 0.7426\n",
      "Epoch 60 Batch 1500 Loss 0.7593 Accuracy 0.7426\n",
      "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-12\n",
      "Epoch 60 Loss 0.7592 Accuracy 0.7426\n",
      "Time taken for 1 epoch: 37.111393451690674 secs\n",
      "\n",
      "epoch lasted: 37.115251302719116\n",
      "Epoch 61 Batch 0 Loss 0.6899 Accuracy 0.7525\n",
      "Epoch 61 Batch 50 Loss 0.7541 Accuracy 0.7451\n",
      "discarded batch 95\n",
      "Epoch 61 Batch 100 Loss 0.7555 Accuracy 0.7430\n",
      "Epoch 61 Batch 150 Loss 0.7540 Accuracy 0.7434\n",
      "Epoch 61 Batch 200 Loss 0.7534 Accuracy 0.7443\n",
      "Epoch 61 Batch 250 Loss 0.7519 Accuracy 0.7451\n",
      "Epoch 61 Batch 300 Loss 0.7531 Accuracy 0.7449\n",
      "Epoch 61 Batch 350 Loss 0.7517 Accuracy 0.7458\n",
      "Epoch 61 Batch 400 Loss 0.7522 Accuracy 0.7456\n",
      "Epoch 61 Batch 450 Loss 0.7518 Accuracy 0.7457\n",
      "Epoch 61 Batch 500 Loss 0.7536 Accuracy 0.7451\n",
      "Epoch 61 Batch 550 Loss 0.7541 Accuracy 0.7447\n",
      "Epoch 61 Batch 600 Loss 0.7542 Accuracy 0.7447\n",
      "Epoch 61 Batch 650 Loss 0.7555 Accuracy 0.7443\n",
      "Epoch 61 Batch 700 Loss 0.7557 Accuracy 0.7442\n",
      "Epoch 61 Batch 750 Loss 0.7557 Accuracy 0.7443\n",
      "Epoch 61 Batch 800 Loss 0.7554 Accuracy 0.7443\n",
      "Epoch 61 Batch 850 Loss 0.7561 Accuracy 0.7440\n",
      "Epoch 61 Batch 900 Loss 0.7562 Accuracy 0.7439\n",
      "Epoch 61 Batch 950 Loss 0.7557 Accuracy 0.7441\n",
      "Epoch 61 Batch 1000 Loss 0.7557 Accuracy 0.7440\n",
      "Epoch 61 Batch 1050 Loss 0.7556 Accuracy 0.7441\n",
      "Epoch 61 Batch 1100 Loss 0.7556 Accuracy 0.7440\n",
      "Epoch 61 Batch 1150 Loss 0.7556 Accuracy 0.7440\n",
      "Epoch 61 Batch 1200 Loss 0.7561 Accuracy 0.7437\n",
      "Epoch 61 Batch 1250 Loss 0.7568 Accuracy 0.7436\n",
      "Epoch 61 Batch 1300 Loss 0.7567 Accuracy 0.7436\n",
      "Epoch 61 Batch 1350 Loss 0.7570 Accuracy 0.7436\n",
      "Epoch 61 Batch 1400 Loss 0.7573 Accuracy 0.7436\n",
      "Epoch 61 Batch 1450 Loss 0.7575 Accuracy 0.7434\n",
      "Epoch 61 Batch 1500 Loss 0.7572 Accuracy 0.7435\n",
      "Epoch 61 Loss 0.7571 Accuracy 0.7435\n",
      "Time taken for 1 epoch: 36.46198654174805 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 61 VALIDATION: Loss 0.7452 Accuracy 0.7492\n",
      "\n",
      "la | tua | be| ni| gni| tà | non | pur | soc| cor| re|\n",
      "a | chi | do| man| da | ma | mol| te | fï| a| te|\n",
      "li| be| ra| men| te al | di| man| dar | pre| cor| re|\n",
      "                                        \n",
      " che | si | vo| le| va | si | vol| ge | si | sta| te|\n",
      "che | sie| te | sie| te | si | vo| le| sti | sta| te|\n",
      "che | si | vol| ge | si | vol| ge | si | pa| re| sti|\n",
      "                                       \n",
      " co| me | si | vol| ge | si | fa| ce | si | spa| da|\n",
      "che | sie| mi| na| ti | se| gui| ta | si | sta| da|\n",
      "che | si | vol| ge | si | fa| ce| va | si | ri| vo| la|\n",
      "  \n",
      "che | sie| mi | vi| sta| va | sì | che | si | sta| da|\n",
      "co| me | si | vol| ge | se| gui| ta| va | si | sta|\n",
      "che | si | vo| le| sti | si | vol| ge | si | sta| da|\n",
      "  \n",
      " che | si | vol| ge | si | vol| ge | si | ri| spon| da|\n",
      "co| me | si | vol| se | che | si | ri| vol| ge | sta| da|\n",
      "co| me | si | vol| se | che | si | ri| vol| ge | sta| da|\n",
      "  \n",
      "e | di| ste| sti| ma | che | si | ri| vol| ge | sti| ma|\n",
      "che | sie| mi| na| ti | sta| va | si | ri| vol| ge|\n",
      "co| sì | la | spi| ra | di | so| lo a | lui | vi| vo|\n",
      " che | se| gui| ta|\n",
      "co| me | si | vo| le| sti | si | spi| ra|\n",
      "che | si | vol| ge | se | tu | ve| di | si | spi| ra|\n",
      "che | si | vol| ge | si | vol| ge | si | ri| spo| lo|\n",
      "                               \n",
      " co| me | si | vol| ge | si | vol| ge | si | sta| li|\n",
      "che | sie| mi| na| ti | se| gui| ta | si | ri| vol| ta|\n",
      "che | sie| te | si | fa| ce| va | sì | che | sta| li|\n",
      "  \n",
      "che | sie| mi | vi| sta | vi| sta | si | ri| spuo| se|\n",
      "co| me | si | vol| ge | se | tu | vi| ve | si| stra|\n",
      "che | sie| me | si | vol| ge | si | ri| vol| ge| va|\n",
      "  \n",
      "che | sie| me | se| gui| ta| va | si | ri| vol| ge|\n",
      "che | sie| te | sie| te | si| mi| li | sta| vi| vo|\n",
      "che | si | vol| ge | si | vol| ge | si | ri| spon| da|\n",
      "  \n",
      "e | di| sco| sta| va | co| me | si | ri| vol| ge|\n",
      "che | sie| te | se| gui| ta | se| gui| ta | si| vol| ge|\n",
      "che | si | voi | si | vol| ge | si | vol| ge | sti| vo|\n",
      " che | se| gui| ta| di| nan| do | si | ri| spuo| se|\n",
      "che | sie| te | se| gui| ta| va | se| gui| ta| di|\n",
      "co| sì | la | spi| ra | si | vol| ge | si | po| se|\n",
      "                                             \n",
      " che | si | vol| ge | si | vol| ge | si | ri| vol| ge|\n",
      "co| me | si | vol| ge | se | non | si | ri| vol| ge|\n",
      "che | sie| te | si | ri| vol| ge | si | ri| vol| ge|\n",
      "  \n",
      "che | sie| me | vi| ve | si | vol| ge | si | stra| gi|\n",
      "co| me | si | vol| gi| ne in | su | la | vi| sta| gi|\n",
      "che | si | vol| gi| ne in | su | la | vi| sta | vi| va|\n",
      " che | se| gui| ta|\n",
      "che | si | vo| le| sti | si | ri| vol| gi|\n",
      "che | si | vol| ge | se | tu | ve| ni | si | spi| sta|\n",
      "co| sì | la | vi| sta | se| gui| ta | si | stri| sta|\n",
      "                             \n",
      " che | si | vol| gi| ne in | su | la | sua | vi| sta| sta|\n",
      "co| sì | che | si | pa| re| va | si | ri| vol| gi|\n",
      "che | sie| te | si | fa| vil| la | se| gui| ta | sti| sta|\n",
      "  \n",
      "che | si | vol| gi| gi | se| gui| ta | si | ri| spo| sta|\n",
      "co| me | si | vol| ge | sta| va | se| gui| ta| vi|\n",
      "che | si | vo| le| sti | si | vol| ge | si | sta| vi|\n",
      "  \n",
      "e | poi | che | sie| me | si | vol| ge | si | sta| va|\n",
      "che | sie| mi| gliar | li | sta| va | si | ri| spo| sta|\n",
      "co| me | si | vol| ge | che | si | vol| ge | sta| va|\n",
      " che | si | vol| gen| do | si | vol| ge | si | sta| va| va|\n",
      "che | sie| mi | si | ri| vol| se | che | si | ri| spon| da|\n",
      "co| me | si | vol| ge | si | ri| vol| ge | sta| va|\n",
      "                             \n",
      " che | si | vol| ge | si | vol| ge | si | ri| vol| ge|\n",
      "co| sì | la | sua | pa| rea | che | si | ri| vol| ge|\n",
      "che | sie| te | si | ri| vol| ge | si | ri| vol| ge|\n",
      "  \n",
      "che | sie| me | si | vol| ge | la | spi| ri| ta | vi| vo|\n",
      "che | sie| te | si | vol| ge | si | ri| vol| gi| vo|\n",
      "co| sì | la | spi| ra | di | sua | pa| ro| la| da|\n",
      "  \n",
      "che | si | vol| gi| ne in | su | la | sua | vi| sta| va|\n",
      "co| sì | che | si | pa| re| va | se| gui| ta | spi| ra|\n",
      "co| sì | la | sua | vi| sta | che | si | sta| va| va|\n",
      "\n",
      "epoch lasted: 516.4711945056915\n",
      "Epoch 62 Batch 0 Loss 0.7015 Accuracy 0.7674\n",
      "Epoch 62 Batch 50 Loss 0.7501 Accuracy 0.7428\n",
      "discarded batch 93\n",
      "Epoch 62 Batch 100 Loss 0.7533 Accuracy 0.7427\n",
      "Epoch 62 Batch 150 Loss 0.7547 Accuracy 0.7428\n",
      "Epoch 62 Batch 200 Loss 0.7527 Accuracy 0.7438\n",
      "Epoch 62 Batch 250 Loss 0.7520 Accuracy 0.7442\n",
      "Epoch 62 Batch 300 Loss 0.7524 Accuracy 0.7440\n",
      "Epoch 62 Batch 350 Loss 0.7519 Accuracy 0.7443\n",
      "Epoch 62 Batch 400 Loss 0.7536 Accuracy 0.7440\n",
      "Epoch 62 Batch 450 Loss 0.7525 Accuracy 0.7443\n",
      "Epoch 62 Batch 500 Loss 0.7526 Accuracy 0.7441\n",
      "Epoch 62 Batch 550 Loss 0.7527 Accuracy 0.7441\n",
      "Epoch 62 Batch 600 Loss 0.7527 Accuracy 0.7440\n",
      "Epoch 62 Batch 650 Loss 0.7526 Accuracy 0.7441\n",
      "Epoch 62 Batch 700 Loss 0.7527 Accuracy 0.7443\n",
      "Epoch 62 Batch 750 Loss 0.7530 Accuracy 0.7442\n",
      "Epoch 62 Batch 800 Loss 0.7541 Accuracy 0.7440\n",
      "Epoch 62 Batch 850 Loss 0.7544 Accuracy 0.7438\n",
      "Epoch 62 Batch 900 Loss 0.7541 Accuracy 0.7440\n",
      "Epoch 62 Batch 950 Loss 0.7537 Accuracy 0.7442\n",
      "Epoch 62 Batch 1000 Loss 0.7540 Accuracy 0.7441\n",
      "Epoch 62 Batch 1050 Loss 0.7542 Accuracy 0.7441\n",
      "Epoch 62 Batch 1100 Loss 0.7548 Accuracy 0.7439\n",
      "Epoch 62 Batch 1150 Loss 0.7547 Accuracy 0.7440\n",
      "Epoch 62 Batch 1200 Loss 0.7551 Accuracy 0.7441\n",
      "Epoch 62 Batch 1250 Loss 0.7550 Accuracy 0.7441\n",
      "Epoch 62 Batch 1300 Loss 0.7552 Accuracy 0.7441\n",
      "Epoch 62 Batch 1350 Loss 0.7552 Accuracy 0.7441\n",
      "Epoch 62 Batch 1400 Loss 0.7553 Accuracy 0.7441\n",
      "Epoch 62 Batch 1450 Loss 0.7554 Accuracy 0.7440\n",
      "Epoch 62 Batch 1500 Loss 0.7555 Accuracy 0.7440\n",
      "Epoch 62 Loss 0.7554 Accuracy 0.7440\n",
      "Time taken for 1 epoch: 36.89901280403137 secs\n",
      "\n",
      "epoch lasted: 36.90284538269043\n",
      "Epoch 63 Batch 0 Loss 0.7936 Accuracy 0.7326\n",
      "Epoch 63 Batch 50 Loss 0.7451 Accuracy 0.7472\n",
      "discarded batch 51\n",
      "Epoch 63 Batch 100 Loss 0.7414 Accuracy 0.7483\n",
      "Epoch 63 Batch 150 Loss 0.7433 Accuracy 0.7477\n",
      "Epoch 63 Batch 200 Loss 0.7430 Accuracy 0.7474\n",
      "Epoch 63 Batch 250 Loss 0.7446 Accuracy 0.7472\n",
      "Epoch 63 Batch 300 Loss 0.7469 Accuracy 0.7470\n",
      "Epoch 63 Batch 350 Loss 0.7467 Accuracy 0.7474\n",
      "Epoch 63 Batch 400 Loss 0.7465 Accuracy 0.7473\n",
      "Epoch 63 Batch 450 Loss 0.7490 Accuracy 0.7465\n",
      "Epoch 63 Batch 500 Loss 0.7496 Accuracy 0.7465\n",
      "Epoch 63 Batch 550 Loss 0.7506 Accuracy 0.7459\n",
      "Epoch 63 Batch 600 Loss 0.7507 Accuracy 0.7459\n",
      "Epoch 63 Batch 650 Loss 0.7518 Accuracy 0.7455\n",
      "Epoch 63 Batch 700 Loss 0.7516 Accuracy 0.7455\n",
      "Epoch 63 Batch 750 Loss 0.7524 Accuracy 0.7451\n",
      "Epoch 63 Batch 800 Loss 0.7521 Accuracy 0.7452\n",
      "Epoch 63 Batch 850 Loss 0.7529 Accuracy 0.7449\n",
      "Epoch 63 Batch 900 Loss 0.7529 Accuracy 0.7449\n",
      "Epoch 63 Batch 950 Loss 0.7534 Accuracy 0.7447\n",
      "Epoch 63 Batch 1000 Loss 0.7531 Accuracy 0.7448\n",
      "Epoch 63 Batch 1050 Loss 0.7525 Accuracy 0.7450\n",
      "Epoch 63 Batch 1100 Loss 0.7526 Accuracy 0.7449\n",
      "Epoch 63 Batch 1150 Loss 0.7526 Accuracy 0.7448\n",
      "Epoch 63 Batch 1200 Loss 0.7528 Accuracy 0.7448\n",
      "Epoch 63 Batch 1250 Loss 0.7529 Accuracy 0.7448\n",
      "Epoch 63 Batch 1300 Loss 0.7529 Accuracy 0.7447\n",
      "Epoch 63 Batch 1350 Loss 0.7526 Accuracy 0.7448\n",
      "Epoch 63 Batch 1400 Loss 0.7524 Accuracy 0.7449\n",
      "Epoch 63 Batch 1450 Loss 0.7527 Accuracy 0.7449\n",
      "Epoch 63 Batch 1500 Loss 0.7532 Accuracy 0.7448\n",
      "Epoch 63 Loss 0.7533 Accuracy 0.7447\n",
      "Time taken for 1 epoch: 36.50468921661377 secs\n",
      "\n",
      "epoch lasted: 36.508280992507935\n",
      "Epoch 64 Batch 0 Loss 0.7463 Accuracy 0.7409\n",
      "Epoch 64 Batch 50 Loss 0.7495 Accuracy 0.7461\n",
      "Epoch 64 Batch 100 Loss 0.7458 Accuracy 0.7476\n",
      "Epoch 64 Batch 150 Loss 0.7477 Accuracy 0.7471\n",
      "Epoch 64 Batch 200 Loss 0.7509 Accuracy 0.7466\n",
      "Epoch 64 Batch 250 Loss 0.7493 Accuracy 0.7468\n",
      "Epoch 64 Batch 300 Loss 0.7502 Accuracy 0.7465\n",
      "Epoch 64 Batch 350 Loss 0.7511 Accuracy 0.7459\n",
      "Epoch 64 Batch 400 Loss 0.7520 Accuracy 0.7457\n",
      "Epoch 64 Batch 450 Loss 0.7511 Accuracy 0.7457\n",
      "Epoch 64 Batch 500 Loss 0.7495 Accuracy 0.7460\n",
      "Epoch 64 Batch 550 Loss 0.7488 Accuracy 0.7459\n",
      "Epoch 64 Batch 600 Loss 0.7499 Accuracy 0.7455\n",
      "Epoch 64 Batch 650 Loss 0.7495 Accuracy 0.7455\n",
      "Epoch 64 Batch 700 Loss 0.7502 Accuracy 0.7453\n",
      "Epoch 64 Batch 750 Loss 0.7505 Accuracy 0.7453\n",
      "Epoch 64 Batch 800 Loss 0.7509 Accuracy 0.7453\n",
      "Epoch 64 Batch 850 Loss 0.7513 Accuracy 0.7452\n",
      "Epoch 64 Batch 900 Loss 0.7514 Accuracy 0.7452\n",
      "Epoch 64 Batch 950 Loss 0.7512 Accuracy 0.7453\n",
      "Epoch 64 Batch 1000 Loss 0.7515 Accuracy 0.7452\n",
      "discarded batch 1014\n",
      "Epoch 64 Batch 1050 Loss 0.7517 Accuracy 0.7452\n",
      "Epoch 64 Batch 1100 Loss 0.7514 Accuracy 0.7453\n",
      "Epoch 64 Batch 1150 Loss 0.7516 Accuracy 0.7452\n",
      "Epoch 64 Batch 1200 Loss 0.7517 Accuracy 0.7453\n",
      "Epoch 64 Batch 1250 Loss 0.7520 Accuracy 0.7452\n",
      "Epoch 64 Batch 1300 Loss 0.7519 Accuracy 0.7452\n",
      "Epoch 64 Batch 1350 Loss 0.7515 Accuracy 0.7453\n",
      "Epoch 64 Batch 1400 Loss 0.7517 Accuracy 0.7452\n",
      "Epoch 64 Batch 1450 Loss 0.7518 Accuracy 0.7452\n",
      "Epoch 64 Batch 1500 Loss 0.7519 Accuracy 0.7452\n",
      "Epoch 64 Loss 0.7519 Accuracy 0.7452\n",
      "Time taken for 1 epoch: 36.25047707557678 secs\n",
      "\n",
      "epoch lasted: 36.25500130653381\n",
      "Epoch 65 Batch 0 Loss 0.7560 Accuracy 0.7525\n",
      "Epoch 65 Batch 50 Loss 0.7369 Accuracy 0.7495\n",
      "Epoch 65 Batch 100 Loss 0.7384 Accuracy 0.7492\n",
      "Epoch 65 Batch 150 Loss 0.7409 Accuracy 0.7490\n",
      "Epoch 65 Batch 200 Loss 0.7433 Accuracy 0.7476\n",
      "Epoch 65 Batch 250 Loss 0.7446 Accuracy 0.7473\n",
      "Epoch 65 Batch 300 Loss 0.7448 Accuracy 0.7473\n",
      "Epoch 65 Batch 350 Loss 0.7445 Accuracy 0.7474\n",
      "Epoch 65 Batch 400 Loss 0.7456 Accuracy 0.7472\n",
      "Epoch 65 Batch 450 Loss 0.7467 Accuracy 0.7469\n",
      "Epoch 65 Batch 500 Loss 0.7476 Accuracy 0.7468\n",
      "Epoch 65 Batch 550 Loss 0.7481 Accuracy 0.7466\n",
      "Epoch 65 Batch 600 Loss 0.7478 Accuracy 0.7469\n",
      "Epoch 65 Batch 650 Loss 0.7477 Accuracy 0.7468\n",
      "Epoch 65 Batch 700 Loss 0.7482 Accuracy 0.7465\n",
      "Epoch 65 Batch 750 Loss 0.7479 Accuracy 0.7467\n",
      "Epoch 65 Batch 800 Loss 0.7482 Accuracy 0.7466\n",
      "Epoch 65 Batch 850 Loss 0.7486 Accuracy 0.7465\n",
      "Epoch 65 Batch 900 Loss 0.7489 Accuracy 0.7464\n",
      "Epoch 65 Batch 950 Loss 0.7489 Accuracy 0.7464\n",
      "Epoch 65 Batch 1000 Loss 0.7485 Accuracy 0.7464\n",
      "discarded batch 1022\n",
      "Epoch 65 Batch 1050 Loss 0.7489 Accuracy 0.7462\n",
      "Epoch 65 Batch 1100 Loss 0.7491 Accuracy 0.7461\n",
      "Epoch 65 Batch 1150 Loss 0.7495 Accuracy 0.7459\n",
      "Epoch 65 Batch 1200 Loss 0.7500 Accuracy 0.7458\n",
      "Epoch 65 Batch 1250 Loss 0.7502 Accuracy 0.7457\n",
      "Epoch 65 Batch 1300 Loss 0.7500 Accuracy 0.7457\n",
      "Epoch 65 Batch 1350 Loss 0.7506 Accuracy 0.7455\n",
      "Epoch 65 Batch 1400 Loss 0.7507 Accuracy 0.7455\n",
      "Epoch 65 Batch 1450 Loss 0.7507 Accuracy 0.7455\n",
      "Epoch 65 Batch 1500 Loss 0.7505 Accuracy 0.7455\n",
      "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-13\n",
      "Epoch 65 Loss 0.7502 Accuracy 0.7456\n",
      "Time taken for 1 epoch: 36.81781077384949 secs\n",
      "\n",
      "epoch lasted: 36.82194519042969\n",
      "Epoch 66 Batch 0 Loss 0.8019 Accuracy 0.7159\n",
      "Epoch 66 Batch 50 Loss 0.7443 Accuracy 0.7479\n",
      "Epoch 66 Batch 100 Loss 0.7432 Accuracy 0.7479\n",
      "Epoch 66 Batch 150 Loss 0.7455 Accuracy 0.7466\n",
      "Epoch 66 Batch 200 Loss 0.7461 Accuracy 0.7468\n",
      "Epoch 66 Batch 250 Loss 0.7436 Accuracy 0.7476\n",
      "Epoch 66 Batch 300 Loss 0.7418 Accuracy 0.7483\n",
      "Epoch 66 Batch 350 Loss 0.7407 Accuracy 0.7487\n",
      "Epoch 66 Batch 400 Loss 0.7416 Accuracy 0.7484\n",
      "Epoch 66 Batch 450 Loss 0.7411 Accuracy 0.7484\n",
      "Epoch 66 Batch 500 Loss 0.7419 Accuracy 0.7481\n",
      "Epoch 66 Batch 550 Loss 0.7423 Accuracy 0.7480\n",
      "Epoch 66 Batch 600 Loss 0.7438 Accuracy 0.7475\n",
      "Epoch 66 Batch 650 Loss 0.7445 Accuracy 0.7472\n",
      "Epoch 66 Batch 700 Loss 0.7447 Accuracy 0.7473\n",
      "Epoch 66 Batch 750 Loss 0.7452 Accuracy 0.7471\n",
      "Epoch 66 Batch 800 Loss 0.7445 Accuracy 0.7473\n",
      "Epoch 66 Batch 850 Loss 0.7448 Accuracy 0.7472\n",
      "Epoch 66 Batch 900 Loss 0.7448 Accuracy 0.7471\n",
      "Epoch 66 Batch 950 Loss 0.7449 Accuracy 0.7471\n",
      "Epoch 66 Batch 1000 Loss 0.7452 Accuracy 0.7471\n",
      "Epoch 66 Batch 1050 Loss 0.7459 Accuracy 0.7469\n",
      "Epoch 66 Batch 1100 Loss 0.7466 Accuracy 0.7465\n",
      "Epoch 66 Batch 1150 Loss 0.7467 Accuracy 0.7465\n",
      "Epoch 66 Batch 1200 Loss 0.7467 Accuracy 0.7465\n",
      "Epoch 66 Batch 1250 Loss 0.7465 Accuracy 0.7466\n",
      "discarded batch 1292\n",
      "Epoch 66 Batch 1300 Loss 0.7473 Accuracy 0.7464\n",
      "Epoch 66 Batch 1350 Loss 0.7475 Accuracy 0.7463\n",
      "Epoch 66 Batch 1400 Loss 0.7482 Accuracy 0.7461\n",
      "Epoch 66 Batch 1450 Loss 0.7482 Accuracy 0.7462\n",
      "Epoch 66 Batch 1500 Loss 0.7482 Accuracy 0.7462\n",
      "Epoch 66 Loss 0.7482 Accuracy 0.7461\n",
      "Time taken for 1 epoch: 36.25963068008423 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 66 VALIDATION: Loss 0.7424 Accuracy 0.7513\n",
      "\n",
      "epoch lasted: 36.42322063446045\n",
      "Epoch 67 Batch 0 Loss 0.6798 Accuracy 0.7641\n",
      "Epoch 67 Batch 50 Loss 0.7423 Accuracy 0.7493\n",
      "Epoch 67 Batch 100 Loss 0.7395 Accuracy 0.7504\n",
      "Epoch 67 Batch 150 Loss 0.7413 Accuracy 0.7494\n",
      "Epoch 67 Batch 200 Loss 0.7405 Accuracy 0.7502\n",
      "Epoch 67 Batch 250 Loss 0.7414 Accuracy 0.7494\n",
      "Epoch 67 Batch 300 Loss 0.7420 Accuracy 0.7492\n",
      "Epoch 67 Batch 350 Loss 0.7431 Accuracy 0.7487\n",
      "Epoch 67 Batch 400 Loss 0.7431 Accuracy 0.7484\n",
      "Epoch 67 Batch 450 Loss 0.7434 Accuracy 0.7482\n",
      "Epoch 67 Batch 500 Loss 0.7430 Accuracy 0.7481\n",
      "Epoch 67 Batch 550 Loss 0.7428 Accuracy 0.7482\n",
      "Epoch 67 Batch 600 Loss 0.7431 Accuracy 0.7482\n",
      "Epoch 67 Batch 650 Loss 0.7434 Accuracy 0.7482\n",
      "Epoch 67 Batch 700 Loss 0.7436 Accuracy 0.7480\n",
      "Epoch 67 Batch 750 Loss 0.7448 Accuracy 0.7478\n",
      "Epoch 67 Batch 800 Loss 0.7450 Accuracy 0.7475\n",
      "discarded batch 844\n",
      "Epoch 67 Batch 850 Loss 0.7455 Accuracy 0.7473\n",
      "Epoch 67 Batch 900 Loss 0.7454 Accuracy 0.7473\n",
      "Epoch 67 Batch 950 Loss 0.7457 Accuracy 0.7471\n",
      "Epoch 67 Batch 1000 Loss 0.7452 Accuracy 0.7473\n",
      "Epoch 67 Batch 1050 Loss 0.7453 Accuracy 0.7472\n",
      "Epoch 67 Batch 1100 Loss 0.7451 Accuracy 0.7474\n",
      "Epoch 67 Batch 1150 Loss 0.7453 Accuracy 0.7473\n",
      "Epoch 67 Batch 1200 Loss 0.7457 Accuracy 0.7472\n",
      "Epoch 67 Batch 1250 Loss 0.7457 Accuracy 0.7472\n",
      "Epoch 67 Batch 1300 Loss 0.7459 Accuracy 0.7471\n",
      "Epoch 67 Batch 1350 Loss 0.7458 Accuracy 0.7472\n",
      "Epoch 67 Batch 1400 Loss 0.7460 Accuracy 0.7472\n",
      "Epoch 67 Batch 1450 Loss 0.7459 Accuracy 0.7472\n",
      "Epoch 67 Batch 1500 Loss 0.7463 Accuracy 0.7471\n",
      "Epoch 67 Loss 0.7464 Accuracy 0.7471\n",
      "Time taken for 1 epoch: 36.52380323410034 secs\n",
      "\n",
      "epoch lasted: 36.527512550354004\n",
      "Epoch 68 Batch 0 Loss 0.7013 Accuracy 0.7641\n",
      "Epoch 68 Batch 50 Loss 0.7316 Accuracy 0.7513\n",
      "Epoch 68 Batch 100 Loss 0.7399 Accuracy 0.7494\n",
      "Epoch 68 Batch 150 Loss 0.7382 Accuracy 0.7491\n",
      "Epoch 68 Batch 200 Loss 0.7398 Accuracy 0.7485\n",
      "Epoch 68 Batch 250 Loss 0.7399 Accuracy 0.7481\n",
      "Epoch 68 Batch 300 Loss 0.7395 Accuracy 0.7481\n",
      "Epoch 68 Batch 350 Loss 0.7389 Accuracy 0.7482\n",
      "Epoch 68 Batch 400 Loss 0.7390 Accuracy 0.7482\n",
      "Epoch 68 Batch 450 Loss 0.7393 Accuracy 0.7483\n",
      "Epoch 68 Batch 500 Loss 0.7403 Accuracy 0.7483\n",
      "Epoch 68 Batch 550 Loss 0.7404 Accuracy 0.7485\n",
      "Epoch 68 Batch 600 Loss 0.7404 Accuracy 0.7484\n",
      "Epoch 68 Batch 650 Loss 0.7400 Accuracy 0.7486\n",
      "Epoch 68 Batch 700 Loss 0.7413 Accuracy 0.7483\n",
      "Epoch 68 Batch 750 Loss 0.7413 Accuracy 0.7482\n",
      "Epoch 68 Batch 800 Loss 0.7413 Accuracy 0.7482\n",
      "Epoch 68 Batch 850 Loss 0.7418 Accuracy 0.7480\n",
      "Epoch 68 Batch 900 Loss 0.7417 Accuracy 0.7482\n",
      "Epoch 68 Batch 950 Loss 0.7414 Accuracy 0.7480\n",
      "Epoch 68 Batch 1000 Loss 0.7418 Accuracy 0.7481\n",
      "Epoch 68 Batch 1050 Loss 0.7425 Accuracy 0.7478\n",
      "discarded batch 1064\n",
      "Epoch 68 Batch 1100 Loss 0.7427 Accuracy 0.7479\n",
      "Epoch 68 Batch 1150 Loss 0.7426 Accuracy 0.7479\n",
      "Epoch 68 Batch 1200 Loss 0.7426 Accuracy 0.7479\n",
      "Epoch 68 Batch 1250 Loss 0.7435 Accuracy 0.7477\n",
      "Epoch 68 Batch 1300 Loss 0.7436 Accuracy 0.7476\n",
      "Epoch 68 Batch 1350 Loss 0.7439 Accuracy 0.7475\n",
      "Epoch 68 Batch 1400 Loss 0.7440 Accuracy 0.7474\n",
      "Epoch 68 Batch 1450 Loss 0.7443 Accuracy 0.7473\n",
      "Epoch 68 Batch 1500 Loss 0.7443 Accuracy 0.7474\n",
      "Epoch 68 Loss 0.7439 Accuracy 0.7476\n",
      "Time taken for 1 epoch: 36.339858293533325 secs\n",
      "\n",
      "epoch lasted: 36.34405493736267\n",
      "Epoch 69 Batch 0 Loss 0.7454 Accuracy 0.7276\n",
      "Epoch 69 Batch 50 Loss 0.7267 Accuracy 0.7519\n",
      "Epoch 69 Batch 100 Loss 0.7326 Accuracy 0.7510\n",
      "Epoch 69 Batch 150 Loss 0.7400 Accuracy 0.7486\n",
      "Epoch 69 Batch 200 Loss 0.7371 Accuracy 0.7491\n",
      "Epoch 69 Batch 250 Loss 0.7376 Accuracy 0.7489\n",
      "Epoch 69 Batch 300 Loss 0.7371 Accuracy 0.7492\n",
      "Epoch 69 Batch 350 Loss 0.7368 Accuracy 0.7493\n",
      "Epoch 69 Batch 400 Loss 0.7371 Accuracy 0.7494\n",
      "Epoch 69 Batch 450 Loss 0.7381 Accuracy 0.7491\n",
      "Epoch 69 Batch 500 Loss 0.7381 Accuracy 0.7493\n",
      "Epoch 69 Batch 550 Loss 0.7385 Accuracy 0.7492\n",
      "Epoch 69 Batch 600 Loss 0.7389 Accuracy 0.7490\n",
      "Epoch 69 Batch 650 Loss 0.7393 Accuracy 0.7489\n",
      "Epoch 69 Batch 700 Loss 0.7393 Accuracy 0.7488\n",
      "Epoch 69 Batch 750 Loss 0.7394 Accuracy 0.7487\n",
      "Epoch 69 Batch 800 Loss 0.7399 Accuracy 0.7485\n",
      "Epoch 69 Batch 850 Loss 0.7402 Accuracy 0.7484\n",
      "Epoch 69 Batch 900 Loss 0.7406 Accuracy 0.7484\n",
      "Epoch 69 Batch 950 Loss 0.7408 Accuracy 0.7486\n",
      "Epoch 69 Batch 1000 Loss 0.7415 Accuracy 0.7483\n",
      "Epoch 69 Batch 1050 Loss 0.7414 Accuracy 0.7483\n",
      "discarded batch 1058\n",
      "Epoch 69 Batch 1100 Loss 0.7417 Accuracy 0.7481\n",
      "Epoch 69 Batch 1150 Loss 0.7419 Accuracy 0.7479\n",
      "Epoch 69 Batch 1200 Loss 0.7421 Accuracy 0.7478\n",
      "Epoch 69 Batch 1250 Loss 0.7425 Accuracy 0.7477\n",
      "Epoch 69 Batch 1300 Loss 0.7423 Accuracy 0.7478\n",
      "Epoch 69 Batch 1350 Loss 0.7424 Accuracy 0.7476\n",
      "Epoch 69 Batch 1400 Loss 0.7425 Accuracy 0.7476\n",
      "Epoch 69 Batch 1450 Loss 0.7426 Accuracy 0.7476\n",
      "Epoch 69 Batch 1500 Loss 0.7424 Accuracy 0.7477\n",
      "Epoch 69 Loss 0.7423 Accuracy 0.7477\n",
      "Time taken for 1 epoch: 36.50905394554138 secs\n",
      "\n",
      "epoch lasted: 36.51335334777832\n",
      "Epoch 70 Batch 0 Loss 0.7240 Accuracy 0.7492\n",
      "Epoch 70 Batch 50 Loss 0.7381 Accuracy 0.7512\n",
      "Epoch 70 Batch 100 Loss 0.7365 Accuracy 0.7506\n",
      "Epoch 70 Batch 150 Loss 0.7392 Accuracy 0.7500\n",
      "Epoch 70 Batch 200 Loss 0.7381 Accuracy 0.7495\n",
      "Epoch 70 Batch 250 Loss 0.7393 Accuracy 0.7491\n",
      "Epoch 70 Batch 300 Loss 0.7393 Accuracy 0.7490\n",
      "Epoch 70 Batch 350 Loss 0.7390 Accuracy 0.7492\n",
      "Epoch 70 Batch 400 Loss 0.7404 Accuracy 0.7486\n",
      "Epoch 70 Batch 450 Loss 0.7403 Accuracy 0.7486\n",
      "Epoch 70 Batch 500 Loss 0.7403 Accuracy 0.7486\n",
      "Epoch 70 Batch 550 Loss 0.7401 Accuracy 0.7487\n",
      "Epoch 70 Batch 600 Loss 0.7396 Accuracy 0.7489\n",
      "Epoch 70 Batch 650 Loss 0.7397 Accuracy 0.7490\n",
      "Epoch 70 Batch 700 Loss 0.7407 Accuracy 0.7488\n",
      "Epoch 70 Batch 750 Loss 0.7409 Accuracy 0.7486\n",
      "Epoch 70 Batch 800 Loss 0.7411 Accuracy 0.7485\n",
      "Epoch 70 Batch 850 Loss 0.7405 Accuracy 0.7485\n",
      "Epoch 70 Batch 900 Loss 0.7404 Accuracy 0.7486\n",
      "Epoch 70 Batch 950 Loss 0.7397 Accuracy 0.7487\n",
      "Epoch 70 Batch 1000 Loss 0.7396 Accuracy 0.7487\n",
      "Epoch 70 Batch 1050 Loss 0.7393 Accuracy 0.7489\n",
      "Epoch 70 Batch 1100 Loss 0.7391 Accuracy 0.7489\n",
      "Epoch 70 Batch 1150 Loss 0.7391 Accuracy 0.7489\n",
      "Epoch 70 Batch 1200 Loss 0.7395 Accuracy 0.7488\n",
      "Epoch 70 Batch 1250 Loss 0.7398 Accuracy 0.7487\n",
      "discarded batch 1258\n",
      "Epoch 70 Batch 1300 Loss 0.7400 Accuracy 0.7487\n",
      "Epoch 70 Batch 1350 Loss 0.7402 Accuracy 0.7487\n",
      "Epoch 70 Batch 1400 Loss 0.7406 Accuracy 0.7485\n",
      "Epoch 70 Batch 1450 Loss 0.7410 Accuracy 0.7484\n",
      "Epoch 70 Batch 1500 Loss 0.7413 Accuracy 0.7483\n",
      "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-14\n",
      "Epoch 70 Loss 0.7412 Accuracy 0.7483\n",
      "Time taken for 1 epoch: 36.485384941101074 secs\n",
      "\n",
      "epoch lasted: 36.48950147628784\n",
      "Epoch 71 Batch 0 Loss 0.6955 Accuracy 0.7674\n",
      "Epoch 71 Batch 50 Loss 0.7269 Accuracy 0.7519\n",
      "Epoch 71 Batch 100 Loss 0.7315 Accuracy 0.7514\n",
      "Epoch 71 Batch 150 Loss 0.7357 Accuracy 0.7509\n",
      "Epoch 71 Batch 200 Loss 0.7398 Accuracy 0.7496\n",
      "Epoch 71 Batch 250 Loss 0.7405 Accuracy 0.7492\n",
      "Epoch 71 Batch 300 Loss 0.7394 Accuracy 0.7495\n",
      "Epoch 71 Batch 350 Loss 0.7379 Accuracy 0.7500\n",
      "Epoch 71 Batch 400 Loss 0.7374 Accuracy 0.7500\n",
      "Epoch 71 Batch 450 Loss 0.7373 Accuracy 0.7499\n",
      "Epoch 71 Batch 500 Loss 0.7374 Accuracy 0.7498\n",
      "Epoch 71 Batch 550 Loss 0.7375 Accuracy 0.7498\n",
      "Epoch 71 Batch 600 Loss 0.7375 Accuracy 0.7497\n",
      "Epoch 71 Batch 650 Loss 0.7370 Accuracy 0.7499\n",
      "Epoch 71 Batch 700 Loss 0.7377 Accuracy 0.7497\n",
      "Epoch 71 Batch 750 Loss 0.7376 Accuracy 0.7497\n",
      "Epoch 71 Batch 800 Loss 0.7379 Accuracy 0.7499\n",
      "Epoch 71 Batch 850 Loss 0.7382 Accuracy 0.7497\n",
      "Epoch 71 Batch 900 Loss 0.7387 Accuracy 0.7495\n",
      "Epoch 71 Batch 950 Loss 0.7388 Accuracy 0.7495\n",
      "Epoch 71 Batch 1000 Loss 0.7390 Accuracy 0.7493\n",
      "Epoch 71 Batch 1050 Loss 0.7381 Accuracy 0.7496\n",
      "Epoch 71 Batch 1100 Loss 0.7386 Accuracy 0.7494\n",
      "Epoch 71 Batch 1150 Loss 0.7386 Accuracy 0.7494\n",
      "Epoch 71 Batch 1200 Loss 0.7384 Accuracy 0.7495\n",
      "Epoch 71 Batch 1250 Loss 0.7382 Accuracy 0.7496\n",
      "Epoch 71 Batch 1300 Loss 0.7385 Accuracy 0.7495\n",
      "Epoch 71 Batch 1350 Loss 0.7384 Accuracy 0.7495\n",
      "Epoch 71 Batch 1400 Loss 0.7384 Accuracy 0.7495\n",
      "discarded batch 1408\n",
      "Epoch 71 Batch 1450 Loss 0.7388 Accuracy 0.7493\n",
      "Epoch 71 Batch 1500 Loss 0.7385 Accuracy 0.7494\n",
      "Epoch 71 Loss 0.7388 Accuracy 0.7493\n",
      "Time taken for 1 epoch: 36.29007053375244 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 71 VALIDATION: Loss 0.7349 Accuracy 0.7543\n",
      "\n",
      "la | tua | be| ni| gni| tà | non | pur | soc| cor| re|\n",
      "a | chi | do| man| da | ma | mol| te | fï| a| te|\n",
      "li| be| ra| men| te al | di| man| dar | pre| cor| re|\n",
      "                                        \n",
      " che | si | fa | con| tra| di| ni e | per | la | sce| sta|\n",
      "co| sì | di | sé | la | spi| ri| tà | di | so| le|\n",
      "co| sì | di | sé | co| sì | di | quel | che | se| sta|\n",
      "                                    \n",
      " che | si | fa | la | sua | vi| sta | si | si | spo| le|\n",
      "che | si | fa| vil| la | si | fa | si | ri| spo| sta|\n",
      "sì | che | si | fa | se| gui| ta | vi| sta | sco| le|\n",
      "  \n",
      " co| sì | di | sé | non | si | vol| ge | si | ste| me|\n",
      "che | se | non | si | fa | la | vi| sta | si | schi| sta|\n",
      "co| sì | di | lei | per | la | vi| sta | si | sce| me|\n",
      " che | se | non | so| no in | sé | non | si | ri| spon| de|\n",
      "co| sì | la | vi| sta | se| gui| ta | si | spi| sta|\n",
      "che | se | non | si | fa | la | vi| sta | si | scon| de|\n",
      "                               \n",
      " co| sì | si | fa | che | si | fa | sì | co| sa| sta|\n",
      "la | vi| sta | si | fa | che | si | fa | se| gue| ra|\n",
      "co| sì | si | fa | se| gui| ta | di | quel| la | stra| sta|\n",
      "  \n",
      " che | si | vol| se | se | tu | ve| di | si | scor| te|\n",
      "co| sì | di | lei | se | tu | ve| di | si | schie| ti|\n",
      "che | se | tu | ve| di | se | non | si | ri| spor| te|\n",
      "  \n",
      " che | se | non | si | voi | sie| te | si | ri| spo| sta|\n",
      "co| me | se | tu | ve| di | la | vi| sta | si | spi| sto|\n",
      "che | se | non | sie| te | si | vol| se | si | sco| sta|\n",
      " che | se | non | so| li | se | non | si | vol| se | ste| sta|\n",
      "co| sì | di | là | do| ve | si | fa | la | spo| sta|\n",
      "co| sì | la | vi| sta | se| gui| ta | si | fa| va|\n",
      "                                  \n",
      " che | si | fa | co| sì | di | sé | la | sua | vi| sta|\n",
      "che | si | fa| vil| la | sua | pa| rea | di| sti| sta|\n",
      "co| sì | si | fa | che | si | fa | sì | co| me | sti| ra|\n",
      " che | se | non | so| le in | sé | non | si | ri| spo| se|\n",
      "co| sì | la | vi| sta | se| gui| ta | si | sco| se|\n",
      "co| sì | di | là | do| ve | si | fa | la | sco| se|\n",
      "                                      \n",
      " co| sì | si | fa| cea | di | sé | la | sua | vi| va| ce|\n",
      "che | si | fa | con | la | sua | vi| ve | si | schie| ta|\n",
      "co| sì | di | sé | non | si | fa| cea | la | spi| ra|\n",
      "  \n",
      " che | si | fa| cea | la | sel| va | si | ri| spo| sta|\n",
      "la | vi| sta | si | fa | che | si | fa | si | spa| de|\n",
      "co| sì | di | sé | co| sì | di | quel| la | sco| sta|\n",
      "                                    \n",
      " che | si | fa | co| sì | di | sé | la | sua | vi| ve|\n",
      "che | si | fa| vel| la | sua | pa| rea | di | so| sta|\n",
      "che | si | fa | se| gui| ta | si | fa | sì | fa| vel| la|\n",
      "  \n",
      " che | se | non | so| le al| tra | vi| sta | si | fa| vel| la|\n",
      "co| sì | di | sé | tut| ta | se| gui| ta | si| gi|\n",
      "che | si | fa | la | vi| sta | di | quel| la | scri| ve|\n",
      "                              \n",
      " che | si | fa | che | si | fa | se| gui| ta | si| gi|\n",
      "sì | che | si | fa | se| gui| ta | si | ri| spo| se|\n",
      "sì | che | si | fa | se| gui| ta | la | sua | vi| ve|\n",
      " che | se | non | so| le in | sé | non | si | ri| spo| sta|\n",
      "co| sì | la | vi| sta | se| gui| ta | si | schi| ri|\n",
      "co| sì | di | là | do| ve | si | fa | la | sco| sta|\n",
      "                                   \n",
      " co| sì | si | vol| ge | si | fa | la | sua | vi| vi| ve|\n",
      "che | si | fa | che | si | fa | se| gui| ta | si | ste| ga|\n",
      "co| sì | di | sé | ta| glia | di | sé | si | fo| sta|\n",
      "  \n",
      " che | se | non | si | ve| de| sti | si | ri| spo| sta|\n",
      "la | vi| sta | vi| sta | se| gui| ta | si | sco| sta|\n",
      "co| sì | di | là | do| ve | si | fa | la | sco| sta|\n",
      " che | se | non | so| le in | sé | non | si | ri| spo| sta|\n",
      "co| sì | la | vi| sta | se| gui| ta | si | sco| sta|\n",
      "co| sì | di | là | do| ve | si | fa | la | sco| sta|\n",
      "                                   \n",
      " che | si | fa | la | sua | vi| sta | si | si | schie| sta|\n",
      "co| sì | di | là | do| ve | si | fa | se| gue | sta| ga|\n",
      "co| sì | di | là | do| ve | si | ri| spo| sta | fo| ra|\n",
      "  \n",
      " che | si | ve| de| stra | vi| ve | si | ri| spo| sta|\n",
      "la | vi| sta | vi| sta | se| gui| ta | si | ri| sto|\n",
      "co| sì | di | là | do| ve | si | fa | la | sco| sta|\n",
      " che | se | tu | ve| di | sé | non | si | vol| ge | ste| gna|\n",
      "la | vi| sta | se| gui| ta | si | ri| spuo| se | sta|\n",
      "che | se | non | si | fa | di | quel| la | si | sce| gna|\n",
      "                           \n",
      " la | vi| sta | vi| sta | si | fa | di | sé | sta| va|\n",
      "che | si | fa | con | la | sua | pa| re| va | sa| li|\n",
      "che | si | fa | se| gui| ta | vi| sta | vi| sta | fa| va|\n",
      "\n",
      "epoch lasted: 577.3986339569092\n",
      "Epoch 72 Batch 0 Loss 0.7243 Accuracy 0.7375\n",
      "Epoch 72 Batch 50 Loss 0.7390 Accuracy 0.7488\n",
      "Epoch 72 Batch 100 Loss 0.7312 Accuracy 0.7527\n",
      "Epoch 72 Batch 150 Loss 0.7297 Accuracy 0.7527\n",
      "Epoch 72 Batch 200 Loss 0.7314 Accuracy 0.7528\n",
      "Epoch 72 Batch 250 Loss 0.7339 Accuracy 0.7518\n",
      "Epoch 72 Batch 300 Loss 0.7349 Accuracy 0.7514\n",
      "Epoch 72 Batch 350 Loss 0.7342 Accuracy 0.7516\n",
      "Epoch 72 Batch 400 Loss 0.7338 Accuracy 0.7518\n",
      "Epoch 72 Batch 450 Loss 0.7342 Accuracy 0.7516\n",
      "Epoch 72 Batch 500 Loss 0.7351 Accuracy 0.7512\n",
      "Epoch 72 Batch 550 Loss 0.7348 Accuracy 0.7513\n",
      "Epoch 72 Batch 600 Loss 0.7353 Accuracy 0.7510\n",
      "discarded batch 640\n",
      "Epoch 72 Batch 650 Loss 0.7350 Accuracy 0.7511\n",
      "Epoch 72 Batch 700 Loss 0.7354 Accuracy 0.7509\n",
      "Epoch 72 Batch 750 Loss 0.7354 Accuracy 0.7508\n",
      "Epoch 72 Batch 800 Loss 0.7349 Accuracy 0.7510\n",
      "Epoch 72 Batch 850 Loss 0.7347 Accuracy 0.7511\n",
      "Epoch 72 Batch 900 Loss 0.7346 Accuracy 0.7511\n",
      "Epoch 72 Batch 950 Loss 0.7351 Accuracy 0.7510\n",
      "Epoch 72 Batch 1000 Loss 0.7352 Accuracy 0.7510\n",
      "Epoch 72 Batch 1050 Loss 0.7358 Accuracy 0.7508\n",
      "Epoch 72 Batch 1100 Loss 0.7357 Accuracy 0.7508\n",
      "Epoch 72 Batch 1150 Loss 0.7357 Accuracy 0.7507\n",
      "Epoch 72 Batch 1200 Loss 0.7359 Accuracy 0.7507\n",
      "Epoch 72 Batch 1250 Loss 0.7360 Accuracy 0.7506\n",
      "Epoch 72 Batch 1300 Loss 0.7365 Accuracy 0.7503\n",
      "Epoch 72 Batch 1350 Loss 0.7369 Accuracy 0.7502\n",
      "Epoch 72 Batch 1400 Loss 0.7369 Accuracy 0.7502\n",
      "Epoch 72 Batch 1450 Loss 0.7372 Accuracy 0.7501\n",
      "Epoch 72 Batch 1500 Loss 0.7372 Accuracy 0.7501\n",
      "Epoch 72 Loss 0.7371 Accuracy 0.7501\n",
      "Time taken for 1 epoch: 37.19205117225647 secs\n",
      "\n",
      "epoch lasted: 37.195868730545044\n",
      "Epoch 73 Batch 0 Loss 0.8985 Accuracy 0.6844\n",
      "Epoch 73 Batch 50 Loss 0.7428 Accuracy 0.7489\n",
      "Epoch 73 Batch 100 Loss 0.7403 Accuracy 0.7487\n",
      "Epoch 73 Batch 150 Loss 0.7399 Accuracy 0.7491\n",
      "Epoch 73 Batch 200 Loss 0.7342 Accuracy 0.7509\n",
      "Epoch 73 Batch 250 Loss 0.7341 Accuracy 0.7510\n",
      "Epoch 73 Batch 300 Loss 0.7349 Accuracy 0.7506\n",
      "Epoch 73 Batch 350 Loss 0.7356 Accuracy 0.7506\n",
      "Epoch 73 Batch 400 Loss 0.7357 Accuracy 0.7505\n",
      "Epoch 73 Batch 450 Loss 0.7355 Accuracy 0.7506\n",
      "Epoch 73 Batch 500 Loss 0.7349 Accuracy 0.7507\n",
      "Epoch 73 Batch 550 Loss 0.7358 Accuracy 0.7505\n",
      "Epoch 73 Batch 600 Loss 0.7348 Accuracy 0.7506\n",
      "Epoch 73 Batch 650 Loss 0.7346 Accuracy 0.7506\n",
      "Epoch 73 Batch 700 Loss 0.7337 Accuracy 0.7509\n",
      "Epoch 73 Batch 750 Loss 0.7343 Accuracy 0.7506\n",
      "Epoch 73 Batch 800 Loss 0.7337 Accuracy 0.7507\n",
      "Epoch 73 Batch 850 Loss 0.7338 Accuracy 0.7507\n",
      "Epoch 73 Batch 900 Loss 0.7340 Accuracy 0.7505\n",
      "Epoch 73 Batch 950 Loss 0.7342 Accuracy 0.7505\n",
      "Epoch 73 Batch 1000 Loss 0.7342 Accuracy 0.7505\n",
      "Epoch 73 Batch 1050 Loss 0.7345 Accuracy 0.7505\n",
      "Epoch 73 Batch 1100 Loss 0.7350 Accuracy 0.7504\n",
      "discarded batch 1131\n",
      "Epoch 73 Batch 1150 Loss 0.7347 Accuracy 0.7505\n",
      "Epoch 73 Batch 1200 Loss 0.7350 Accuracy 0.7503\n",
      "Epoch 73 Batch 1250 Loss 0.7357 Accuracy 0.7502\n",
      "Epoch 73 Batch 1300 Loss 0.7359 Accuracy 0.7501\n",
      "Epoch 73 Batch 1350 Loss 0.7359 Accuracy 0.7501\n",
      "Epoch 73 Batch 1400 Loss 0.7362 Accuracy 0.7500\n",
      "Epoch 73 Batch 1450 Loss 0.7359 Accuracy 0.7499\n",
      "Epoch 73 Batch 1500 Loss 0.7360 Accuracy 0.7499\n",
      "Epoch 73 Loss 0.7358 Accuracy 0.7499\n",
      "Time taken for 1 epoch: 36.63422966003418 secs\n",
      "\n",
      "epoch lasted: 36.63798117637634\n",
      "Epoch 74 Batch 0 Loss 0.6958 Accuracy 0.7774\n",
      "Epoch 74 Batch 50 Loss 0.7247 Accuracy 0.7550\n",
      "Epoch 74 Batch 100 Loss 0.7305 Accuracy 0.7516\n",
      "Epoch 74 Batch 150 Loss 0.7342 Accuracy 0.7507\n",
      "Epoch 74 Batch 200 Loss 0.7323 Accuracy 0.7514\n",
      "Epoch 74 Batch 250 Loss 0.7323 Accuracy 0.7516\n",
      "Epoch 74 Batch 300 Loss 0.7351 Accuracy 0.7510\n",
      "Epoch 74 Batch 350 Loss 0.7332 Accuracy 0.7516\n",
      "Epoch 74 Batch 400 Loss 0.7312 Accuracy 0.7519\n",
      "Epoch 74 Batch 450 Loss 0.7314 Accuracy 0.7520\n",
      "Epoch 74 Batch 500 Loss 0.7314 Accuracy 0.7517\n",
      "Epoch 74 Batch 550 Loss 0.7307 Accuracy 0.7519\n",
      "Epoch 74 Batch 600 Loss 0.7307 Accuracy 0.7520\n",
      "Epoch 74 Batch 650 Loss 0.7304 Accuracy 0.7520\n",
      "Epoch 74 Batch 700 Loss 0.7313 Accuracy 0.7518\n",
      "Epoch 74 Batch 750 Loss 0.7320 Accuracy 0.7516\n",
      "Epoch 74 Batch 800 Loss 0.7328 Accuracy 0.7512\n",
      "Epoch 74 Batch 850 Loss 0.7330 Accuracy 0.7512\n",
      "Epoch 74 Batch 900 Loss 0.7328 Accuracy 0.7512\n",
      "Epoch 74 Batch 950 Loss 0.7332 Accuracy 0.7512\n",
      "Epoch 74 Batch 1000 Loss 0.7332 Accuracy 0.7512\n",
      "Epoch 74 Batch 1050 Loss 0.7327 Accuracy 0.7513\n",
      "Epoch 74 Batch 1100 Loss 0.7335 Accuracy 0.7510\n",
      "Epoch 74 Batch 1150 Loss 0.7334 Accuracy 0.7511\n",
      "Epoch 74 Batch 1200 Loss 0.7334 Accuracy 0.7510\n",
      "Epoch 74 Batch 1250 Loss 0.7334 Accuracy 0.7509\n",
      "Epoch 74 Batch 1300 Loss 0.7335 Accuracy 0.7509\n",
      "Epoch 74 Batch 1350 Loss 0.7335 Accuracy 0.7509\n",
      "Epoch 74 Batch 1400 Loss 0.7333 Accuracy 0.7510\n",
      "Epoch 74 Batch 1450 Loss 0.7339 Accuracy 0.7509\n",
      "Epoch 74 Batch 1500 Loss 0.7339 Accuracy 0.7509\n",
      "discarded batch 1541\n",
      "Epoch 74 Loss 0.7339 Accuracy 0.7509\n",
      "Time taken for 1 epoch: 36.857916593551636 secs\n",
      "\n",
      "epoch lasted: 36.86207056045532\n",
      "Epoch 75 Batch 0 Loss 0.7083 Accuracy 0.7542\n",
      "Epoch 75 Batch 50 Loss 0.7119 Accuracy 0.7579\n",
      "Epoch 75 Batch 100 Loss 0.7202 Accuracy 0.7550\n",
      "Epoch 75 Batch 150 Loss 0.7210 Accuracy 0.7549\n",
      "Epoch 75 Batch 200 Loss 0.7235 Accuracy 0.7539\n",
      "Epoch 75 Batch 250 Loss 0.7232 Accuracy 0.7538\n",
      "Epoch 75 Batch 300 Loss 0.7234 Accuracy 0.7536\n",
      "Epoch 75 Batch 350 Loss 0.7255 Accuracy 0.7529\n",
      "Epoch 75 Batch 400 Loss 0.7257 Accuracy 0.7531\n",
      "Epoch 75 Batch 450 Loss 0.7261 Accuracy 0.7529\n",
      "Epoch 75 Batch 500 Loss 0.7269 Accuracy 0.7528\n",
      "Epoch 75 Batch 550 Loss 0.7273 Accuracy 0.7530\n",
      "Epoch 75 Batch 600 Loss 0.7278 Accuracy 0.7530\n",
      "discarded batch 643\n",
      "Epoch 75 Batch 650 Loss 0.7283 Accuracy 0.7528\n",
      "Epoch 75 Batch 700 Loss 0.7283 Accuracy 0.7528\n",
      "Epoch 75 Batch 750 Loss 0.7278 Accuracy 0.7529\n",
      "Epoch 75 Batch 800 Loss 0.7283 Accuracy 0.7527\n",
      "Epoch 75 Batch 850 Loss 0.7289 Accuracy 0.7525\n",
      "Epoch 75 Batch 900 Loss 0.7295 Accuracy 0.7523\n",
      "Epoch 75 Batch 950 Loss 0.7298 Accuracy 0.7523\n",
      "Epoch 75 Batch 1000 Loss 0.7301 Accuracy 0.7522\n",
      "Epoch 75 Batch 1050 Loss 0.7302 Accuracy 0.7522\n",
      "Epoch 75 Batch 1100 Loss 0.7307 Accuracy 0.7520\n",
      "Epoch 75 Batch 1150 Loss 0.7308 Accuracy 0.7519\n",
      "Epoch 75 Batch 1200 Loss 0.7317 Accuracy 0.7516\n",
      "Epoch 75 Batch 1250 Loss 0.7317 Accuracy 0.7516\n",
      "Epoch 75 Batch 1300 Loss 0.7322 Accuracy 0.7514\n",
      "Epoch 75 Batch 1350 Loss 0.7323 Accuracy 0.7514\n",
      "Epoch 75 Batch 1400 Loss 0.7322 Accuracy 0.7513\n",
      "Epoch 75 Batch 1450 Loss 0.7320 Accuracy 0.7514\n",
      "Epoch 75 Batch 1500 Loss 0.7323 Accuracy 0.7513\n",
      "Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-15\n",
      "Epoch 75 Loss 0.7326 Accuracy 0.7512\n",
      "Time taken for 1 epoch: 37.09109091758728 secs\n",
      "\n",
      "epoch lasted: 37.09520196914673\n",
      "Epoch 76 Batch 0 Loss 0.6894 Accuracy 0.7525\n",
      "Epoch 76 Batch 50 Loss 0.7192 Accuracy 0.7550\n",
      "Epoch 76 Batch 100 Loss 0.7217 Accuracy 0.7547\n",
      "Epoch 76 Batch 150 Loss 0.7261 Accuracy 0.7540\n",
      "Epoch 76 Batch 200 Loss 0.7275 Accuracy 0.7541\n",
      "Epoch 76 Batch 250 Loss 0.7262 Accuracy 0.7545\n",
      "Epoch 76 Batch 300 Loss 0.7271 Accuracy 0.7541\n",
      "Epoch 76 Batch 350 Loss 0.7281 Accuracy 0.7535\n",
      "Epoch 76 Batch 400 Loss 0.7275 Accuracy 0.7536\n",
      "Epoch 76 Batch 450 Loss 0.7276 Accuracy 0.7535\n",
      "Epoch 76 Batch 500 Loss 0.7288 Accuracy 0.7532\n",
      "Epoch 76 Batch 550 Loss 0.7287 Accuracy 0.7532\n",
      "Epoch 76 Batch 600 Loss 0.7288 Accuracy 0.7531\n",
      "Epoch 76 Batch 650 Loss 0.7298 Accuracy 0.7526\n",
      "Epoch 76 Batch 700 Loss 0.7306 Accuracy 0.7523\n",
      "Epoch 76 Batch 750 Loss 0.7305 Accuracy 0.7524\n",
      "Epoch 76 Batch 800 Loss 0.7301 Accuracy 0.7525\n",
      "Epoch 76 Batch 850 Loss 0.7294 Accuracy 0.7526\n",
      "Epoch 76 Batch 900 Loss 0.7297 Accuracy 0.7525\n",
      "Epoch 76 Batch 950 Loss 0.7290 Accuracy 0.7528\n",
      "Epoch 76 Batch 1000 Loss 0.7291 Accuracy 0.7527\n",
      "Epoch 76 Batch 1050 Loss 0.7291 Accuracy 0.7527\n",
      "Epoch 76 Batch 1100 Loss 0.7294 Accuracy 0.7526\n",
      "Epoch 76 Batch 1150 Loss 0.7294 Accuracy 0.7526\n",
      "Epoch 76 Batch 1200 Loss 0.7294 Accuracy 0.7527\n",
      "Epoch 76 Batch 1250 Loss 0.7294 Accuracy 0.7526\n",
      "Epoch 76 Batch 1300 Loss 0.7296 Accuracy 0.7526\n",
      "Epoch 76 Batch 1350 Loss 0.7298 Accuracy 0.7525\n",
      "Epoch 76 Batch 1400 Loss 0.7302 Accuracy 0.7523\n",
      "Epoch 76 Batch 1450 Loss 0.7303 Accuracy 0.7522\n",
      "discarded batch 1471\n",
      "Epoch 76 Batch 1500 Loss 0.7307 Accuracy 0.7522\n",
      "Epoch 76 Loss 0.7310 Accuracy 0.7521\n",
      "Time taken for 1 epoch: 36.41842198371887 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 76 VALIDATION: Loss 0.7326 Accuracy 0.7547\n",
      "\n",
      "epoch lasted: 36.566951513290405\n",
      "Epoch 77 Batch 0 Loss 0.6771 Accuracy 0.7641\n",
      "Epoch 77 Batch 50 Loss 0.7256 Accuracy 0.7549\n",
      "Epoch 77 Batch 100 Loss 0.7228 Accuracy 0.7552\n",
      "Epoch 77 Batch 150 Loss 0.7270 Accuracy 0.7530\n",
      "Epoch 77 Batch 200 Loss 0.7292 Accuracy 0.7526\n",
      "Epoch 77 Batch 250 Loss 0.7288 Accuracy 0.7526\n",
      "Epoch 77 Batch 300 Loss 0.7279 Accuracy 0.7530\n",
      "discarded batch 304\n",
      "Epoch 77 Batch 350 Loss 0.7274 Accuracy 0.7530\n",
      "Epoch 77 Batch 400 Loss 0.7268 Accuracy 0.7532\n",
      "Epoch 77 Batch 450 Loss 0.7248 Accuracy 0.7541\n",
      "Epoch 77 Batch 500 Loss 0.7258 Accuracy 0.7538\n",
      "Epoch 77 Batch 550 Loss 0.7262 Accuracy 0.7539\n",
      "Epoch 77 Batch 600 Loss 0.7273 Accuracy 0.7535\n",
      "Epoch 77 Batch 650 Loss 0.7277 Accuracy 0.7534\n",
      "Epoch 77 Batch 700 Loss 0.7276 Accuracy 0.7533\n",
      "Epoch 77 Batch 750 Loss 0.7287 Accuracy 0.7530\n",
      "Epoch 77 Batch 800 Loss 0.7284 Accuracy 0.7531\n",
      "Epoch 77 Batch 850 Loss 0.7281 Accuracy 0.7531\n",
      "Epoch 77 Batch 900 Loss 0.7283 Accuracy 0.7530\n",
      "Epoch 77 Batch 950 Loss 0.7279 Accuracy 0.7532\n",
      "Epoch 77 Batch 1000 Loss 0.7277 Accuracy 0.7533\n",
      "Epoch 77 Batch 1050 Loss 0.7277 Accuracy 0.7534\n",
      "Epoch 77 Batch 1100 Loss 0.7279 Accuracy 0.7532\n",
      "Epoch 77 Batch 1150 Loss 0.7282 Accuracy 0.7531\n",
      "Epoch 77 Batch 1200 Loss 0.7284 Accuracy 0.7531\n",
      "Epoch 77 Batch 1250 Loss 0.7287 Accuracy 0.7530\n",
      "Epoch 77 Batch 1300 Loss 0.7287 Accuracy 0.7530\n",
      "Epoch 77 Batch 1350 Loss 0.7289 Accuracy 0.7529\n",
      "Epoch 77 Batch 1400 Loss 0.7294 Accuracy 0.7526\n",
      "Epoch 77 Batch 1450 Loss 0.7293 Accuracy 0.7526\n",
      "Epoch 77 Batch 1500 Loss 0.7289 Accuracy 0.7527\n",
      "Epoch 77 Loss 0.7292 Accuracy 0.7526\n",
      "Time taken for 1 epoch: 36.46257734298706 secs\n",
      "\n",
      "epoch lasted: 36.46626925468445\n",
      "Epoch 78 Batch 0 Loss 0.7525 Accuracy 0.7176\n",
      "Epoch 78 Batch 50 Loss 0.7148 Accuracy 0.7564\n",
      "Epoch 78 Batch 100 Loss 0.7193 Accuracy 0.7546\n",
      "Epoch 78 Batch 150 Loss 0.7219 Accuracy 0.7541\n",
      "Epoch 78 Batch 200 Loss 0.7219 Accuracy 0.7540\n",
      "Epoch 78 Batch 250 Loss 0.7227 Accuracy 0.7534\n",
      "Epoch 78 Batch 300 Loss 0.7235 Accuracy 0.7533\n",
      "Epoch 78 Batch 350 Loss 0.7241 Accuracy 0.7535\n",
      "Epoch 78 Batch 400 Loss 0.7242 Accuracy 0.7535\n",
      "Epoch 78 Batch 450 Loss 0.7233 Accuracy 0.7540\n",
      "Epoch 78 Batch 500 Loss 0.7238 Accuracy 0.7537\n",
      "Epoch 78 Batch 550 Loss 0.7239 Accuracy 0.7537\n",
      "Epoch 78 Batch 600 Loss 0.7251 Accuracy 0.7533\n",
      "Epoch 78 Batch 650 Loss 0.7256 Accuracy 0.7534\n",
      "Epoch 78 Batch 700 Loss 0.7258 Accuracy 0.7535\n",
      "Epoch 78 Batch 750 Loss 0.7256 Accuracy 0.7534\n",
      "Epoch 78 Batch 800 Loss 0.7259 Accuracy 0.7533\n",
      "Epoch 78 Batch 850 Loss 0.7257 Accuracy 0.7534\n",
      "Epoch 78 Batch 900 Loss 0.7262 Accuracy 0.7533\n",
      "Epoch 78 Batch 950 Loss 0.7265 Accuracy 0.7532\n",
      "Epoch 78 Batch 1000 Loss 0.7264 Accuracy 0.7533\n",
      "Epoch 78 Batch 1050 Loss 0.7270 Accuracy 0.7529\n",
      "Epoch 78 Batch 1100 Loss 0.7270 Accuracy 0.7528\n",
      "Epoch 78 Batch 1150 Loss 0.7272 Accuracy 0.7529\n",
      "Epoch 78 Batch 1200 Loss 0.7277 Accuracy 0.7529\n",
      "Epoch 78 Batch 1250 Loss 0.7281 Accuracy 0.7526\n",
      "Epoch 78 Batch 1300 Loss 0.7282 Accuracy 0.7526\n",
      "Epoch 78 Batch 1350 Loss 0.7279 Accuracy 0.7526\n",
      "Epoch 78 Batch 1400 Loss 0.7278 Accuracy 0.7526\n",
      "discarded batch 1435\n",
      "Epoch 78 Batch 1450 Loss 0.7280 Accuracy 0.7526\n",
      "Epoch 78 Batch 1500 Loss 0.7281 Accuracy 0.7526\n",
      "Epoch 78 Loss 0.7283 Accuracy 0.7525\n",
      "Time taken for 1 epoch: 36.322551250457764 secs\n",
      "\n",
      "epoch lasted: 36.32623052597046\n",
      "Epoch 79 Batch 0 Loss 0.6827 Accuracy 0.7708\n",
      "Epoch 79 Batch 50 Loss 0.7235 Accuracy 0.7557\n",
      "Epoch 79 Batch 100 Loss 0.7292 Accuracy 0.7535\n",
      "Epoch 79 Batch 150 Loss 0.7265 Accuracy 0.7538\n",
      "discarded batch 160\n",
      "Epoch 79 Batch 200 Loss 0.7235 Accuracy 0.7550\n",
      "Epoch 79 Batch 250 Loss 0.7210 Accuracy 0.7557\n",
      "Epoch 79 Batch 300 Loss 0.7231 Accuracy 0.7549\n",
      "Epoch 79 Batch 350 Loss 0.7222 Accuracy 0.7556\n",
      "Epoch 79 Batch 400 Loss 0.7230 Accuracy 0.7552\n",
      "Epoch 79 Batch 450 Loss 0.7227 Accuracy 0.7551\n",
      "Epoch 79 Batch 500 Loss 0.7229 Accuracy 0.7551\n",
      "Epoch 79 Batch 550 Loss 0.7231 Accuracy 0.7548\n",
      "Epoch 79 Batch 600 Loss 0.7228 Accuracy 0.7550\n",
      "Epoch 79 Batch 650 Loss 0.7231 Accuracy 0.7549\n",
      "Epoch 79 Batch 700 Loss 0.7235 Accuracy 0.7546\n",
      "Epoch 79 Batch 750 Loss 0.7236 Accuracy 0.7545\n",
      "Epoch 79 Batch 800 Loss 0.7237 Accuracy 0.7545\n",
      "Epoch 79 Batch 850 Loss 0.7243 Accuracy 0.7544\n",
      "Epoch 79 Batch 900 Loss 0.7246 Accuracy 0.7542\n",
      "Epoch 79 Batch 950 Loss 0.7248 Accuracy 0.7541\n",
      "Epoch 79 Batch 1000 Loss 0.7247 Accuracy 0.7541\n",
      "Epoch 79 Batch 1050 Loss 0.7249 Accuracy 0.7540\n",
      "Epoch 79 Batch 1100 Loss 0.7257 Accuracy 0.7539\n",
      "Epoch 79 Batch 1150 Loss 0.7256 Accuracy 0.7538\n",
      "Epoch 79 Batch 1200 Loss 0.7257 Accuracy 0.7538\n",
      "Epoch 79 Batch 1250 Loss 0.7262 Accuracy 0.7537\n",
      "Epoch 79 Batch 1300 Loss 0.7268 Accuracy 0.7535\n",
      "Epoch 79 Batch 1350 Loss 0.7265 Accuracy 0.7536\n",
      "Epoch 79 Batch 1400 Loss 0.7266 Accuracy 0.7535\n",
      "Epoch 79 Batch 1450 Loss 0.7271 Accuracy 0.7534\n",
      "Epoch 79 Batch 1500 Loss 0.7269 Accuracy 0.7534\n",
      "Epoch 79 Loss 0.7269 Accuracy 0.7534\n",
      "Time taken for 1 epoch: 36.3629674911499 secs\n",
      "\n",
      "epoch lasted: 36.36667847633362\n",
      "Epoch 80 Batch 0 Loss 0.7483 Accuracy 0.7392\n",
      "Epoch 80 Batch 50 Loss 0.7191 Accuracy 0.7561\n",
      "Epoch 80 Batch 100 Loss 0.7257 Accuracy 0.7538\n",
      "Epoch 80 Batch 150 Loss 0.7246 Accuracy 0.7547\n",
      "Epoch 80 Batch 200 Loss 0.7223 Accuracy 0.7551\n",
      "Epoch 80 Batch 250 Loss 0.7185 Accuracy 0.7559\n",
      "Epoch 80 Batch 300 Loss 0.7199 Accuracy 0.7555\n",
      "Epoch 80 Batch 350 Loss 0.7187 Accuracy 0.7560\n",
      "Epoch 80 Batch 400 Loss 0.7202 Accuracy 0.7554\n",
      "Epoch 80 Batch 450 Loss 0.7199 Accuracy 0.7555\n",
      "Epoch 80 Batch 500 Loss 0.7206 Accuracy 0.7553\n",
      "Epoch 80 Batch 550 Loss 0.7221 Accuracy 0.7549\n",
      "Epoch 80 Batch 600 Loss 0.7235 Accuracy 0.7545\n",
      "Epoch 80 Batch 650 Loss 0.7234 Accuracy 0.7545\n",
      "Epoch 80 Batch 700 Loss 0.7228 Accuracy 0.7547\n",
      "Epoch 80 Batch 750 Loss 0.7228 Accuracy 0.7548\n",
      "Epoch 80 Batch 800 Loss 0.7232 Accuracy 0.7548\n",
      "Epoch 80 Batch 850 Loss 0.7234 Accuracy 0.7547\n",
      "Epoch 80 Batch 900 Loss 0.7241 Accuracy 0.7546\n",
      "Epoch 80 Batch 950 Loss 0.7250 Accuracy 0.7542\n",
      "Epoch 80 Batch 1000 Loss 0.7250 Accuracy 0.7542\n",
      "Epoch 80 Batch 1050 Loss 0.7246 Accuracy 0.7543\n",
      "Epoch 80 Batch 1100 Loss 0.7248 Accuracy 0.7541\n",
      "Epoch 80 Batch 1150 Loss 0.7252 Accuracy 0.7540\n",
      "Epoch 80 Batch 1200 Loss 0.7255 Accuracy 0.7538\n",
      "Epoch 80 Batch 1250 Loss 0.7252 Accuracy 0.7540\n",
      "Epoch 80 Batch 1300 Loss 0.7252 Accuracy 0.7540\n",
      "Epoch 80 Batch 1350 Loss 0.7254 Accuracy 0.7539\n",
      "Epoch 80 Batch 1400 Loss 0.7252 Accuracy 0.7539\n",
      "Epoch 80 Batch 1450 Loss 0.7254 Accuracy 0.7539\n",
      "Epoch 80 Batch 1500 Loss 0.7253 Accuracy 0.7539\n",
      "discarded batch 1518\n",
      "Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-16\n",
      "Epoch 80 Loss 0.7255 Accuracy 0.7539\n",
      "Time taken for 1 epoch: 36.69874548912048 secs\n",
      "\n",
      "epoch lasted: 36.702773571014404\n",
      "Epoch 81 Batch 0 Loss 0.6877 Accuracy 0.7658\n",
      "Epoch 81 Batch 50 Loss 0.7146 Accuracy 0.7576\n",
      "Epoch 81 Batch 100 Loss 0.7101 Accuracy 0.7591\n",
      "Epoch 81 Batch 150 Loss 0.7128 Accuracy 0.7582\n",
      "Epoch 81 Batch 200 Loss 0.7142 Accuracy 0.7572\n",
      "Epoch 81 Batch 250 Loss 0.7136 Accuracy 0.7575\n",
      "Epoch 81 Batch 300 Loss 0.7157 Accuracy 0.7567\n",
      "Epoch 81 Batch 350 Loss 0.7161 Accuracy 0.7564\n",
      "Epoch 81 Batch 400 Loss 0.7162 Accuracy 0.7565\n",
      "Epoch 81 Batch 450 Loss 0.7170 Accuracy 0.7563\n",
      "Epoch 81 Batch 500 Loss 0.7184 Accuracy 0.7558\n",
      "Epoch 81 Batch 550 Loss 0.7204 Accuracy 0.7550\n",
      "Epoch 81 Batch 600 Loss 0.7214 Accuracy 0.7548\n",
      "Epoch 81 Batch 650 Loss 0.7212 Accuracy 0.7550\n",
      "Epoch 81 Batch 700 Loss 0.7213 Accuracy 0.7548\n",
      "Epoch 81 Batch 750 Loss 0.7215 Accuracy 0.7549\n",
      "Epoch 81 Batch 800 Loss 0.7218 Accuracy 0.7548\n",
      "Epoch 81 Batch 850 Loss 0.7224 Accuracy 0.7546\n",
      "Epoch 81 Batch 900 Loss 0.7228 Accuracy 0.7544\n",
      "Epoch 81 Batch 950 Loss 0.7228 Accuracy 0.7544\n",
      "Epoch 81 Batch 1000 Loss 0.7229 Accuracy 0.7544\n",
      "Epoch 81 Batch 1050 Loss 0.7231 Accuracy 0.7543\n",
      "Epoch 81 Batch 1100 Loss 0.7234 Accuracy 0.7541\n",
      "discarded batch 1125\n",
      "Epoch 81 Batch 1150 Loss 0.7232 Accuracy 0.7541\n",
      "Epoch 81 Batch 1200 Loss 0.7234 Accuracy 0.7542\n",
      "Epoch 81 Batch 1250 Loss 0.7237 Accuracy 0.7541\n",
      "Epoch 81 Batch 1300 Loss 0.7234 Accuracy 0.7542\n",
      "Epoch 81 Batch 1350 Loss 0.7235 Accuracy 0.7542\n",
      "Epoch 81 Batch 1400 Loss 0.7234 Accuracy 0.7542\n",
      "Epoch 81 Batch 1450 Loss 0.7237 Accuracy 0.7542\n",
      "Epoch 81 Batch 1500 Loss 0.7234 Accuracy 0.7543\n",
      "Epoch 81 Loss 0.7236 Accuracy 0.7543\n",
      "Time taken for 1 epoch: 36.33315825462341 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 81 VALIDATION: Loss 0.7335 Accuracy 0.7529\n",
      "\n",
      "la | tua | be| ni| gni| tà | non | pur | soc| cor| re|\n",
      "a | chi | do| man| da | ma | mol| te | fï| a| te|\n",
      "li| be| ra| men| te al | di| man| dar | pre| cor| re|\n",
      "                                        \n",
      " e | di | là | do| ve | si | vol| ge | si | sa| te|\n",
      "che | la | vi| sta | vi| sta | se| gui| tar | vi| va|\n",
      "e | la | mia | di | so| vra | si | vol| ge | sa| te|\n",
      "                                          \n",
      " e | di| scer| na| ta | si | ri| pre| se | spi| va|\n",
      "co| sì | di | so| vra | sì | che | si | spi| ra|\n",
      "sì | vol| ge | di | sé | ve| de| sti | di | so| sta|\n",
      "  \n",
      "e | io | a | lui | di| ver| si | la | vi| sta| ta|\n",
      "che | si | ri| vol| ge | la | vi| sta | via | vi| sta|\n",
      "co| sì | di | la | vi| sta | di | sé | di| sta| ta|\n",
      " e | al| lo a | lui | di| si| ri| ta | di| stri| sta|\n",
      "di | sé | che | se| gui| ta | se| gui| ta| va | si| sta|\n",
      "co| sì | di | là | do| ve | la | vi| sta | vi| sta|\n",
      "                                      \n",
      " e | al| tri | si | ra| gio| ne | si | ra| gio| ni|\n",
      "e | di | ve| der | la | vi| sta | vi| sta | vi| sta|\n",
      "che | la | vi| sta | vi| sta | se| gui| ta | vi| sta|\n",
      "  \n",
      "e | la | vi| sta | che | se| gui| ta | si | ri| stri| sta|\n",
      "la | mia | di| stri| sta | se| gui| ta | si | pi| sta|\n",
      "co| sì | di | la | vi| sta | di | sé | di| spo| sta|\n",
      " e | al| tri | vi| sta | che | la | vi| sta | se| gui|\n",
      "e | di | sua | vi| sta | vi| sta | vi| sta | vi| sta|\n",
      "co| sì | di | la| scia| va | di | sé | a| ve| ri|\n",
      "                                          \n",
      " e | al| tri | si | ra| gion | di | sé | pa| re| sta|\n",
      "co| sì | di| scer| na| va | la | vi| sta | vi| sta|\n",
      "che | la | vi| sta | vi| sta | vi| sta | vi| sta | vi| sta|\n",
      " e|\n",
      "al | mio | a| vea | di | sé | a | la | sua | vi| sta|\n",
      "che | la | vi| sta | se| gui| ta | si | ri| pi| sta|\n",
      "co| sì | di| nan| zi a | la | vi| sta | sua | vi| sta|\n",
      "                                   \n",
      " e | di| sta| ta | vi| sta | mi | fa| vi| gi| sta|\n",
      "la | mia | di| sta| men| te | che | si | spi| ri|\n",
      "la | vi| sta | vi| sta | vi| sta | vi| sta | vi| sta|\n",
      "  \n",
      "e | la | vi| sta | che | si | vol| ge | si | ri| sta|\n",
      "co| sì | di| sta| va | la | vi| sta|\n",
      "co| sì | di| spi| ri| ta|\n",
      "co| sì | la | vi| sta | di | sé | di| spo| sta|\n",
      " e | al| tri| ma | vi| sta | che | si | ri| pi| sta|\n",
      "la | mia | di| sti| zia | di | sé | vi| sta| ta|\n",
      "e | di | so| vra | la | vi| sta | se| gui| ta|\n",
      "di | sé | vi| sta|\n",
      "                                 \n",
      " e | di | sé | di| sta| ta | di | sé | di| si| sta|\n",
      "la | mia | di| sce| sa | di | sua | vi| sta | vi| sta|\n",
      "che | la | vi| sta | vi| sta | vi| sta | vi| sta | vi| sta|\n",
      "  \n",
      "e | di | la| scia| va | di | sé | la | vi| sta | vi| sta|\n",
      "co| sì | di| si| sta | vi| sta| va| mo | si| sta|\n",
      "co| sì | di| scer| na | si | vol| ge | di | sé|\n",
      "  \n",
      "e | al| tri | con | la | vi| sta | si | ri| spo| sta| ta|\n",
      "co| sì | di| sta| ta | vi| sta | vi| sta | vi| sta|\n",
      "co| sì | di| nan| zi a | la | vir| tù | di| spo| sta|\n",
      " e | al| tri | che | la | vi| sta | si | ri| vi| sta|\n",
      "la | mia | di| sta| ta | di | so| vra | se| gui|\n",
      "e | di | so| vra | la | vi| sta | di | sé | vi| sta|\n",
      "                                             \n",
      " che | si | ri| vol| ge | di | sé | la | sua | vi| sta|\n",
      "co| sì | di| sce| sa | vi| sta | se| gui| ta|\n",
      "di | sé | di| scor| te | vi| sta | la | vi| sta|\n",
      "  \n",
      "e | io | a | lui | di| si| ri | di | sé | a| spet| ta|\n",
      "co| sì | di| si| de | spe| ra|\n",
      "co| sì | di | sua | vi| sta|\n",
      "co| me | si | ri| vol| ta | la | vi| sta | ve| ra|\n",
      "  \n",
      "e | di| sco| per| ta | la | vi| sta | si | ri| ve| ra|\n",
      "che | si | ri| pa | la | vi| sta | vi| sta | vi| sta|\n",
      "co| sì | di| scer| na | si | vol| ge | di| ste| ra|\n",
      " e | di| spa| re|\n",
      "co| sì | di | sé | la | vi| sta | vi| sta|\n",
      "co| sì | di| sta| ta | spa| da| ta| ta | so| sta|\n",
      "co| sì | di| sce| se | la | vi| sta | vi| sta|\n",
      "                                           \n",
      " e | di| sta| ta | di | sé | di | sé | di| si| sta|\n",
      "la | mia | di| sce| sa | di | sua | vi| sta| ta|\n",
      "di | sé | di| ver| so | sa| li| zia | di | sé|\n",
      "  \n",
      " co| sì | di| ver| sa|\n",
      "co| sì | di | sé | la | spia| da|\n",
      "co| sì | di| sta| ta | se| gui| ta|\n",
      "co| sì | di| scer| so|\n",
      "e | io | a | la | vi| sta | di | sé | di| scor| sa|\n",
      "                                 \n",
      " e | di| scer| na| ta | si | ri| pa| re | sa| li|\n",
      "co| sì | di| scer| na| va | la | sua | vi| sta|\n",
      "di| stra| va | la | vi| sta|\n",
      "co| sì | di | sé | non | si | sa| li|\n",
      "\n",
      "epoch lasted: 568.7100071907043\n",
      "Epoch 82 Batch 0 Loss 0.6962 Accuracy 0.7508\n",
      "Epoch 82 Batch 50 Loss 0.7195 Accuracy 0.7555\n",
      "Epoch 82 Batch 100 Loss 0.7158 Accuracy 0.7565\n",
      "Epoch 82 Batch 150 Loss 0.7191 Accuracy 0.7553\n",
      "Epoch 82 Batch 200 Loss 0.7196 Accuracy 0.7549\n",
      "Epoch 82 Batch 250 Loss 0.7200 Accuracy 0.7545\n",
      "Epoch 82 Batch 300 Loss 0.7205 Accuracy 0.7547\n",
      "discarded batch 308\n",
      "Epoch 82 Batch 350 Loss 0.7184 Accuracy 0.7555\n",
      "Epoch 82 Batch 400 Loss 0.7181 Accuracy 0.7559\n",
      "Epoch 82 Batch 450 Loss 0.7185 Accuracy 0.7558\n",
      "Epoch 82 Batch 500 Loss 0.7199 Accuracy 0.7555\n",
      "Epoch 82 Batch 550 Loss 0.7220 Accuracy 0.7548\n",
      "Epoch 82 Batch 600 Loss 0.7220 Accuracy 0.7549\n",
      "Epoch 82 Batch 650 Loss 0.7223 Accuracy 0.7547\n",
      "Epoch 82 Batch 700 Loss 0.7226 Accuracy 0.7547\n",
      "Epoch 82 Batch 750 Loss 0.7225 Accuracy 0.7547\n",
      "Epoch 82 Batch 800 Loss 0.7226 Accuracy 0.7547\n",
      "Epoch 82 Batch 850 Loss 0.7221 Accuracy 0.7548\n",
      "Epoch 82 Batch 900 Loss 0.7222 Accuracy 0.7547\n",
      "Epoch 82 Batch 950 Loss 0.7223 Accuracy 0.7547\n",
      "Epoch 82 Batch 1000 Loss 0.7218 Accuracy 0.7548\n",
      "Epoch 82 Batch 1050 Loss 0.7215 Accuracy 0.7549\n",
      "Epoch 82 Batch 1100 Loss 0.7215 Accuracy 0.7548\n",
      "Epoch 82 Batch 1150 Loss 0.7213 Accuracy 0.7549\n",
      "Epoch 82 Batch 1200 Loss 0.7218 Accuracy 0.7547\n",
      "Epoch 82 Batch 1250 Loss 0.7219 Accuracy 0.7547\n",
      "Epoch 82 Batch 1300 Loss 0.7222 Accuracy 0.7546\n",
      "Epoch 82 Batch 1350 Loss 0.7221 Accuracy 0.7546\n",
      "Epoch 82 Batch 1400 Loss 0.7220 Accuracy 0.7547\n",
      "Epoch 82 Batch 1450 Loss 0.7220 Accuracy 0.7547\n",
      "Epoch 82 Batch 1500 Loss 0.7225 Accuracy 0.7545\n",
      "Epoch 82 Loss 0.7223 Accuracy 0.7546\n",
      "Time taken for 1 epoch: 36.04341387748718 secs\n",
      "\n",
      "epoch lasted: 36.04749274253845\n",
      "Epoch 83 Batch 0 Loss 0.7404 Accuracy 0.7558\n",
      "Epoch 83 Batch 50 Loss 0.7093 Accuracy 0.7604\n",
      "Epoch 83 Batch 100 Loss 0.7197 Accuracy 0.7566\n",
      "Epoch 83 Batch 150 Loss 0.7156 Accuracy 0.7573\n",
      "discarded batch 195\n",
      "Epoch 83 Batch 200 Loss 0.7149 Accuracy 0.7570\n",
      "Epoch 83 Batch 250 Loss 0.7139 Accuracy 0.7567\n",
      "Epoch 83 Batch 300 Loss 0.7138 Accuracy 0.7572\n",
      "Epoch 83 Batch 350 Loss 0.7140 Accuracy 0.7568\n",
      "Epoch 83 Batch 400 Loss 0.7147 Accuracy 0.7567\n",
      "Epoch 83 Batch 450 Loss 0.7162 Accuracy 0.7564\n",
      "Epoch 83 Batch 500 Loss 0.7164 Accuracy 0.7564\n",
      "Epoch 83 Batch 550 Loss 0.7165 Accuracy 0.7565\n",
      "Epoch 83 Batch 600 Loss 0.7166 Accuracy 0.7565\n",
      "Epoch 83 Batch 650 Loss 0.7179 Accuracy 0.7561\n",
      "Epoch 83 Batch 700 Loss 0.7175 Accuracy 0.7561\n",
      "Epoch 83 Batch 750 Loss 0.7176 Accuracy 0.7559\n",
      "Epoch 83 Batch 800 Loss 0.7188 Accuracy 0.7556\n",
      "Epoch 83 Batch 850 Loss 0.7194 Accuracy 0.7553\n",
      "Epoch 83 Batch 900 Loss 0.7197 Accuracy 0.7553\n",
      "Epoch 83 Batch 950 Loss 0.7195 Accuracy 0.7555\n",
      "Epoch 83 Batch 1000 Loss 0.7191 Accuracy 0.7556\n",
      "Epoch 83 Batch 1050 Loss 0.7190 Accuracy 0.7558\n",
      "Epoch 83 Batch 1100 Loss 0.7196 Accuracy 0.7556\n",
      "Epoch 83 Batch 1150 Loss 0.7201 Accuracy 0.7554\n",
      "Epoch 83 Batch 1200 Loss 0.7204 Accuracy 0.7553\n",
      "Epoch 83 Batch 1250 Loss 0.7208 Accuracy 0.7552\n",
      "Epoch 83 Batch 1300 Loss 0.7209 Accuracy 0.7551\n",
      "Epoch 83 Batch 1350 Loss 0.7209 Accuracy 0.7551\n",
      "Epoch 83 Batch 1400 Loss 0.7214 Accuracy 0.7550\n",
      "Epoch 83 Batch 1450 Loss 0.7214 Accuracy 0.7551\n",
      "Epoch 83 Batch 1500 Loss 0.7216 Accuracy 0.7550\n",
      "Epoch 83 Loss 0.7218 Accuracy 0.7549\n",
      "Time taken for 1 epoch: 35.94740962982178 secs\n",
      "\n",
      "epoch lasted: 35.95120930671692\n",
      "Epoch 84 Batch 0 Loss 0.6631 Accuracy 0.7625\n",
      "Epoch 84 Batch 50 Loss 0.7087 Accuracy 0.7571\n",
      "Epoch 84 Batch 100 Loss 0.7160 Accuracy 0.7550\n",
      "discarded batch 130\n",
      "Epoch 84 Batch 150 Loss 0.7126 Accuracy 0.7569\n",
      "Epoch 84 Batch 200 Loss 0.7115 Accuracy 0.7575\n",
      "Epoch 84 Batch 250 Loss 0.7143 Accuracy 0.7570\n",
      "Epoch 84 Batch 300 Loss 0.7163 Accuracy 0.7566\n",
      "Epoch 84 Batch 350 Loss 0.7158 Accuracy 0.7568\n",
      "Epoch 84 Batch 400 Loss 0.7164 Accuracy 0.7568\n",
      "Epoch 84 Batch 450 Loss 0.7171 Accuracy 0.7564\n",
      "Epoch 84 Batch 500 Loss 0.7172 Accuracy 0.7561\n",
      "Epoch 84 Batch 550 Loss 0.7178 Accuracy 0.7558\n",
      "Epoch 84 Batch 600 Loss 0.7178 Accuracy 0.7558\n",
      "Epoch 84 Batch 650 Loss 0.7169 Accuracy 0.7560\n",
      "Epoch 84 Batch 700 Loss 0.7177 Accuracy 0.7555\n",
      "Epoch 84 Batch 750 Loss 0.7179 Accuracy 0.7554\n",
      "Epoch 84 Batch 800 Loss 0.7177 Accuracy 0.7556\n",
      "Epoch 84 Batch 850 Loss 0.7178 Accuracy 0.7554\n",
      "Epoch 84 Batch 900 Loss 0.7176 Accuracy 0.7555\n",
      "Epoch 84 Batch 950 Loss 0.7180 Accuracy 0.7554\n",
      "Epoch 84 Batch 1000 Loss 0.7185 Accuracy 0.7553\n",
      "Epoch 84 Batch 1050 Loss 0.7189 Accuracy 0.7552\n",
      "Epoch 84 Batch 1100 Loss 0.7194 Accuracy 0.7551\n",
      "Epoch 84 Batch 1150 Loss 0.7194 Accuracy 0.7552\n",
      "Epoch 84 Batch 1200 Loss 0.7195 Accuracy 0.7552\n",
      "Epoch 84 Batch 1250 Loss 0.7197 Accuracy 0.7551\n",
      "Epoch 84 Batch 1300 Loss 0.7195 Accuracy 0.7552\n",
      "Epoch 84 Batch 1350 Loss 0.7198 Accuracy 0.7552\n",
      "Epoch 84 Batch 1400 Loss 0.7197 Accuracy 0.7553\n",
      "Epoch 84 Batch 1450 Loss 0.7195 Accuracy 0.7553\n",
      "Epoch 84 Batch 1500 Loss 0.7197 Accuracy 0.7553\n",
      "Epoch 84 Loss 0.7198 Accuracy 0.7553\n",
      "Time taken for 1 epoch: 36.17158484458923 secs\n",
      "\n",
      "epoch lasted: 36.17539024353027\n",
      "Epoch 85 Batch 0 Loss 0.6627 Accuracy 0.7674\n",
      "Epoch 85 Batch 50 Loss 0.7094 Accuracy 0.7573\n",
      "discarded batch 65\n",
      "Epoch 85 Batch 100 Loss 0.7120 Accuracy 0.7582\n",
      "Epoch 85 Batch 150 Loss 0.7122 Accuracy 0.7584\n",
      "Epoch 85 Batch 200 Loss 0.7163 Accuracy 0.7563\n",
      "Epoch 85 Batch 250 Loss 0.7158 Accuracy 0.7565\n",
      "Epoch 85 Batch 300 Loss 0.7149 Accuracy 0.7570\n",
      "Epoch 85 Batch 350 Loss 0.7154 Accuracy 0.7568\n",
      "Epoch 85 Batch 400 Loss 0.7149 Accuracy 0.7572\n",
      "Epoch 85 Batch 450 Loss 0.7155 Accuracy 0.7571\n",
      "Epoch 85 Batch 500 Loss 0.7154 Accuracy 0.7571\n",
      "Epoch 85 Batch 550 Loss 0.7166 Accuracy 0.7567\n",
      "Epoch 85 Batch 600 Loss 0.7159 Accuracy 0.7569\n",
      "Epoch 85 Batch 650 Loss 0.7167 Accuracy 0.7566\n",
      "Epoch 85 Batch 700 Loss 0.7163 Accuracy 0.7567\n",
      "Epoch 85 Batch 750 Loss 0.7160 Accuracy 0.7569\n",
      "Epoch 85 Batch 800 Loss 0.7160 Accuracy 0.7569\n",
      "Epoch 85 Batch 850 Loss 0.7169 Accuracy 0.7566\n",
      "Epoch 85 Batch 900 Loss 0.7169 Accuracy 0.7567\n",
      "Epoch 85 Batch 950 Loss 0.7170 Accuracy 0.7566\n",
      "Epoch 85 Batch 1000 Loss 0.7178 Accuracy 0.7564\n",
      "Epoch 85 Batch 1050 Loss 0.7175 Accuracy 0.7565\n",
      "Epoch 85 Batch 1100 Loss 0.7177 Accuracy 0.7564\n",
      "Epoch 85 Batch 1150 Loss 0.7179 Accuracy 0.7565\n",
      "Epoch 85 Batch 1200 Loss 0.7176 Accuracy 0.7565\n",
      "Epoch 85 Batch 1250 Loss 0.7177 Accuracy 0.7564\n",
      "Epoch 85 Batch 1300 Loss 0.7178 Accuracy 0.7562\n",
      "Epoch 85 Batch 1350 Loss 0.7181 Accuracy 0.7562\n",
      "Epoch 85 Batch 1400 Loss 0.7183 Accuracy 0.7561\n",
      "Epoch 85 Batch 1450 Loss 0.7184 Accuracy 0.7561\n",
      "Epoch 85 Batch 1500 Loss 0.7186 Accuracy 0.7559\n",
      "Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-17\n",
      "Epoch 85 Loss 0.7186 Accuracy 0.7559\n",
      "Time taken for 1 epoch: 36.50401306152344 secs\n",
      "\n",
      "epoch lasted: 36.50917863845825\n",
      "Epoch 86 Batch 0 Loss 0.7369 Accuracy 0.7625\n",
      "Epoch 86 Batch 50 Loss 0.7070 Accuracy 0.7607\n",
      "Epoch 86 Batch 100 Loss 0.7060 Accuracy 0.7604\n",
      "Epoch 86 Batch 150 Loss 0.7063 Accuracy 0.7605\n",
      "Epoch 86 Batch 200 Loss 0.7093 Accuracy 0.7592\n",
      "Epoch 86 Batch 250 Loss 0.7094 Accuracy 0.7592\n",
      "Epoch 86 Batch 300 Loss 0.7099 Accuracy 0.7590\n",
      "Epoch 86 Batch 350 Loss 0.7101 Accuracy 0.7590\n",
      "Epoch 86 Batch 400 Loss 0.7092 Accuracy 0.7596\n",
      "Epoch 86 Batch 450 Loss 0.7102 Accuracy 0.7591\n",
      "Epoch 86 Batch 500 Loss 0.7109 Accuracy 0.7589\n",
      "Epoch 86 Batch 550 Loss 0.7120 Accuracy 0.7584\n",
      "Epoch 86 Batch 600 Loss 0.7122 Accuracy 0.7584\n",
      "Epoch 86 Batch 650 Loss 0.7135 Accuracy 0.7579\n",
      "Epoch 86 Batch 700 Loss 0.7137 Accuracy 0.7576\n",
      "Epoch 86 Batch 750 Loss 0.7144 Accuracy 0.7574\n",
      "discarded batch 766\n",
      "Epoch 86 Batch 800 Loss 0.7142 Accuracy 0.7574\n",
      "Epoch 86 Batch 850 Loss 0.7142 Accuracy 0.7574\n",
      "Epoch 86 Batch 900 Loss 0.7145 Accuracy 0.7573\n",
      "Epoch 86 Batch 950 Loss 0.7152 Accuracy 0.7571\n",
      "Epoch 86 Batch 1000 Loss 0.7158 Accuracy 0.7570\n",
      "Epoch 86 Batch 1050 Loss 0.7164 Accuracy 0.7568\n",
      "Epoch 86 Batch 1100 Loss 0.7161 Accuracy 0.7569\n",
      "Epoch 86 Batch 1150 Loss 0.7161 Accuracy 0.7569\n",
      "Epoch 86 Batch 1200 Loss 0.7162 Accuracy 0.7568\n",
      "Epoch 86 Batch 1250 Loss 0.7168 Accuracy 0.7566\n",
      "Epoch 86 Batch 1300 Loss 0.7167 Accuracy 0.7566\n",
      "Epoch 86 Batch 1350 Loss 0.7171 Accuracy 0.7565\n",
      "Epoch 86 Batch 1400 Loss 0.7173 Accuracy 0.7565\n",
      "Epoch 86 Batch 1450 Loss 0.7176 Accuracy 0.7564\n",
      "Epoch 86 Batch 1500 Loss 0.7176 Accuracy 0.7564\n",
      "Epoch 86 Loss 0.7177 Accuracy 0.7563\n",
      "Time taken for 1 epoch: 36.35027527809143 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 86 VALIDATION: Loss 0.7294 Accuracy 0.7553\n",
      "\n",
      "epoch lasted: 36.49773144721985\n",
      "Epoch 87 Batch 0 Loss 0.7565 Accuracy 0.7508\n",
      "Epoch 87 Batch 50 Loss 0.7002 Accuracy 0.7623\n",
      "Epoch 87 Batch 100 Loss 0.7073 Accuracy 0.7601\n",
      "Epoch 87 Batch 150 Loss 0.7050 Accuracy 0.7609\n",
      "Epoch 87 Batch 200 Loss 0.7061 Accuracy 0.7607\n",
      "Epoch 87 Batch 250 Loss 0.7090 Accuracy 0.7599\n",
      "Epoch 87 Batch 300 Loss 0.7096 Accuracy 0.7599\n",
      "Epoch 87 Batch 350 Loss 0.7091 Accuracy 0.7601\n",
      "Epoch 87 Batch 400 Loss 0.7100 Accuracy 0.7597\n",
      "Epoch 87 Batch 450 Loss 0.7104 Accuracy 0.7595\n",
      "discarded batch 468\n",
      "Epoch 87 Batch 500 Loss 0.7110 Accuracy 0.7592\n",
      "Epoch 87 Batch 550 Loss 0.7113 Accuracy 0.7588\n",
      "Epoch 87 Batch 600 Loss 0.7119 Accuracy 0.7586\n",
      "Epoch 87 Batch 650 Loss 0.7126 Accuracy 0.7582\n",
      "Epoch 87 Batch 700 Loss 0.7135 Accuracy 0.7579\n",
      "Epoch 87 Batch 750 Loss 0.7142 Accuracy 0.7575\n",
      "Epoch 87 Batch 800 Loss 0.7147 Accuracy 0.7574\n",
      "Epoch 87 Batch 850 Loss 0.7147 Accuracy 0.7574\n",
      "Epoch 87 Batch 900 Loss 0.7144 Accuracy 0.7574\n",
      "Epoch 87 Batch 950 Loss 0.7141 Accuracy 0.7575\n",
      "Epoch 87 Batch 1000 Loss 0.7146 Accuracy 0.7572\n",
      "Epoch 87 Batch 1050 Loss 0.7157 Accuracy 0.7568\n",
      "Epoch 87 Batch 1100 Loss 0.7154 Accuracy 0.7570\n",
      "Epoch 87 Batch 1150 Loss 0.7158 Accuracy 0.7568\n",
      "Epoch 87 Batch 1200 Loss 0.7158 Accuracy 0.7569\n",
      "Epoch 87 Batch 1250 Loss 0.7158 Accuracy 0.7568\n",
      "Epoch 87 Batch 1300 Loss 0.7161 Accuracy 0.7567\n",
      "Epoch 87 Batch 1350 Loss 0.7161 Accuracy 0.7568\n",
      "Epoch 87 Batch 1400 Loss 0.7159 Accuracy 0.7568\n",
      "Epoch 87 Batch 1450 Loss 0.7157 Accuracy 0.7569\n",
      "Epoch 87 Batch 1500 Loss 0.7158 Accuracy 0.7568\n",
      "Epoch 87 Loss 0.7157 Accuracy 0.7568\n",
      "Time taken for 1 epoch: 36.42856740951538 secs\n",
      "\n",
      "epoch lasted: 36.432658433914185\n",
      "Epoch 88 Batch 0 Loss 0.6799 Accuracy 0.7824\n",
      "Epoch 88 Batch 50 Loss 0.7074 Accuracy 0.7615\n",
      "Epoch 88 Batch 100 Loss 0.7138 Accuracy 0.7590\n",
      "Epoch 88 Batch 150 Loss 0.7136 Accuracy 0.7581\n",
      "discarded batch 177\n",
      "Epoch 88 Batch 200 Loss 0.7139 Accuracy 0.7576\n",
      "Epoch 88 Batch 250 Loss 0.7130 Accuracy 0.7579\n",
      "Epoch 88 Batch 300 Loss 0.7140 Accuracy 0.7576\n",
      "Epoch 88 Batch 350 Loss 0.7143 Accuracy 0.7573\n",
      "Epoch 88 Batch 400 Loss 0.7150 Accuracy 0.7569\n",
      "Epoch 88 Batch 450 Loss 0.7139 Accuracy 0.7573\n",
      "Epoch 88 Batch 500 Loss 0.7144 Accuracy 0.7570\n",
      "Epoch 88 Batch 550 Loss 0.7140 Accuracy 0.7571\n",
      "Epoch 88 Batch 600 Loss 0.7135 Accuracy 0.7571\n",
      "Epoch 88 Batch 650 Loss 0.7144 Accuracy 0.7570\n",
      "Epoch 88 Batch 700 Loss 0.7151 Accuracy 0.7567\n",
      "Epoch 88 Batch 750 Loss 0.7147 Accuracy 0.7568\n",
      "Epoch 88 Batch 800 Loss 0.7150 Accuracy 0.7569\n",
      "Epoch 88 Batch 850 Loss 0.7152 Accuracy 0.7567\n",
      "Epoch 88 Batch 900 Loss 0.7148 Accuracy 0.7569\n",
      "Epoch 88 Batch 950 Loss 0.7146 Accuracy 0.7569\n",
      "Epoch 88 Batch 1000 Loss 0.7146 Accuracy 0.7571\n",
      "Epoch 88 Batch 1050 Loss 0.7141 Accuracy 0.7572\n",
      "Epoch 88 Batch 1100 Loss 0.7139 Accuracy 0.7572\n",
      "Epoch 88 Batch 1150 Loss 0.7143 Accuracy 0.7571\n",
      "Epoch 88 Batch 1200 Loss 0.7143 Accuracy 0.7570\n",
      "Epoch 88 Batch 1250 Loss 0.7148 Accuracy 0.7569\n",
      "Epoch 88 Batch 1300 Loss 0.7151 Accuracy 0.7569\n",
      "Epoch 88 Batch 1350 Loss 0.7154 Accuracy 0.7567\n",
      "Epoch 88 Batch 1400 Loss 0.7155 Accuracy 0.7567\n",
      "Epoch 88 Batch 1450 Loss 0.7156 Accuracy 0.7567\n",
      "Epoch 88 Batch 1500 Loss 0.7155 Accuracy 0.7567\n",
      "Epoch 88 Loss 0.7155 Accuracy 0.7567\n",
      "Time taken for 1 epoch: 36.45369362831116 secs\n",
      "\n",
      "epoch lasted: 36.458536863327026\n",
      "Epoch 89 Batch 0 Loss 0.7211 Accuracy 0.7791\n",
      "Epoch 89 Batch 50 Loss 0.7002 Accuracy 0.7617\n",
      "Epoch 89 Batch 100 Loss 0.7065 Accuracy 0.7601\n",
      "Epoch 89 Batch 150 Loss 0.7074 Accuracy 0.7598\n",
      "Epoch 89 Batch 200 Loss 0.7082 Accuracy 0.7595\n",
      "Epoch 89 Batch 250 Loss 0.7083 Accuracy 0.7594\n",
      "Epoch 89 Batch 300 Loss 0.7110 Accuracy 0.7589\n",
      "Epoch 89 Batch 350 Loss 0.7106 Accuracy 0.7592\n",
      "Epoch 89 Batch 400 Loss 0.7114 Accuracy 0.7590\n",
      "Epoch 89 Batch 450 Loss 0.7116 Accuracy 0.7592\n",
      "Epoch 89 Batch 500 Loss 0.7122 Accuracy 0.7588\n",
      "Epoch 89 Batch 550 Loss 0.7126 Accuracy 0.7584\n",
      "Epoch 89 Batch 600 Loss 0.7134 Accuracy 0.7582\n",
      "Epoch 89 Batch 650 Loss 0.7132 Accuracy 0.7581\n",
      "Epoch 89 Batch 700 Loss 0.7128 Accuracy 0.7583\n",
      "Epoch 89 Batch 750 Loss 0.7132 Accuracy 0.7581\n",
      "Epoch 89 Batch 800 Loss 0.7135 Accuracy 0.7579\n",
      "Epoch 89 Batch 850 Loss 0.7135 Accuracy 0.7580\n",
      "Epoch 89 Batch 900 Loss 0.7127 Accuracy 0.7582\n",
      "Epoch 89 Batch 950 Loss 0.7129 Accuracy 0.7581\n",
      "Epoch 89 Batch 1000 Loss 0.7126 Accuracy 0.7583\n",
      "Epoch 89 Batch 1050 Loss 0.7120 Accuracy 0.7584\n",
      "Epoch 89 Batch 1100 Loss 0.7124 Accuracy 0.7582\n",
      "discarded batch 1106\n",
      "Epoch 89 Batch 1150 Loss 0.7124 Accuracy 0.7581\n",
      "Epoch 89 Batch 1200 Loss 0.7127 Accuracy 0.7580\n",
      "Epoch 89 Batch 1250 Loss 0.7131 Accuracy 0.7579\n",
      "Epoch 89 Batch 1300 Loss 0.7132 Accuracy 0.7578\n",
      "Epoch 89 Batch 1350 Loss 0.7133 Accuracy 0.7578\n",
      "Epoch 89 Batch 1400 Loss 0.7134 Accuracy 0.7577\n",
      "Epoch 89 Batch 1450 Loss 0.7136 Accuracy 0.7577\n",
      "Epoch 89 Batch 1500 Loss 0.7139 Accuracy 0.7576\n",
      "Epoch 89 Loss 0.7140 Accuracy 0.7577\n",
      "Time taken for 1 epoch: 36.46247625350952 secs\n",
      "\n",
      "epoch lasted: 36.46740937232971\n",
      "Epoch 90 Batch 0 Loss 0.6713 Accuracy 0.7691\n",
      "Epoch 90 Batch 50 Loss 0.7075 Accuracy 0.7565\n",
      "Epoch 90 Batch 100 Loss 0.7065 Accuracy 0.7586\n",
      "Epoch 90 Batch 150 Loss 0.7036 Accuracy 0.7598\n",
      "Epoch 90 Batch 200 Loss 0.7040 Accuracy 0.7599\n",
      "Epoch 90 Batch 250 Loss 0.7055 Accuracy 0.7595\n",
      "Epoch 90 Batch 300 Loss 0.7057 Accuracy 0.7594\n",
      "Epoch 90 Batch 350 Loss 0.7069 Accuracy 0.7591\n",
      "Epoch 90 Batch 400 Loss 0.7091 Accuracy 0.7586\n",
      "Epoch 90 Batch 450 Loss 0.7088 Accuracy 0.7588\n",
      "Epoch 90 Batch 500 Loss 0.7093 Accuracy 0.7586\n",
      "discarded batch 513\n",
      "Epoch 90 Batch 550 Loss 0.7090 Accuracy 0.7587\n",
      "Epoch 90 Batch 600 Loss 0.7093 Accuracy 0.7587\n",
      "Epoch 90 Batch 650 Loss 0.7095 Accuracy 0.7588\n",
      "Epoch 90 Batch 700 Loss 0.7097 Accuracy 0.7586\n",
      "Epoch 90 Batch 750 Loss 0.7086 Accuracy 0.7590\n",
      "Epoch 90 Batch 800 Loss 0.7098 Accuracy 0.7586\n",
      "Epoch 90 Batch 850 Loss 0.7102 Accuracy 0.7585\n",
      "Epoch 90 Batch 900 Loss 0.7108 Accuracy 0.7583\n",
      "Epoch 90 Batch 950 Loss 0.7112 Accuracy 0.7581\n",
      "Epoch 90 Batch 1000 Loss 0.7119 Accuracy 0.7579\n",
      "Epoch 90 Batch 1050 Loss 0.7123 Accuracy 0.7579\n",
      "Epoch 90 Batch 1100 Loss 0.7127 Accuracy 0.7576\n",
      "Epoch 90 Batch 1150 Loss 0.7130 Accuracy 0.7575\n",
      "Epoch 90 Batch 1200 Loss 0.7131 Accuracy 0.7576\n",
      "Epoch 90 Batch 1250 Loss 0.7131 Accuracy 0.7576\n",
      "Epoch 90 Batch 1300 Loss 0.7130 Accuracy 0.7576\n",
      "Epoch 90 Batch 1350 Loss 0.7129 Accuracy 0.7576\n",
      "Epoch 90 Batch 1400 Loss 0.7129 Accuracy 0.7577\n",
      "Epoch 90 Batch 1450 Loss 0.7127 Accuracy 0.7578\n",
      "Epoch 90 Batch 1500 Loss 0.7128 Accuracy 0.7577\n",
      "Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-18\n",
      "Epoch 90 Loss 0.7132 Accuracy 0.7577\n",
      "Time taken for 1 epoch: 36.75231885910034 secs\n",
      "\n",
      "epoch lasted: 36.756226539611816\n",
      "Epoch 91 Batch 0 Loss 0.6784 Accuracy 0.7824\n",
      "Epoch 91 Batch 50 Loss 0.6983 Accuracy 0.7648\n",
      "Epoch 91 Batch 100 Loss 0.6974 Accuracy 0.7635\n",
      "Epoch 91 Batch 150 Loss 0.6950 Accuracy 0.7636\n",
      "Epoch 91 Batch 200 Loss 0.6966 Accuracy 0.7628\n",
      "Epoch 91 Batch 250 Loss 0.7015 Accuracy 0.7608\n",
      "Epoch 91 Batch 300 Loss 0.7028 Accuracy 0.7606\n",
      "Epoch 91 Batch 350 Loss 0.7037 Accuracy 0.7602\n",
      "Epoch 91 Batch 400 Loss 0.7034 Accuracy 0.7604\n",
      "Epoch 91 Batch 450 Loss 0.7045 Accuracy 0.7599\n",
      "Epoch 91 Batch 500 Loss 0.7052 Accuracy 0.7597\n",
      "discarded batch 531\n",
      "Epoch 91 Batch 550 Loss 0.7059 Accuracy 0.7595\n",
      "Epoch 91 Batch 600 Loss 0.7066 Accuracy 0.7591\n",
      "Epoch 91 Batch 650 Loss 0.7068 Accuracy 0.7590\n",
      "Epoch 91 Batch 700 Loss 0.7071 Accuracy 0.7589\n",
      "Epoch 91 Batch 750 Loss 0.7074 Accuracy 0.7587\n",
      "Epoch 91 Batch 800 Loss 0.7084 Accuracy 0.7585\n",
      "Epoch 91 Batch 850 Loss 0.7080 Accuracy 0.7587\n",
      "Epoch 91 Batch 900 Loss 0.7077 Accuracy 0.7588\n",
      "Epoch 91 Batch 950 Loss 0.7086 Accuracy 0.7585\n",
      "Epoch 91 Batch 1000 Loss 0.7092 Accuracy 0.7584\n",
      "Epoch 91 Batch 1050 Loss 0.7097 Accuracy 0.7582\n",
      "Epoch 91 Batch 1100 Loss 0.7098 Accuracy 0.7583\n",
      "Epoch 91 Batch 1150 Loss 0.7104 Accuracy 0.7583\n",
      "Epoch 91 Batch 1200 Loss 0.7109 Accuracy 0.7582\n",
      "Epoch 91 Batch 1250 Loss 0.7111 Accuracy 0.7581\n",
      "Epoch 91 Batch 1300 Loss 0.7114 Accuracy 0.7580\n",
      "Epoch 91 Batch 1350 Loss 0.7119 Accuracy 0.7579\n",
      "Epoch 91 Batch 1400 Loss 0.7120 Accuracy 0.7579\n",
      "Epoch 91 Batch 1450 Loss 0.7117 Accuracy 0.7580\n",
      "Epoch 91 Batch 1500 Loss 0.7116 Accuracy 0.7581\n",
      "Epoch 91 Loss 0.7116 Accuracy 0.7582\n",
      "Time taken for 1 epoch: 36.35127854347229 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 91 VALIDATION: Loss 0.7288 Accuracy 0.7545\n",
      "\n",
      "la | tua | be| ni| gni| tà | non | pur | soc| cor| re|\n",
      "a | chi | do| man| da | ma | mol| te | fï| a| te|\n",
      "li| be| ra| men| te al | di| man| dar | pre| cor| re|\n",
      "                                        \n",
      " co| me | si | vol| se | co| me | si | ri| va| te|\n",
      "co| me | si | mo| vea | se | non | sa| reb| be an| ni|\n",
      "co| me | si | vol| se | l’ a| ni| ma | di| spa| ga|\n",
      "                                           \n",
      " co| me | si | par| ve | sua | me| na | se| gue|\n",
      "la | mia | mia | mi | par| ve | sì | co| me | sca| ga|\n",
      "la | mia | di| scen| de | sì | co| me | suo| na|\n",
      "  \n",
      " e | co| me | se| gui| va | l’ al| tra | mia | mi| ra|\n",
      "che | si | mo| ve | so| lo | spe| ra| va | sco| me|\n",
      "co| me | si | sco| glie| ra | se| gui| ta | sco| me|\n",
      "                                         \n",
      " la | mia | di| man| da | mia | di | so| lo| men| te|\n",
      "co| me | si | spi| ri| ta | se | non | sa| li| re|\n",
      "co| me | si | po| scia | di | me | se | non | men| te|\n",
      "  \n",
      " e | ve| de| mon| da | la | mia | con | l’ al| tre | sca| va|\n",
      "co| me | si | spe| ra| va | sì | co| me | sco| scia|\n",
      "co| me | si | mo| ve | si | par| ve | si | sca| va|\n",
      "                                  \n",
      " co| me | si | spa| re | sua | ve| der | la | sco| scia|\n",
      "co| me | so| lo | spe| ra| va | la | mia | vi| sta|\n",
      "che | si | mo| ve | so| lo| na | se| gui| va| no|\n",
      "  \n",
      " e | la | mia | con| tra | che | si | ri| vol| ge| ma|\n",
      "co| me | si | vol| ge | di | so| lo in| ten| de| ra|\n",
      "co| me | si | par| ve | so| lo in | su | la | ma| ni|\n",
      "                                       \n",
      " co| me | si | po| scia | di | sé | l’ al| tra | sco| la|\n",
      "co| me | so| lo | spi| ra | la | sua | vi| sta| ni|\n",
      "co| me | si | po| scia | di | so| vra | se| gui| na|\n",
      "  \n",
      " co|\n",
      "a | la | mia | do| ve | la | mia | di| man| da| va|\n",
      "co| sì | di| scor| da | la | mia | di| scor| da|\n",
      "co| sì | si | mo| ve | si | mo| ve | so| lo|\n",
      "                                                  \n",
      " e | di| sce| si | so| la | se| gui| va | scor| da|\n",
      "co| me | si | vol| se | l’ a| ni| ma | se| gui| na|\n",
      "co| me | si | vol| se | l’ a| ni| ma | con | l’ a| ni|\n",
      "  \n",
      " co| me | si | vol| se | con | la | mia | se| gua | vol| se|\n",
      "co| me | si | vol| se | con | la | mia | men| ta| ni|\n",
      "co| me | se| gui| va | la | mia | men| te | scol| se|\n",
      "                                \n",
      " co| me | si | mo| ve| ra| men| te | si | scol| se|\n",
      "la | mia | mia | mi | par| la | mia | mia | mi| su| ra|\n",
      "co| me | si | mo| ve| ra| men| te | se| gui| na|\n",
      "  \n",
      " co| me | si | vol| se | co| me | si | vol| se | sar| la|\n",
      "co| me | si | vol| se | con | la | sco| glia | po| na|\n",
      "co| sì | si | par| ve | sua | me| na | vi| va|\n",
      "                                         \n",
      " co| me | si | par| ve | sua | me| na | si | sco| na|\n",
      "co| me | si | spi| ri| ta| va | se | non | spi| va|\n",
      "co| me | si | vol| se | va | la | mia | men| te| sta|\n",
      "  \n",
      " e | ve| di | si | vol| se | co| me | si | s’ a| va| va|\n",
      "co| me | si | vol| se | con | la | sco| glia | mol| se|\n",
      "co| sì | si | par| ve | sua | me| na | via|\n",
      "                                            \n",
      " co| me | si | pa| rea | di| spo| sta | si | scol| se|\n",
      "co| me | si | spi| ri| tra | se | non | s’ ap| pres| sa|\n",
      "co| me | si | vol| se | l’ a| ni| ma | di| scol| se|\n",
      "  \n",
      "\n",
      "epoch lasted: 418.11788392066956\n",
      "Epoch 92 Batch 0 Loss 0.6469 Accuracy 0.7924\n",
      "Epoch 92 Batch 50 Loss 0.7099 Accuracy 0.7614\n",
      "Epoch 92 Batch 100 Loss 0.7071 Accuracy 0.7604\n",
      "Epoch 92 Batch 150 Loss 0.7080 Accuracy 0.7603\n",
      "Epoch 92 Batch 200 Loss 0.7078 Accuracy 0.7606\n",
      "Epoch 92 Batch 250 Loss 0.7070 Accuracy 0.7604\n",
      "Epoch 92 Batch 300 Loss 0.7072 Accuracy 0.7600\n",
      "Epoch 92 Batch 350 Loss 0.7087 Accuracy 0.7593\n",
      "Epoch 92 Batch 400 Loss 0.7094 Accuracy 0.7589\n",
      "Epoch 92 Batch 450 Loss 0.7088 Accuracy 0.7592\n",
      "Epoch 92 Batch 500 Loss 0.7084 Accuracy 0.7593\n",
      "Epoch 92 Batch 550 Loss 0.7089 Accuracy 0.7590\n",
      "Epoch 92 Batch 600 Loss 0.7085 Accuracy 0.7592\n",
      "Epoch 92 Batch 650 Loss 0.7084 Accuracy 0.7591\n",
      "Epoch 92 Batch 700 Loss 0.7091 Accuracy 0.7589\n",
      "Epoch 92 Batch 750 Loss 0.7091 Accuracy 0.7588\n",
      "Epoch 92 Batch 800 Loss 0.7094 Accuracy 0.7586\n",
      "Epoch 92 Batch 850 Loss 0.7098 Accuracy 0.7585\n",
      "Epoch 92 Batch 900 Loss 0.7094 Accuracy 0.7585\n",
      "Epoch 92 Batch 950 Loss 0.7095 Accuracy 0.7585\n",
      "Epoch 92 Batch 1000 Loss 0.7100 Accuracy 0.7583\n",
      "Epoch 92 Batch 1050 Loss 0.7103 Accuracy 0.7583\n",
      "Epoch 92 Batch 1100 Loss 0.7101 Accuracy 0.7583\n",
      "Epoch 92 Batch 1150 Loss 0.7102 Accuracy 0.7583\n",
      "Epoch 92 Batch 1200 Loss 0.7105 Accuracy 0.7582\n",
      "Epoch 92 Batch 1250 Loss 0.7102 Accuracy 0.7584\n",
      "discarded batch 1299\n",
      "Epoch 92 Batch 1300 Loss 0.7100 Accuracy 0.7585\n",
      "Epoch 92 Batch 1350 Loss 0.7099 Accuracy 0.7585\n",
      "Epoch 92 Batch 1400 Loss 0.7105 Accuracy 0.7584\n",
      "Epoch 92 Batch 1450 Loss 0.7107 Accuracy 0.7583\n",
      "Epoch 92 Batch 1500 Loss 0.7106 Accuracy 0.7584\n",
      "Epoch 92 Loss 0.7106 Accuracy 0.7584\n",
      "Time taken for 1 epoch: 36.24996495246887 secs\n",
      "\n",
      "epoch lasted: 36.253894090652466\n",
      "Epoch 93 Batch 0 Loss 0.7242 Accuracy 0.7442\n",
      "Epoch 93 Batch 50 Loss 0.7051 Accuracy 0.7607\n",
      "Epoch 93 Batch 100 Loss 0.7034 Accuracy 0.7601\n",
      "discarded batch 148\n",
      "Epoch 93 Batch 150 Loss 0.7042 Accuracy 0.7593\n",
      "Epoch 93 Batch 200 Loss 0.7051 Accuracy 0.7593\n",
      "Epoch 93 Batch 250 Loss 0.7043 Accuracy 0.7602\n",
      "Epoch 93 Batch 300 Loss 0.7041 Accuracy 0.7602\n",
      "Epoch 93 Batch 350 Loss 0.7051 Accuracy 0.7597\n",
      "Epoch 93 Batch 400 Loss 0.7056 Accuracy 0.7595\n",
      "Epoch 93 Batch 450 Loss 0.7050 Accuracy 0.7595\n",
      "Epoch 93 Batch 500 Loss 0.7053 Accuracy 0.7595\n",
      "Epoch 93 Batch 550 Loss 0.7058 Accuracy 0.7595\n",
      "Epoch 93 Batch 600 Loss 0.7059 Accuracy 0.7594\n",
      "Epoch 93 Batch 650 Loss 0.7061 Accuracy 0.7591\n",
      "Epoch 93 Batch 700 Loss 0.7063 Accuracy 0.7592\n",
      "Epoch 93 Batch 750 Loss 0.7057 Accuracy 0.7595\n",
      "Epoch 93 Batch 800 Loss 0.7066 Accuracy 0.7593\n",
      "Epoch 93 Batch 850 Loss 0.7064 Accuracy 0.7594\n",
      "Epoch 93 Batch 900 Loss 0.7065 Accuracy 0.7593\n",
      "Epoch 93 Batch 950 Loss 0.7068 Accuracy 0.7591\n",
      "Epoch 93 Batch 1000 Loss 0.7070 Accuracy 0.7591\n",
      "Epoch 93 Batch 1050 Loss 0.7070 Accuracy 0.7591\n",
      "Epoch 93 Batch 1100 Loss 0.7069 Accuracy 0.7592\n",
      "Epoch 93 Batch 1150 Loss 0.7071 Accuracy 0.7592\n",
      "Epoch 93 Batch 1200 Loss 0.7072 Accuracy 0.7591\n",
      "Epoch 93 Batch 1250 Loss 0.7072 Accuracy 0.7592\n",
      "Epoch 93 Batch 1300 Loss 0.7072 Accuracy 0.7592\n",
      "Epoch 93 Batch 1350 Loss 0.7074 Accuracy 0.7592\n",
      "Epoch 93 Batch 1400 Loss 0.7075 Accuracy 0.7592\n",
      "Epoch 93 Batch 1450 Loss 0.7080 Accuracy 0.7590\n",
      "Epoch 93 Batch 1500 Loss 0.7084 Accuracy 0.7590\n",
      "Epoch 93 Loss 0.7087 Accuracy 0.7589\n",
      "Time taken for 1 epoch: 36.597251415252686 secs\n",
      "\n",
      "epoch lasted: 36.60062599182129\n",
      "Epoch 94 Batch 0 Loss 0.6733 Accuracy 0.7724\n",
      "Epoch 94 Batch 50 Loss 0.6848 Accuracy 0.7680\n",
      "Epoch 94 Batch 100 Loss 0.6994 Accuracy 0.7633\n",
      "Epoch 94 Batch 150 Loss 0.7010 Accuracy 0.7619\n",
      "Epoch 94 Batch 200 Loss 0.7044 Accuracy 0.7607\n",
      "Epoch 94 Batch 250 Loss 0.7023 Accuracy 0.7614\n",
      "Epoch 94 Batch 300 Loss 0.7010 Accuracy 0.7616\n",
      "discarded batch 321\n",
      "Epoch 94 Batch 350 Loss 0.7024 Accuracy 0.7610\n",
      "Epoch 94 Batch 400 Loss 0.7033 Accuracy 0.7608\n",
      "Epoch 94 Batch 450 Loss 0.7039 Accuracy 0.7607\n",
      "Epoch 94 Batch 500 Loss 0.7041 Accuracy 0.7606\n",
      "Epoch 94 Batch 550 Loss 0.7043 Accuracy 0.7605\n",
      "Epoch 94 Batch 600 Loss 0.7046 Accuracy 0.7606\n",
      "Epoch 94 Batch 650 Loss 0.7045 Accuracy 0.7606\n",
      "Epoch 94 Batch 700 Loss 0.7053 Accuracy 0.7603\n",
      "Epoch 94 Batch 750 Loss 0.7045 Accuracy 0.7604\n",
      "Epoch 94 Batch 800 Loss 0.7047 Accuracy 0.7604\n",
      "Epoch 94 Batch 850 Loss 0.7047 Accuracy 0.7604\n",
      "Epoch 94 Batch 900 Loss 0.7046 Accuracy 0.7605\n",
      "Epoch 94 Batch 950 Loss 0.7051 Accuracy 0.7602\n",
      "Epoch 94 Batch 1000 Loss 0.7053 Accuracy 0.7601\n",
      "Epoch 94 Batch 1050 Loss 0.7054 Accuracy 0.7601\n",
      "Epoch 94 Batch 1100 Loss 0.7057 Accuracy 0.7599\n",
      "Epoch 94 Batch 1150 Loss 0.7058 Accuracy 0.7601\n",
      "Epoch 94 Batch 1200 Loss 0.7062 Accuracy 0.7599\n",
      "Epoch 94 Batch 1250 Loss 0.7067 Accuracy 0.7597\n",
      "Epoch 94 Batch 1300 Loss 0.7066 Accuracy 0.7598\n",
      "Epoch 94 Batch 1350 Loss 0.7069 Accuracy 0.7597\n",
      "Epoch 94 Batch 1400 Loss 0.7075 Accuracy 0.7594\n",
      "Epoch 94 Batch 1450 Loss 0.7081 Accuracy 0.7592\n",
      "Epoch 94 Batch 1500 Loss 0.7085 Accuracy 0.7591\n",
      "Epoch 94 Loss 0.7088 Accuracy 0.7590\n",
      "Time taken for 1 epoch: 36.30908131599426 secs\n",
      "\n",
      "epoch lasted: 36.313249826431274\n",
      "Epoch 95 Batch 0 Loss 0.7711 Accuracy 0.7542\n",
      "Epoch 95 Batch 50 Loss 0.6975 Accuracy 0.7643\n",
      "Epoch 95 Batch 100 Loss 0.7027 Accuracy 0.7624\n",
      "Epoch 95 Batch 150 Loss 0.7031 Accuracy 0.7619\n",
      "Epoch 95 Batch 200 Loss 0.7027 Accuracy 0.7616\n",
      "Epoch 95 Batch 250 Loss 0.7017 Accuracy 0.7617\n",
      "Epoch 95 Batch 300 Loss 0.7013 Accuracy 0.7616\n",
      "Epoch 95 Batch 350 Loss 0.7017 Accuracy 0.7614\n",
      "Epoch 95 Batch 400 Loss 0.7013 Accuracy 0.7613\n",
      "Epoch 95 Batch 450 Loss 0.7010 Accuracy 0.7616\n",
      "Epoch 95 Batch 500 Loss 0.7003 Accuracy 0.7618\n",
      "Epoch 95 Batch 550 Loss 0.7010 Accuracy 0.7614\n",
      "Epoch 95 Batch 600 Loss 0.7021 Accuracy 0.7610\n",
      "Epoch 95 Batch 650 Loss 0.7022 Accuracy 0.7609\n",
      "Epoch 95 Batch 700 Loss 0.7031 Accuracy 0.7607\n",
      "Epoch 95 Batch 750 Loss 0.7032 Accuracy 0.7608\n",
      "discarded batch 798\n",
      "Epoch 95 Batch 800 Loss 0.7036 Accuracy 0.7608\n",
      "Epoch 95 Batch 850 Loss 0.7039 Accuracy 0.7607\n",
      "Epoch 95 Batch 900 Loss 0.7039 Accuracy 0.7606\n",
      "Epoch 95 Batch 950 Loss 0.7050 Accuracy 0.7603\n",
      "Epoch 95 Batch 1000 Loss 0.7054 Accuracy 0.7601\n",
      "Epoch 95 Batch 1050 Loss 0.7057 Accuracy 0.7600\n",
      "Epoch 95 Batch 1100 Loss 0.7058 Accuracy 0.7600\n",
      "Epoch 95 Batch 1150 Loss 0.7058 Accuracy 0.7599\n",
      "Epoch 95 Batch 1200 Loss 0.7059 Accuracy 0.7598\n",
      "Epoch 95 Batch 1250 Loss 0.7065 Accuracy 0.7597\n",
      "Epoch 95 Batch 1300 Loss 0.7067 Accuracy 0.7596\n",
      "Epoch 95 Batch 1350 Loss 0.7071 Accuracy 0.7594\n",
      "Epoch 95 Batch 1400 Loss 0.7071 Accuracy 0.7593\n",
      "Epoch 95 Batch 1450 Loss 0.7070 Accuracy 0.7594\n",
      "Epoch 95 Batch 1500 Loss 0.7073 Accuracy 0.7593\n",
      "Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-19\n",
      "Epoch 95 Loss 0.7071 Accuracy 0.7595\n",
      "Time taken for 1 epoch: 36.83742070198059 secs\n",
      "\n",
      "epoch lasted: 36.84198069572449\n",
      "Epoch 96 Batch 0 Loss 0.6676 Accuracy 0.7890\n",
      "Epoch 96 Batch 50 Loss 0.6895 Accuracy 0.7663\n",
      "Epoch 96 Batch 100 Loss 0.6959 Accuracy 0.7633\n",
      "Epoch 96 Batch 150 Loss 0.6947 Accuracy 0.7632\n",
      "Epoch 96 Batch 200 Loss 0.6994 Accuracy 0.7617\n",
      "Epoch 96 Batch 250 Loss 0.6966 Accuracy 0.7626\n",
      "Epoch 96 Batch 300 Loss 0.6969 Accuracy 0.7625\n",
      "Epoch 96 Batch 350 Loss 0.6975 Accuracy 0.7621\n",
      "Epoch 96 Batch 400 Loss 0.6995 Accuracy 0.7615\n",
      "Epoch 96 Batch 450 Loss 0.7002 Accuracy 0.7611\n",
      "Epoch 96 Batch 500 Loss 0.6992 Accuracy 0.7614\n",
      "Epoch 96 Batch 550 Loss 0.6995 Accuracy 0.7614\n",
      "Epoch 96 Batch 600 Loss 0.6995 Accuracy 0.7615\n",
      "Epoch 96 Batch 650 Loss 0.6999 Accuracy 0.7614\n",
      "Epoch 96 Batch 700 Loss 0.7000 Accuracy 0.7615\n",
      "Epoch 96 Batch 750 Loss 0.7007 Accuracy 0.7613\n",
      "Epoch 96 Batch 800 Loss 0.7016 Accuracy 0.7611\n",
      "Epoch 96 Batch 850 Loss 0.7030 Accuracy 0.7607\n",
      "Epoch 96 Batch 900 Loss 0.7032 Accuracy 0.7607\n",
      "Epoch 96 Batch 950 Loss 0.7036 Accuracy 0.7605\n",
      "Epoch 96 Batch 1000 Loss 0.7035 Accuracy 0.7605\n",
      "Epoch 96 Batch 1050 Loss 0.7039 Accuracy 0.7605\n",
      "Epoch 96 Batch 1100 Loss 0.7048 Accuracy 0.7602\n",
      "Epoch 96 Batch 1150 Loss 0.7051 Accuracy 0.7601\n",
      "discarded batch 1199\n",
      "Epoch 96 Batch 1200 Loss 0.7054 Accuracy 0.7599\n",
      "Epoch 96 Batch 1250 Loss 0.7050 Accuracy 0.7601\n",
      "Epoch 96 Batch 1300 Loss 0.7050 Accuracy 0.7601\n",
      "Epoch 96 Batch 1350 Loss 0.7049 Accuracy 0.7602\n",
      "Epoch 96 Batch 1400 Loss 0.7051 Accuracy 0.7601\n",
      "Epoch 96 Batch 1450 Loss 0.7058 Accuracy 0.7599\n",
      "Epoch 96 Batch 1500 Loss 0.7058 Accuracy 0.7598\n",
      "Epoch 96 Loss 0.7060 Accuracy 0.7598\n",
      "Time taken for 1 epoch: 36.94472122192383 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 96 VALIDATION: Loss 0.7289 Accuracy 0.7569\n",
      "\n",
      "epoch lasted: 37.098050117492676\n",
      "Epoch 97 Batch 0 Loss 0.7116 Accuracy 0.7508\n",
      "Epoch 97 Batch 50 Loss 0.7025 Accuracy 0.7612\n",
      "Epoch 97 Batch 100 Loss 0.7051 Accuracy 0.7601\n",
      "Epoch 97 Batch 150 Loss 0.7028 Accuracy 0.7615\n",
      "discarded batch 157\n",
      "Epoch 97 Batch 200 Loss 0.7038 Accuracy 0.7608\n",
      "Epoch 97 Batch 250 Loss 0.7011 Accuracy 0.7615\n",
      "Epoch 97 Batch 300 Loss 0.7016 Accuracy 0.7616\n",
      "Epoch 97 Batch 350 Loss 0.7017 Accuracy 0.7613\n",
      "Epoch 97 Batch 400 Loss 0.7018 Accuracy 0.7616\n",
      "Epoch 97 Batch 450 Loss 0.7015 Accuracy 0.7616\n",
      "Epoch 97 Batch 500 Loss 0.7021 Accuracy 0.7612\n",
      "Epoch 97 Batch 550 Loss 0.7021 Accuracy 0.7612\n",
      "Epoch 97 Batch 600 Loss 0.7029 Accuracy 0.7608\n",
      "Epoch 97 Batch 650 Loss 0.7032 Accuracy 0.7606\n",
      "Epoch 97 Batch 700 Loss 0.7034 Accuracy 0.7607\n",
      "Epoch 97 Batch 750 Loss 0.7036 Accuracy 0.7606\n",
      "Epoch 97 Batch 800 Loss 0.7035 Accuracy 0.7606\n",
      "Epoch 97 Batch 850 Loss 0.7037 Accuracy 0.7605\n",
      "Epoch 97 Batch 900 Loss 0.7040 Accuracy 0.7604\n",
      "Epoch 97 Batch 950 Loss 0.7039 Accuracy 0.7605\n",
      "Epoch 97 Batch 1000 Loss 0.7037 Accuracy 0.7606\n",
      "Epoch 97 Batch 1050 Loss 0.7040 Accuracy 0.7605\n",
      "Epoch 97 Batch 1100 Loss 0.7043 Accuracy 0.7605\n",
      "Epoch 97 Batch 1150 Loss 0.7046 Accuracy 0.7604\n",
      "Epoch 97 Batch 1200 Loss 0.7049 Accuracy 0.7603\n",
      "Epoch 97 Batch 1250 Loss 0.7053 Accuracy 0.7602\n",
      "Epoch 97 Batch 1300 Loss 0.7055 Accuracy 0.7601\n",
      "Epoch 97 Batch 1350 Loss 0.7057 Accuracy 0.7601\n",
      "Epoch 97 Batch 1400 Loss 0.7054 Accuracy 0.7602\n",
      "Epoch 97 Batch 1450 Loss 0.7053 Accuracy 0.7602\n",
      "Epoch 97 Batch 1500 Loss 0.7051 Accuracy 0.7603\n",
      "Epoch 97 Loss 0.7051 Accuracy 0.7603\n",
      "Time taken for 1 epoch: 37.32353639602661 secs\n",
      "\n",
      "epoch lasted: 37.32711982727051\n",
      "Epoch 98 Batch 0 Loss 0.6474 Accuracy 0.7807\n",
      "Epoch 98 Batch 50 Loss 0.6947 Accuracy 0.7607\n",
      "Epoch 98 Batch 100 Loss 0.6988 Accuracy 0.7602\n",
      "Epoch 98 Batch 150 Loss 0.6986 Accuracy 0.7609\n",
      "Epoch 98 Batch 200 Loss 0.6986 Accuracy 0.7613\n",
      "Epoch 98 Batch 250 Loss 0.6988 Accuracy 0.7614\n",
      "Epoch 98 Batch 300 Loss 0.7002 Accuracy 0.7609\n",
      "Epoch 98 Batch 350 Loss 0.6999 Accuracy 0.7609\n",
      "Epoch 98 Batch 400 Loss 0.7004 Accuracy 0.7607\n",
      "Epoch 98 Batch 450 Loss 0.7005 Accuracy 0.7607\n",
      "Epoch 98 Batch 500 Loss 0.7003 Accuracy 0.7607\n",
      "Epoch 98 Batch 550 Loss 0.7001 Accuracy 0.7609\n",
      "Epoch 98 Batch 600 Loss 0.6996 Accuracy 0.7611\n",
      "Epoch 98 Batch 650 Loss 0.6996 Accuracy 0.7612\n",
      "Epoch 98 Batch 700 Loss 0.6987 Accuracy 0.7616\n",
      "Epoch 98 Batch 750 Loss 0.6982 Accuracy 0.7617\n",
      "Epoch 98 Batch 800 Loss 0.6981 Accuracy 0.7617\n",
      "Epoch 98 Batch 850 Loss 0.6989 Accuracy 0.7614\n",
      "Epoch 98 Batch 900 Loss 0.6993 Accuracy 0.7614\n",
      "Epoch 98 Batch 950 Loss 0.6999 Accuracy 0.7612\n",
      "Epoch 98 Batch 1000 Loss 0.7009 Accuracy 0.7610\n",
      "Epoch 98 Batch 1050 Loss 0.7013 Accuracy 0.7608\n",
      "Epoch 98 Batch 1100 Loss 0.7015 Accuracy 0.7608\n",
      "Epoch 98 Batch 1150 Loss 0.7017 Accuracy 0.7608\n",
      "Epoch 98 Batch 1200 Loss 0.7021 Accuracy 0.7607\n",
      "Epoch 98 Batch 1250 Loss 0.7024 Accuracy 0.7607\n",
      "Epoch 98 Batch 1300 Loss 0.7026 Accuracy 0.7606\n",
      "Epoch 98 Batch 1350 Loss 0.7030 Accuracy 0.7604\n",
      "Epoch 98 Batch 1400 Loss 0.7029 Accuracy 0.7604\n",
      "Epoch 98 Batch 1450 Loss 0.7029 Accuracy 0.7605\n",
      "Epoch 98 Batch 1500 Loss 0.7028 Accuracy 0.7605\n",
      "discarded batch 1516\n",
      "Epoch 98 Loss 0.7031 Accuracy 0.7605\n",
      "Time taken for 1 epoch: 36.51012301445007 secs\n",
      "\n",
      "epoch lasted: 36.51440715789795\n",
      "Epoch 99 Batch 0 Loss 0.7108 Accuracy 0.7475\n",
      "Epoch 99 Batch 50 Loss 0.6936 Accuracy 0.7642\n",
      "Epoch 99 Batch 100 Loss 0.6908 Accuracy 0.7662\n",
      "Epoch 99 Batch 150 Loss 0.6900 Accuracy 0.7660\n",
      "Epoch 99 Batch 200 Loss 0.6917 Accuracy 0.7652\n",
      "Epoch 99 Batch 250 Loss 0.6916 Accuracy 0.7654\n",
      "Epoch 99 Batch 300 Loss 0.6944 Accuracy 0.7646\n",
      "Epoch 99 Batch 350 Loss 0.6959 Accuracy 0.7637\n",
      "Epoch 99 Batch 400 Loss 0.6981 Accuracy 0.7631\n",
      "Epoch 99 Batch 450 Loss 0.6991 Accuracy 0.7627\n",
      "Epoch 99 Batch 500 Loss 0.7000 Accuracy 0.7623\n",
      "Epoch 99 Batch 550 Loss 0.7001 Accuracy 0.7621\n",
      "Epoch 99 Batch 600 Loss 0.7011 Accuracy 0.7619\n",
      "Epoch 99 Batch 650 Loss 0.7014 Accuracy 0.7618\n",
      "Epoch 99 Batch 700 Loss 0.7012 Accuracy 0.7618\n",
      "Epoch 99 Batch 750 Loss 0.7014 Accuracy 0.7617\n",
      "Epoch 99 Batch 800 Loss 0.7012 Accuracy 0.7617\n",
      "Epoch 99 Batch 850 Loss 0.7015 Accuracy 0.7616\n",
      "Epoch 99 Batch 900 Loss 0.7020 Accuracy 0.7613\n",
      "Epoch 99 Batch 950 Loss 0.7022 Accuracy 0.7612\n",
      "Epoch 99 Batch 1000 Loss 0.7026 Accuracy 0.7611\n",
      "Epoch 99 Batch 1050 Loss 0.7031 Accuracy 0.7610\n",
      "Epoch 99 Batch 1100 Loss 0.7029 Accuracy 0.7610\n",
      "discarded batch 1127\n",
      "Epoch 99 Batch 1150 Loss 0.7035 Accuracy 0.7608\n",
      "Epoch 99 Batch 1200 Loss 0.7038 Accuracy 0.7606\n",
      "Epoch 99 Batch 1250 Loss 0.7038 Accuracy 0.7606\n",
      "Epoch 99 Batch 1300 Loss 0.7042 Accuracy 0.7605\n",
      "Epoch 99 Batch 1350 Loss 0.7039 Accuracy 0.7605\n",
      "Epoch 99 Batch 1400 Loss 0.7037 Accuracy 0.7605\n",
      "Epoch 99 Batch 1450 Loss 0.7037 Accuracy 0.7604\n",
      "Epoch 99 Batch 1500 Loss 0.7037 Accuracy 0.7605\n",
      "Epoch 99 Loss 0.7036 Accuracy 0.7605\n",
      "Time taken for 1 epoch: 36.330503702163696 secs\n",
      "\n",
      "epoch lasted: 36.33467078208923\n",
      "Epoch 100 Batch 0 Loss 0.6682 Accuracy 0.7425\n",
      "Epoch 100 Batch 50 Loss 0.6880 Accuracy 0.7646\n",
      "Epoch 100 Batch 100 Loss 0.6912 Accuracy 0.7643\n",
      "Epoch 100 Batch 150 Loss 0.6945 Accuracy 0.7634\n",
      "Epoch 100 Batch 200 Loss 0.6942 Accuracy 0.7635\n",
      "Epoch 100 Batch 250 Loss 0.6946 Accuracy 0.7636\n",
      "Epoch 100 Batch 300 Loss 0.6938 Accuracy 0.7636\n",
      "Epoch 100 Batch 350 Loss 0.6941 Accuracy 0.7635\n",
      "Epoch 100 Batch 400 Loss 0.6956 Accuracy 0.7631\n",
      "Epoch 100 Batch 450 Loss 0.6963 Accuracy 0.7630\n",
      "Epoch 100 Batch 500 Loss 0.6969 Accuracy 0.7630\n",
      "Epoch 100 Batch 550 Loss 0.6980 Accuracy 0.7628\n",
      "Epoch 100 Batch 600 Loss 0.6981 Accuracy 0.7629\n",
      "Epoch 100 Batch 650 Loss 0.6978 Accuracy 0.7630\n",
      "Epoch 100 Batch 700 Loss 0.6980 Accuracy 0.7629\n",
      "Epoch 100 Batch 750 Loss 0.6980 Accuracy 0.7629\n",
      "Epoch 100 Batch 800 Loss 0.6985 Accuracy 0.7627\n",
      "Epoch 100 Batch 850 Loss 0.6979 Accuracy 0.7628\n",
      "Epoch 100 Batch 900 Loss 0.6990 Accuracy 0.7624\n",
      "discarded batch 908\n",
      "Epoch 100 Batch 950 Loss 0.6989 Accuracy 0.7624\n",
      "Epoch 100 Batch 1000 Loss 0.6988 Accuracy 0.7625\n",
      "Epoch 100 Batch 1050 Loss 0.6997 Accuracy 0.7621\n",
      "Epoch 100 Batch 1100 Loss 0.6998 Accuracy 0.7620\n",
      "Epoch 100 Batch 1150 Loss 0.7005 Accuracy 0.7618\n",
      "Epoch 100 Batch 1200 Loss 0.7011 Accuracy 0.7616\n",
      "Epoch 100 Batch 1250 Loss 0.7013 Accuracy 0.7614\n",
      "Epoch 100 Batch 1300 Loss 0.7012 Accuracy 0.7615\n",
      "Epoch 100 Batch 1350 Loss 0.7014 Accuracy 0.7614\n",
      "Epoch 100 Batch 1400 Loss 0.7017 Accuracy 0.7614\n",
      "Epoch 100 Batch 1450 Loss 0.7018 Accuracy 0.7613\n",
      "Epoch 100 Batch 1500 Loss 0.7020 Accuracy 0.7612\n",
      "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-20\n",
      "Epoch 100 Loss 0.7021 Accuracy 0.7611\n",
      "Time taken for 1 epoch: 36.576181411743164 secs\n",
      "\n",
      "epoch lasted: 36.57943058013916\n",
      "Epoch 101 Batch 0 Loss 0.6648 Accuracy 0.7791\n",
      "Epoch 101 Batch 50 Loss 0.6991 Accuracy 0.7652\n",
      "Epoch 101 Batch 100 Loss 0.6966 Accuracy 0.7648\n",
      "Epoch 101 Batch 150 Loss 0.6977 Accuracy 0.7634\n",
      "Epoch 101 Batch 200 Loss 0.6968 Accuracy 0.7630\n",
      "Epoch 101 Batch 250 Loss 0.6990 Accuracy 0.7622\n",
      "discarded batch 285\n",
      "Epoch 101 Batch 300 Loss 0.6989 Accuracy 0.7619\n",
      "Epoch 101 Batch 350 Loss 0.6979 Accuracy 0.7620\n",
      "Epoch 101 Batch 400 Loss 0.6963 Accuracy 0.7625\n",
      "Epoch 101 Batch 450 Loss 0.6961 Accuracy 0.7628\n",
      "Epoch 101 Batch 500 Loss 0.6976 Accuracy 0.7623\n",
      "Epoch 101 Batch 550 Loss 0.6976 Accuracy 0.7626\n",
      "Epoch 101 Batch 600 Loss 0.6975 Accuracy 0.7626\n",
      "Epoch 101 Batch 650 Loss 0.6968 Accuracy 0.7628\n",
      "Epoch 101 Batch 700 Loss 0.6972 Accuracy 0.7626\n",
      "Epoch 101 Batch 750 Loss 0.6981 Accuracy 0.7622\n",
      "Epoch 101 Batch 800 Loss 0.6980 Accuracy 0.7623\n",
      "Epoch 101 Batch 850 Loss 0.6983 Accuracy 0.7621\n",
      "Epoch 101 Batch 900 Loss 0.6987 Accuracy 0.7621\n",
      "Epoch 101 Batch 950 Loss 0.6990 Accuracy 0.7619\n",
      "Epoch 101 Batch 1000 Loss 0.6990 Accuracy 0.7620\n",
      "Epoch 101 Batch 1050 Loss 0.6992 Accuracy 0.7619\n",
      "Epoch 101 Batch 1100 Loss 0.6997 Accuracy 0.7619\n",
      "Epoch 101 Batch 1150 Loss 0.7001 Accuracy 0.7618\n",
      "Epoch 101 Batch 1200 Loss 0.6993 Accuracy 0.7620\n",
      "Epoch 101 Batch 1250 Loss 0.6997 Accuracy 0.7619\n",
      "Epoch 101 Batch 1300 Loss 0.7001 Accuracy 0.7617\n",
      "Epoch 101 Batch 1350 Loss 0.7005 Accuracy 0.7616\n",
      "Epoch 101 Batch 1400 Loss 0.7004 Accuracy 0.7616\n",
      "Epoch 101 Batch 1450 Loss 0.7007 Accuracy 0.7615\n",
      "Epoch 101 Batch 1500 Loss 0.7012 Accuracy 0.7613\n",
      "Epoch 101 Loss 0.7010 Accuracy 0.7614\n",
      "Time taken for 1 epoch: 36.51758623123169 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 101 VALIDATION: Loss 0.7301 Accuracy 0.7539\n",
      "\n",
      "la | tua | be| ni| gni| tà | non | pur | soc| cor| re|\n",
      "a | chi | do| man| da | ma | mol| te | fï| a| te|\n",
      "li| be| ra| men| te al | di| man| dar | pre| cor| re|\n",
      "                                        \n",
      " e | la | men| te | che | son | la | men| te | sa| te|\n",
      "la | men| te | che | son | li | so| le a | la | ro| ra|\n",
      "e | la | mia | ma| no | sen| te | mia | ma| ni| na|\n",
      "                                      \n",
      " e | la | mia | vi| sta | ma| ni| fe| sta | so| ra|\n",
      "la | men| te | che | la | men| te | la | mia | vi| sta|\n",
      "che | la | mia | di| si| ma | de | la | men| te | so| ra|\n",
      "  \n",
      "e | la | men| te | che | la | mia | mia | men| te| sta|\n",
      "la | men| te | vi| sta | mia | me| na| va | so| ma|\n",
      "co| me | si | ri| mi| glia | mia | ma| no | sce| sta|\n",
      "  \n",
      " e | la | mia | ma| ni| na | mia | ma | di| stria| da|\n",
      "che | la | me| stra | mia | ma| ni| fe| sta| na|\n",
      "di | me| glia | mi | pa| rea | la | vi| sta | via|\n",
      "                                              \n",
      " e | la | mia | ma| ni| fe| ria | di | sé | a| na|\n",
      "la | men| te | mia | ma| ni| fe| sti| na | mia|\n",
      "di | me| ga | me| glia|\n",
      "co| me | so| vra | la | men| te | la | ri| ga|\n",
      "  \n",
      " lo | mon| do | mio | di | me | a | la | me| stri| na|\n",
      "che | la | me| stra | mia | me| na| va| gheg| gia|\n",
      "co| me | so| vra | la | vi| sta | se | non | stel| va|\n",
      "                                        \n",
      " la | men| te | so| la | sco| glia | de | la | ri| va|\n",
      "che | la | men| te | so| la | se| gui| na | so| la|\n",
      "la | mia | ma| no | sco| glia | la | mia | men | vi| va|\n",
      "                                                                                                                                                                                                         \n",
      " la | men| te | la | men| te | di | ma| ni| fe| ga|\n",
      "la | men| te | che | la | mia | ma| ni| fe| sta| va|\n",
      "la | mia | men| te | so| la | se| gui| na | me| ga|\n",
      "  \n",
      "di | me| glie | di | me | per | la | men| do | me| gna|\n",
      "lo | mi| no| va | mia | me| na| va | mia | men| te|\n",
      "co| me | si | ri| ma| ni a | la | men| te | sce| gna|\n",
      "  \n",
      " lo | mio | a| vea | ma | non | sa| rei | a| scol| ta|\n",
      "e | al | mio | a | lui | co| me | sco| glio| na| ni|\n",
      "e | al | mio | a | lui | co| me | sì | a| vea|\n",
      "  \n",
      " e | al | mio | a | lui | al| tra | mi | fa| vi| glia|\n",
      "e | la | men| te | vi| sta | mia | vi| sta| va|\n",
      "di | men| te | la | men| te | che | la | men| te|\n",
      "                                                \n",
      " la | men| te | mia | ma| ni| fe| re | di| vi| glia|\n",
      "e | la | men| te | mia | men| te | la | men| te | so| ma|\n",
      "e | la | mia | men| te | mia | ma| ni| fe| re| va|\n",
      "  \n",
      " an| da|\n",
      "la | mia | ma| gi| nar | le | mem| bra | men| te|\n",
      "che | la | men| te | so| vra | se| gui| na | schia| ni|\n",
      "che | la | mia | ma| ni| ra | de | la | ma| ni| ra|\n",
      "                                  \n",
      " la | men| te | mia | ma| ni| fe| re | di | va| ni|\n",
      "a | la | men| te | mia | men| te | la | mia | vi| sta|\n",
      "e | la | mia | men| te | ma| ni| fe| re | sta| va|\n",
      "  \n",
      " lo | mon| do | mio | di | me | a | la | men| te| ra|\n",
      "che | la | men| te | mia | ma| ni| fe| ra|\n",
      "di | men| te | scen| de| re a | la | men| te | ste| ma|\n",
      "                                                \n",
      " e | la | mia | ma| ni| fe| ra | di | me| na| no|\n",
      "lo | ma| ni| fe| ria | me| ri| to | de | la | sce| ma|\n",
      "e | la | men| te | ma| ni| fe| ria | de | la | ri| ga|\n",
      "  \n",
      "che | di | me| ri| na | mia | ma| ni| fe| ri| ce|\n",
      "a | la | me| ri| na | me| sta | vi| va| nia|\n",
      "di | sa| li| ca| va| va|\n",
      "la | men| te | che | si | pa| rea | me| na|\n",
      "\n",
      "epoch lasted: 459.6414852142334\n",
      "Epoch 102 Batch 0 Loss 0.6838 Accuracy 0.7575\n",
      "Epoch 102 Batch 50 Loss 0.6966 Accuracy 0.7644\n",
      "Epoch 102 Batch 100 Loss 0.6917 Accuracy 0.7643\n",
      "Epoch 102 Batch 150 Loss 0.6925 Accuracy 0.7644\n",
      "Epoch 102 Batch 200 Loss 0.6913 Accuracy 0.7644\n",
      "Epoch 102 Batch 250 Loss 0.6943 Accuracy 0.7633\n",
      "Epoch 102 Batch 300 Loss 0.6925 Accuracy 0.7641\n",
      "Epoch 102 Batch 350 Loss 0.6944 Accuracy 0.7633\n",
      "Epoch 102 Batch 400 Loss 0.6950 Accuracy 0.7630\n",
      "Epoch 102 Batch 450 Loss 0.6955 Accuracy 0.7630\n",
      "discarded batch 488\n",
      "Epoch 102 Batch 500 Loss 0.6962 Accuracy 0.7627\n",
      "Epoch 102 Batch 550 Loss 0.6966 Accuracy 0.7626\n",
      "Epoch 102 Batch 600 Loss 0.6974 Accuracy 0.7625\n",
      "Epoch 102 Batch 650 Loss 0.6976 Accuracy 0.7624\n",
      "Epoch 102 Batch 700 Loss 0.6972 Accuracy 0.7625\n",
      "Epoch 102 Batch 750 Loss 0.6972 Accuracy 0.7624\n",
      "Epoch 102 Batch 800 Loss 0.6966 Accuracy 0.7628\n",
      "Epoch 102 Batch 850 Loss 0.6965 Accuracy 0.7627\n",
      "Epoch 102 Batch 900 Loss 0.6960 Accuracy 0.7628\n",
      "Epoch 102 Batch 950 Loss 0.6964 Accuracy 0.7626\n",
      "Epoch 102 Batch 1000 Loss 0.6968 Accuracy 0.7624\n",
      "Epoch 102 Batch 1050 Loss 0.6971 Accuracy 0.7624\n",
      "Epoch 102 Batch 1100 Loss 0.6973 Accuracy 0.7624\n",
      "Epoch 102 Batch 1150 Loss 0.6977 Accuracy 0.7622\n",
      "Epoch 102 Batch 1200 Loss 0.6989 Accuracy 0.7620\n",
      "Epoch 102 Batch 1250 Loss 0.6992 Accuracy 0.7619\n",
      "Epoch 102 Batch 1300 Loss 0.6990 Accuracy 0.7620\n",
      "Epoch 102 Batch 1350 Loss 0.6991 Accuracy 0.7619\n",
      "Epoch 102 Batch 1400 Loss 0.6993 Accuracy 0.7618\n",
      "Epoch 102 Batch 1450 Loss 0.6995 Accuracy 0.7618\n",
      "Epoch 102 Batch 1500 Loss 0.6998 Accuracy 0.7618\n",
      "Epoch 102 Loss 0.6998 Accuracy 0.7618\n",
      "Time taken for 1 epoch: 37.3937885761261 secs\n",
      "\n",
      "epoch lasted: 37.39872145652771\n",
      "Epoch 103 Batch 0 Loss 0.7148 Accuracy 0.7508\n",
      "Epoch 103 Batch 50 Loss 0.6950 Accuracy 0.7648\n",
      "Epoch 103 Batch 100 Loss 0.6896 Accuracy 0.7668\n",
      "Epoch 103 Batch 150 Loss 0.6954 Accuracy 0.7648\n",
      "Epoch 103 Batch 200 Loss 0.6945 Accuracy 0.7650\n",
      "Epoch 103 Batch 250 Loss 0.6943 Accuracy 0.7652\n",
      "Epoch 103 Batch 300 Loss 0.6954 Accuracy 0.7648\n",
      "Epoch 103 Batch 350 Loss 0.6971 Accuracy 0.7640\n",
      "discarded batch 380\n",
      "Epoch 103 Batch 400 Loss 0.6970 Accuracy 0.7638\n",
      "Epoch 103 Batch 450 Loss 0.6972 Accuracy 0.7636\n",
      "Epoch 103 Batch 500 Loss 0.6974 Accuracy 0.7633\n",
      "Epoch 103 Batch 550 Loss 0.6973 Accuracy 0.7632\n",
      "Epoch 103 Batch 600 Loss 0.6983 Accuracy 0.7629\n",
      "Epoch 103 Batch 650 Loss 0.6979 Accuracy 0.7629\n",
      "Epoch 103 Batch 700 Loss 0.6975 Accuracy 0.7629\n",
      "Epoch 103 Batch 750 Loss 0.6970 Accuracy 0.7631\n",
      "Epoch 103 Batch 800 Loss 0.6966 Accuracy 0.7631\n",
      "Epoch 103 Batch 850 Loss 0.6966 Accuracy 0.7631\n",
      "Epoch 103 Batch 900 Loss 0.6967 Accuracy 0.7630\n",
      "Epoch 103 Batch 950 Loss 0.6972 Accuracy 0.7629\n",
      "Epoch 103 Batch 1000 Loss 0.6972 Accuracy 0.7628\n",
      "Epoch 103 Batch 1050 Loss 0.6970 Accuracy 0.7629\n",
      "Epoch 103 Batch 1100 Loss 0.6973 Accuracy 0.7628\n",
      "Epoch 103 Batch 1150 Loss 0.6976 Accuracy 0.7628\n",
      "Epoch 103 Batch 1200 Loss 0.6981 Accuracy 0.7625\n",
      "Epoch 103 Batch 1250 Loss 0.6986 Accuracy 0.7624\n",
      "Epoch 103 Batch 1300 Loss 0.6985 Accuracy 0.7625\n",
      "Epoch 103 Batch 1350 Loss 0.6986 Accuracy 0.7625\n",
      "Epoch 103 Batch 1400 Loss 0.6988 Accuracy 0.7625\n",
      "Epoch 103 Batch 1450 Loss 0.6990 Accuracy 0.7624\n",
      "Epoch 103 Batch 1500 Loss 0.6990 Accuracy 0.7623\n",
      "Epoch 103 Loss 0.6990 Accuracy 0.7624\n",
      "Time taken for 1 epoch: 36.51653003692627 secs\n",
      "\n",
      "epoch lasted: 36.51991629600525\n",
      "Epoch 104 Batch 0 Loss 0.7200 Accuracy 0.7591\n",
      "Epoch 104 Batch 50 Loss 0.6885 Accuracy 0.7663\n",
      "Epoch 104 Batch 100 Loss 0.6846 Accuracy 0.7668\n",
      "Epoch 104 Batch 150 Loss 0.6851 Accuracy 0.7666\n",
      "Epoch 104 Batch 200 Loss 0.6871 Accuracy 0.7662\n",
      "Epoch 104 Batch 250 Loss 0.6903 Accuracy 0.7649\n",
      "discarded batch 298\n",
      "Epoch 104 Batch 300 Loss 0.6902 Accuracy 0.7649\n",
      "Epoch 104 Batch 350 Loss 0.6890 Accuracy 0.7650\n",
      "Epoch 104 Batch 400 Loss 0.6894 Accuracy 0.7651\n",
      "Epoch 104 Batch 450 Loss 0.6914 Accuracy 0.7645\n",
      "Epoch 104 Batch 500 Loss 0.6913 Accuracy 0.7646\n",
      "Epoch 104 Batch 550 Loss 0.6917 Accuracy 0.7643\n",
      "Epoch 104 Batch 600 Loss 0.6925 Accuracy 0.7641\n",
      "Epoch 104 Batch 650 Loss 0.6926 Accuracy 0.7638\n",
      "Epoch 104 Batch 700 Loss 0.6927 Accuracy 0.7637\n",
      "Epoch 104 Batch 750 Loss 0.6927 Accuracy 0.7636\n",
      "Epoch 104 Batch 800 Loss 0.6933 Accuracy 0.7635\n",
      "Epoch 104 Batch 850 Loss 0.6936 Accuracy 0.7635\n",
      "Epoch 104 Batch 900 Loss 0.6940 Accuracy 0.7634\n",
      "Epoch 104 Batch 950 Loss 0.6940 Accuracy 0.7633\n",
      "Epoch 104 Batch 1000 Loss 0.6946 Accuracy 0.7631\n",
      "Epoch 104 Batch 1050 Loss 0.6948 Accuracy 0.7630\n",
      "Epoch 104 Batch 1100 Loss 0.6951 Accuracy 0.7629\n",
      "Epoch 104 Batch 1150 Loss 0.6957 Accuracy 0.7628\n",
      "Epoch 104 Batch 1200 Loss 0.6956 Accuracy 0.7629\n",
      "Epoch 104 Batch 1250 Loss 0.6951 Accuracy 0.7631\n",
      "Epoch 104 Batch 1300 Loss 0.6954 Accuracy 0.7632\n",
      "Epoch 104 Batch 1350 Loss 0.6957 Accuracy 0.7631\n",
      "Epoch 104 Batch 1400 Loss 0.6967 Accuracy 0.7628\n",
      "Epoch 104 Batch 1450 Loss 0.6967 Accuracy 0.7627\n",
      "Epoch 104 Batch 1500 Loss 0.6971 Accuracy 0.7627\n",
      "Epoch 104 Loss 0.6971 Accuracy 0.7628\n",
      "Time taken for 1 epoch: 36.71216702461243 secs\n",
      "\n",
      "epoch lasted: 36.7162344455719\n",
      "Epoch 105 Batch 0 Loss 0.6721 Accuracy 0.7392\n",
      "Epoch 105 Batch 50 Loss 0.6816 Accuracy 0.7660\n",
      "discarded batch 70\n",
      "Epoch 105 Batch 100 Loss 0.6796 Accuracy 0.7681\n",
      "Epoch 105 Batch 150 Loss 0.6850 Accuracy 0.7661\n",
      "Epoch 105 Batch 200 Loss 0.6886 Accuracy 0.7660\n",
      "Epoch 105 Batch 250 Loss 0.6906 Accuracy 0.7647\n",
      "Epoch 105 Batch 300 Loss 0.6904 Accuracy 0.7650\n",
      "Epoch 105 Batch 350 Loss 0.6920 Accuracy 0.7643\n",
      "Epoch 105 Batch 400 Loss 0.6938 Accuracy 0.7637\n",
      "Epoch 105 Batch 450 Loss 0.6948 Accuracy 0.7633\n",
      "Epoch 105 Batch 500 Loss 0.6939 Accuracy 0.7637\n",
      "Epoch 105 Batch 550 Loss 0.6948 Accuracy 0.7634\n",
      "Epoch 105 Batch 600 Loss 0.6943 Accuracy 0.7636\n",
      "Epoch 105 Batch 650 Loss 0.6943 Accuracy 0.7636\n",
      "Epoch 105 Batch 700 Loss 0.6951 Accuracy 0.7635\n",
      "Epoch 105 Batch 750 Loss 0.6943 Accuracy 0.7637\n",
      "Epoch 105 Batch 800 Loss 0.6949 Accuracy 0.7636\n",
      "Epoch 105 Batch 850 Loss 0.6947 Accuracy 0.7636\n",
      "Epoch 105 Batch 900 Loss 0.6950 Accuracy 0.7635\n",
      "Epoch 105 Batch 950 Loss 0.6957 Accuracy 0.7633\n",
      "Epoch 105 Batch 1000 Loss 0.6953 Accuracy 0.7635\n",
      "Epoch 105 Batch 1050 Loss 0.6955 Accuracy 0.7633\n",
      "Epoch 105 Batch 1100 Loss 0.6956 Accuracy 0.7633\n",
      "Epoch 105 Batch 1150 Loss 0.6958 Accuracy 0.7633\n",
      "Epoch 105 Batch 1200 Loss 0.6963 Accuracy 0.7630\n",
      "Epoch 105 Batch 1250 Loss 0.6967 Accuracy 0.7630\n",
      "Epoch 105 Batch 1300 Loss 0.6962 Accuracy 0.7630\n",
      "Epoch 105 Batch 1350 Loss 0.6965 Accuracy 0.7629\n",
      "Epoch 105 Batch 1400 Loss 0.6972 Accuracy 0.7627\n",
      "Epoch 105 Batch 1450 Loss 0.6973 Accuracy 0.7626\n",
      "Epoch 105 Batch 1500 Loss 0.6970 Accuracy 0.7628\n",
      "Saving checkpoint for epoch 105 at ./checkpoints/train/ckpt-21\n",
      "Epoch 105 Loss 0.6972 Accuracy 0.7627\n",
      "Time taken for 1 epoch: 36.811774015426636 secs\n",
      "\n",
      "epoch lasted: 36.81602716445923\n",
      "Epoch 106 Batch 0 Loss 0.6594 Accuracy 0.7757\n",
      "Epoch 106 Batch 50 Loss 0.6881 Accuracy 0.7655\n",
      "Epoch 106 Batch 100 Loss 0.6876 Accuracy 0.7654\n",
      "Epoch 106 Batch 150 Loss 0.6911 Accuracy 0.7644\n",
      "Epoch 106 Batch 200 Loss 0.6914 Accuracy 0.7644\n",
      "Epoch 106 Batch 250 Loss 0.6912 Accuracy 0.7646\n",
      "Epoch 106 Batch 300 Loss 0.6900 Accuracy 0.7649\n",
      "Epoch 106 Batch 350 Loss 0.6895 Accuracy 0.7653\n",
      "Epoch 106 Batch 400 Loss 0.6903 Accuracy 0.7651\n",
      "Epoch 106 Batch 450 Loss 0.6912 Accuracy 0.7647\n",
      "Epoch 106 Batch 500 Loss 0.6905 Accuracy 0.7648\n",
      "Epoch 106 Batch 550 Loss 0.6906 Accuracy 0.7645\n",
      "Epoch 106 Batch 600 Loss 0.6911 Accuracy 0.7643\n",
      "Epoch 106 Batch 650 Loss 0.6912 Accuracy 0.7642\n",
      "Epoch 106 Batch 700 Loss 0.6915 Accuracy 0.7641\n",
      "Epoch 106 Batch 750 Loss 0.6914 Accuracy 0.7641\n",
      "Epoch 106 Batch 800 Loss 0.6916 Accuracy 0.7640\n",
      "discarded batch 822\n",
      "Epoch 106 Batch 850 Loss 0.6922 Accuracy 0.7638\n",
      "Epoch 106 Batch 900 Loss 0.6919 Accuracy 0.7638\n",
      "Epoch 106 Batch 950 Loss 0.6924 Accuracy 0.7635\n",
      "Epoch 106 Batch 1000 Loss 0.6931 Accuracy 0.7633\n",
      "Epoch 106 Batch 1050 Loss 0.6931 Accuracy 0.7635\n",
      "Epoch 106 Batch 1100 Loss 0.6937 Accuracy 0.7633\n",
      "Epoch 106 Batch 1150 Loss 0.6940 Accuracy 0.7632\n",
      "Epoch 106 Batch 1200 Loss 0.6936 Accuracy 0.7633\n",
      "Epoch 106 Batch 1250 Loss 0.6942 Accuracy 0.7632\n",
      "Epoch 106 Batch 1300 Loss 0.6945 Accuracy 0.7630\n",
      "Epoch 106 Batch 1350 Loss 0.6948 Accuracy 0.7629\n",
      "Epoch 106 Batch 1400 Loss 0.6948 Accuracy 0.7629\n",
      "Epoch 106 Batch 1450 Loss 0.6956 Accuracy 0.7626\n",
      "Epoch 106 Batch 1500 Loss 0.6958 Accuracy 0.7626\n",
      "Epoch 106 Loss 0.6961 Accuracy 0.7625\n",
      "Time taken for 1 epoch: 37.4678156375885 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 106 VALIDATION: Loss 0.7262 Accuracy 0.7568\n",
      "\n",
      "epoch lasted: 37.61635184288025\n",
      "Epoch 107 Batch 0 Loss 0.6969 Accuracy 0.7392\n",
      "Epoch 107 Batch 50 Loss 0.6974 Accuracy 0.7644\n",
      "Epoch 107 Batch 100 Loss 0.6906 Accuracy 0.7652\n",
      "Epoch 107 Batch 150 Loss 0.6885 Accuracy 0.7652\n",
      "Epoch 107 Batch 200 Loss 0.6879 Accuracy 0.7657\n",
      "Epoch 107 Batch 250 Loss 0.6894 Accuracy 0.7651\n",
      "Epoch 107 Batch 300 Loss 0.6892 Accuracy 0.7653\n",
      "Epoch 107 Batch 350 Loss 0.6897 Accuracy 0.7653\n",
      "Epoch 107 Batch 400 Loss 0.6909 Accuracy 0.7653\n",
      "Epoch 107 Batch 450 Loss 0.6914 Accuracy 0.7651\n",
      "Epoch 107 Batch 500 Loss 0.6909 Accuracy 0.7651\n",
      "Epoch 107 Batch 550 Loss 0.6915 Accuracy 0.7648\n",
      "Epoch 107 Batch 600 Loss 0.6914 Accuracy 0.7648\n",
      "Epoch 107 Batch 650 Loss 0.6918 Accuracy 0.7647\n",
      "Epoch 107 Batch 700 Loss 0.6924 Accuracy 0.7645\n",
      "Epoch 107 Batch 750 Loss 0.6924 Accuracy 0.7644\n",
      "Epoch 107 Batch 800 Loss 0.6924 Accuracy 0.7645\n",
      "Epoch 107 Batch 850 Loss 0.6930 Accuracy 0.7642\n",
      "Epoch 107 Batch 900 Loss 0.6928 Accuracy 0.7643\n",
      "Epoch 107 Batch 950 Loss 0.6929 Accuracy 0.7642\n",
      "discarded batch 960\n",
      "Epoch 107 Batch 1000 Loss 0.6933 Accuracy 0.7640\n",
      "Epoch 107 Batch 1050 Loss 0.6930 Accuracy 0.7641\n",
      "Epoch 107 Batch 1100 Loss 0.6930 Accuracy 0.7640\n",
      "Epoch 107 Batch 1150 Loss 0.6935 Accuracy 0.7639\n",
      "Epoch 107 Batch 1200 Loss 0.6938 Accuracy 0.7638\n",
      "Epoch 107 Batch 1250 Loss 0.6945 Accuracy 0.7636\n",
      "Epoch 107 Batch 1300 Loss 0.6949 Accuracy 0.7636\n",
      "Epoch 107 Batch 1350 Loss 0.6952 Accuracy 0.7635\n",
      "Epoch 107 Batch 1400 Loss 0.6955 Accuracy 0.7634\n",
      "Epoch 107 Batch 1450 Loss 0.6956 Accuracy 0.7634\n",
      "Epoch 107 Batch 1500 Loss 0.6959 Accuracy 0.7633\n",
      "Epoch 107 Loss 0.6958 Accuracy 0.7634\n",
      "Time taken for 1 epoch: 37.375784158706665 secs\n",
      "\n",
      "epoch lasted: 37.379247426986694\n",
      "Epoch 108 Batch 0 Loss 0.7261 Accuracy 0.7625\n",
      "Epoch 108 Batch 50 Loss 0.6862 Accuracy 0.7659\n",
      "Epoch 108 Batch 100 Loss 0.6869 Accuracy 0.7667\n",
      "Epoch 108 Batch 150 Loss 0.6850 Accuracy 0.7670\n",
      "Epoch 108 Batch 200 Loss 0.6838 Accuracy 0.7671\n",
      "Epoch 108 Batch 250 Loss 0.6841 Accuracy 0.7672\n",
      "discarded batch 273\n",
      "Epoch 108 Batch 300 Loss 0.6851 Accuracy 0.7668\n",
      "Epoch 108 Batch 350 Loss 0.6861 Accuracy 0.7665\n",
      "Epoch 108 Batch 400 Loss 0.6865 Accuracy 0.7662\n",
      "Epoch 108 Batch 450 Loss 0.6864 Accuracy 0.7661\n",
      "Epoch 108 Batch 500 Loss 0.6870 Accuracy 0.7658\n",
      "Epoch 108 Batch 550 Loss 0.6883 Accuracy 0.7652\n",
      "Epoch 108 Batch 600 Loss 0.6892 Accuracy 0.7648\n",
      "Epoch 108 Batch 650 Loss 0.6894 Accuracy 0.7648\n",
      "Epoch 108 Batch 700 Loss 0.6897 Accuracy 0.7647\n",
      "Epoch 108 Batch 750 Loss 0.6901 Accuracy 0.7647\n",
      "Epoch 108 Batch 800 Loss 0.6899 Accuracy 0.7646\n",
      "Epoch 108 Batch 850 Loss 0.6906 Accuracy 0.7645\n",
      "Epoch 108 Batch 900 Loss 0.6907 Accuracy 0.7644\n",
      "Epoch 108 Batch 950 Loss 0.6911 Accuracy 0.7642\n",
      "Epoch 108 Batch 1000 Loss 0.6913 Accuracy 0.7641\n",
      "Epoch 108 Batch 1050 Loss 0.6918 Accuracy 0.7640\n",
      "Epoch 108 Batch 1100 Loss 0.6915 Accuracy 0.7640\n",
      "Epoch 108 Batch 1150 Loss 0.6914 Accuracy 0.7640\n",
      "Epoch 108 Batch 1200 Loss 0.6913 Accuracy 0.7641\n",
      "Epoch 108 Batch 1250 Loss 0.6912 Accuracy 0.7641\n",
      "Epoch 108 Batch 1300 Loss 0.6918 Accuracy 0.7640\n",
      "Epoch 108 Batch 1350 Loss 0.6923 Accuracy 0.7639\n",
      "Epoch 108 Batch 1400 Loss 0.6930 Accuracy 0.7636\n",
      "Epoch 108 Batch 1450 Loss 0.6934 Accuracy 0.7635\n",
      "Epoch 108 Batch 1500 Loss 0.6939 Accuracy 0.7633\n",
      "Epoch 108 Loss 0.6942 Accuracy 0.7632\n",
      "Time taken for 1 epoch: 36.79553031921387 secs\n",
      "\n",
      "epoch lasted: 36.799365282058716\n",
      "Epoch 109 Batch 0 Loss 0.8175 Accuracy 0.7243\n",
      "Epoch 109 Batch 50 Loss 0.6834 Accuracy 0.7684\n",
      "Epoch 109 Batch 100 Loss 0.6882 Accuracy 0.7670\n",
      "Epoch 109 Batch 150 Loss 0.6884 Accuracy 0.7667\n",
      "Epoch 109 Batch 200 Loss 0.6892 Accuracy 0.7662\n",
      "Epoch 109 Batch 250 Loss 0.6880 Accuracy 0.7661\n",
      "Epoch 109 Batch 300 Loss 0.6884 Accuracy 0.7658\n",
      "Epoch 109 Batch 350 Loss 0.6894 Accuracy 0.7656\n",
      "Epoch 109 Batch 400 Loss 0.6904 Accuracy 0.7651\n",
      "Epoch 109 Batch 450 Loss 0.6908 Accuracy 0.7648\n",
      "discarded batch 457\n",
      "Epoch 109 Batch 500 Loss 0.6903 Accuracy 0.7650\n",
      "Epoch 109 Batch 550 Loss 0.6896 Accuracy 0.7654\n",
      "Epoch 109 Batch 600 Loss 0.6894 Accuracy 0.7654\n",
      "Epoch 109 Batch 650 Loss 0.6896 Accuracy 0.7653\n",
      "Epoch 109 Batch 700 Loss 0.6899 Accuracy 0.7651\n",
      "Epoch 109 Batch 750 Loss 0.6910 Accuracy 0.7645\n",
      "Epoch 109 Batch 800 Loss 0.6912 Accuracy 0.7645\n",
      "Epoch 109 Batch 850 Loss 0.6913 Accuracy 0.7644\n",
      "Epoch 109 Batch 900 Loss 0.6917 Accuracy 0.7643\n",
      "Epoch 109 Batch 950 Loss 0.6914 Accuracy 0.7643\n",
      "Epoch 109 Batch 1000 Loss 0.6916 Accuracy 0.7643\n",
      "Epoch 109 Batch 1050 Loss 0.6920 Accuracy 0.7641\n",
      "Epoch 109 Batch 1100 Loss 0.6923 Accuracy 0.7641\n",
      "Epoch 109 Batch 1150 Loss 0.6920 Accuracy 0.7642\n",
      "Epoch 109 Batch 1200 Loss 0.6923 Accuracy 0.7640\n",
      "Epoch 109 Batch 1250 Loss 0.6925 Accuracy 0.7639\n",
      "Epoch 109 Batch 1300 Loss 0.6929 Accuracy 0.7639\n",
      "Epoch 109 Batch 1350 Loss 0.6934 Accuracy 0.7636\n",
      "Epoch 109 Batch 1400 Loss 0.6935 Accuracy 0.7636\n",
      "Epoch 109 Batch 1450 Loss 0.6934 Accuracy 0.7637\n",
      "Epoch 109 Batch 1500 Loss 0.6931 Accuracy 0.7638\n",
      "Epoch 109 Loss 0.6935 Accuracy 0.7637\n",
      "Time taken for 1 epoch: 36.49238920211792 secs\n",
      "\n",
      "epoch lasted: 36.49728298187256\n",
      "Epoch 110 Batch 0 Loss 0.6643 Accuracy 0.7691\n",
      "Epoch 110 Batch 50 Loss 0.6792 Accuracy 0.7691\n",
      "Epoch 110 Batch 100 Loss 0.6807 Accuracy 0.7685\n",
      "Epoch 110 Batch 150 Loss 0.6849 Accuracy 0.7669\n",
      "Epoch 110 Batch 200 Loss 0.6870 Accuracy 0.7666\n",
      "Epoch 110 Batch 250 Loss 0.6895 Accuracy 0.7658\n",
      "Epoch 110 Batch 300 Loss 0.6888 Accuracy 0.7654\n",
      "Epoch 110 Batch 350 Loss 0.6876 Accuracy 0.7661\n",
      "discarded batch 398\n",
      "Epoch 110 Batch 400 Loss 0.6869 Accuracy 0.7661\n",
      "Epoch 110 Batch 450 Loss 0.6871 Accuracy 0.7661\n",
      "Epoch 110 Batch 500 Loss 0.6867 Accuracy 0.7661\n",
      "Epoch 110 Batch 550 Loss 0.6879 Accuracy 0.7656\n",
      "Epoch 110 Batch 600 Loss 0.6873 Accuracy 0.7658\n",
      "Epoch 110 Batch 650 Loss 0.6876 Accuracy 0.7658\n",
      "Epoch 110 Batch 700 Loss 0.6876 Accuracy 0.7659\n",
      "Epoch 110 Batch 750 Loss 0.6877 Accuracy 0.7657\n",
      "Epoch 110 Batch 800 Loss 0.6878 Accuracy 0.7657\n",
      "Epoch 110 Batch 850 Loss 0.6883 Accuracy 0.7655\n",
      "Epoch 110 Batch 900 Loss 0.6885 Accuracy 0.7655\n",
      "Epoch 110 Batch 950 Loss 0.6890 Accuracy 0.7655\n",
      "Epoch 110 Batch 1000 Loss 0.6897 Accuracy 0.7653\n",
      "Epoch 110 Batch 1050 Loss 0.6903 Accuracy 0.7651\n",
      "Epoch 110 Batch 1100 Loss 0.6901 Accuracy 0.7652\n",
      "Epoch 110 Batch 1150 Loss 0.6904 Accuracy 0.7651\n",
      "Epoch 110 Batch 1200 Loss 0.6907 Accuracy 0.7650\n",
      "Epoch 110 Batch 1250 Loss 0.6910 Accuracy 0.7650\n",
      "Epoch 110 Batch 1300 Loss 0.6912 Accuracy 0.7649\n",
      "Epoch 110 Batch 1350 Loss 0.6915 Accuracy 0.7649\n",
      "Epoch 110 Batch 1400 Loss 0.6914 Accuracy 0.7648\n",
      "Epoch 110 Batch 1450 Loss 0.6916 Accuracy 0.7648\n",
      "Epoch 110 Batch 1500 Loss 0.6917 Accuracy 0.7648\n",
      "Saving checkpoint for epoch 110 at ./checkpoints/train/ckpt-22\n",
      "Epoch 110 Loss 0.6921 Accuracy 0.7647\n",
      "Time taken for 1 epoch: 36.74815320968628 secs\n",
      "\n",
      "epoch lasted: 36.75245237350464\n",
      "Epoch 111 Batch 0 Loss 0.6828 Accuracy 0.7741\n",
      "Epoch 111 Batch 50 Loss 0.6810 Accuracy 0.7691\n",
      "Epoch 111 Batch 100 Loss 0.6831 Accuracy 0.7676\n",
      "Epoch 111 Batch 150 Loss 0.6820 Accuracy 0.7675\n",
      "Epoch 111 Batch 200 Loss 0.6819 Accuracy 0.7678\n",
      "Epoch 111 Batch 250 Loss 0.6824 Accuracy 0.7671\n",
      "Epoch 111 Batch 300 Loss 0.6849 Accuracy 0.7664\n",
      "Epoch 111 Batch 350 Loss 0.6860 Accuracy 0.7660\n",
      "Epoch 111 Batch 400 Loss 0.6853 Accuracy 0.7663\n",
      "Epoch 111 Batch 450 Loss 0.6869 Accuracy 0.7656\n",
      "Epoch 111 Batch 500 Loss 0.6882 Accuracy 0.7653\n",
      "Epoch 111 Batch 550 Loss 0.6891 Accuracy 0.7651\n",
      "Epoch 111 Batch 600 Loss 0.6889 Accuracy 0.7651\n",
      "Epoch 111 Batch 650 Loss 0.6882 Accuracy 0.7654\n",
      "Epoch 111 Batch 700 Loss 0.6891 Accuracy 0.7651\n",
      "Epoch 111 Batch 750 Loss 0.6891 Accuracy 0.7652\n",
      "Epoch 111 Batch 800 Loss 0.6891 Accuracy 0.7653\n",
      "Epoch 111 Batch 850 Loss 0.6894 Accuracy 0.7651\n",
      "Epoch 111 Batch 900 Loss 0.6893 Accuracy 0.7652\n",
      "Epoch 111 Batch 950 Loss 0.6891 Accuracy 0.7652\n",
      "Epoch 111 Batch 1000 Loss 0.6893 Accuracy 0.7652\n",
      "Epoch 111 Batch 1050 Loss 0.6894 Accuracy 0.7652\n",
      "discarded batch 1090\n",
      "Epoch 111 Batch 1100 Loss 0.6895 Accuracy 0.7652\n",
      "Epoch 111 Batch 1150 Loss 0.6894 Accuracy 0.7653\n",
      "Epoch 111 Batch 1200 Loss 0.6894 Accuracy 0.7653\n",
      "Epoch 111 Batch 1250 Loss 0.6895 Accuracy 0.7652\n",
      "Epoch 111 Batch 1300 Loss 0.6898 Accuracy 0.7651\n",
      "Epoch 111 Batch 1350 Loss 0.6900 Accuracy 0.7651\n",
      "Epoch 111 Batch 1400 Loss 0.6901 Accuracy 0.7650\n",
      "Epoch 111 Batch 1450 Loss 0.6905 Accuracy 0.7649\n",
      "Epoch 111 Batch 1500 Loss 0.6910 Accuracy 0.7647\n",
      "Epoch 111 Loss 0.6912 Accuracy 0.7646\n",
      "Time taken for 1 epoch: 36.975750207901 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 111 VALIDATION: Loss 0.7286 Accuracy 0.7566\n",
      "\n",
      "la | tua | be| ni| gni| tà | non | pur | soc| cor| re|\n",
      "a | chi | do| man| da | ma | mol| te | fï| a| te|\n",
      "li| be| ra| men| te al | di| man| dar | pre| cor| re|\n",
      "                                        \n",
      " co| min| ciò | co| me ’l | ma| e| stro | m’ a| scol| ta|\n",
      "che | l’ a| ni| ma | di| scen| der | lo | suo | va| ca|\n",
      "per | l’ a| ni| ma | che | si | vo’ | la | sua | via|\n",
      "                                 \n",
      " e | per | la | men| te | di| scer| na | di| ve| na|\n",
      "e | per | la | mia | men| te | di| scer| na | via|\n",
      "e | di| scen| de| si | la | mia | men| te | via|\n",
      " co|\n",
      "an| co| min| ciò | ch’ i’ | non | m’ a| van| na | par| te|\n",
      "lo | do| ve | si | mo| ver | lo | suo | m’ a| va| mi|\n",
      "lo | do| ve | si | vol| se | mi | fa| va| rea|\n",
      "                                    \n",
      " e | io | a | lui | co| me | sì | ch’ i’ | vi| va| mi|\n",
      "la | mia | mi| se| ri| ta | co| me | se| guia|\n",
      "e | po| co| min| cia’ | io | vi| di | la | mia|\n",
      " e | co|\n",
      "                 \n",
      "l’ a| ni| ma | d’ a| ma | pa| re|\n",
      "ch’ i’ | vi| di | mi | par| ver | la | mia | di| vi| na|\n",
      "per | la | mia | me| na | mia | ma| ni| fe| ra| va|\n",
      " co|\n",
      "an| na| va | di| ce | l’ al| tra | di| scer| na|\n",
      "lo | do| ve | la | mia | ch’ i’ | mi | par| ti’ | in| te| ra|\n",
      "e | co| me | si | vol| se a | me | per | la | per| na|\n",
      "                              \n",
      " e | per | la | men| te | di| scer| na | mi | sca| vi|\n",
      "che | la | mia | vi| sta | mia | men| te | sta| vi|\n",
      "ch’ io | a | la | mi| se| ri| tà | m’ a| vea | va| vi|\n",
      " co|\n",
      "e | se | l’ a| ni| ma | di | co| lor | m’ a| van| ta|\n",
      "che | la | mi| se| ria | di | co| lui | che | s’ a| va|\n",
      "la | mia | mia | mia | mi| se| ri| tà | m’ a| van| ta|\n",
      "                               \n",
      " e | io | a | lui | che | la | mia | mia | di| se| va|\n",
      "e | co| me | l’ a| ni| ma | del | mio | di| va| no|\n",
      "che | l’ a| ni| ma | di| scen| do | di | me| na| va|\n",
      " e | se|\n",
      "non | vi| di | m’ a| vea | la | mia | di| ve|\n",
      "ch’ i’ | vi| di | la | mia | me| na | mia | mi | sca| vi|\n",
      "lo | do| ve | l’ a| ni| ma | ch’ i’ | vi| di | se| na|\n",
      "                                 \n",
      " e | per | l’ a| ni| ma | de | l’ a| ni| ma | sa| vi| va|\n",
      "e | la | mi| se| ri| ta | di | mi| se | sta| va|\n",
      "che | l’ a| ni| ma | di | me | sì | com’ | i| sta| va|\n",
      " che | se | l’ a| ni| ma | di | sta| va | s’ a| scol| ta|\n",
      "co| sì | l’ a| ni| ma | ch’ i’ | vi| di | s’ ap| pa| ri|\n",
      "per | la | mia | co| lui | ch’ i’ | vi| di | s’ av| vi| va|\n",
      "                          \n",
      " e | per | la | vi| sta | do| v’ io | mi | ri| va| ri|\n",
      "e | co| me | l’ a| ni| ma | che | si | ve| nì| va|\n",
      "e | vi| d’ io | co| sì | la | mia | me| na | va| na|\n",
      " e | co|\n",
      "non | vi| di | ch’ i’ | vi| di | s’ av| ve| des| si|\n",
      "lo | do| ve | son | lo | spi| ri| to | m’ a| va| ’ a|\n",
      "per | la | men| te | ch’ io | mi | par| ve | s’ af| en| si|\n",
      "                         \n",
      " e | per | la | men| te | di| scer| na| va | sa| va|\n",
      "e | po| scia | mi | fe| ce | dis| se | l’ a| ma| sa|\n",
      "la | mia | do| vea | la | mia | don| na | ma| dra|\n",
      " ch’ i’ | mi | ve| dea | la | mia | con | l’ a| sco| sto| scia|\n",
      "ch’ i’ | vi| di | dis| si | di | ma| ra| vi | sco| scia|\n",
      "e | co| me | l’ a| ni| ma | del | ma| e| stro| scia|\n",
      "                           \n",
      " e | per | l’ a| ni| ma | di | ma| ra| vi| glia| vi|\n",
      "e | per | la | mia | don| na | mia | men| te | sa| lia|\n",
      "e | co| me | di| scer| na | mi | dis| se | va| vi|\n",
      " co|\n",
      "an| co| min| cia’ | io | ve| dea | la | spe| da|\n",
      "che | l’ a| ma| re a | me | con | li | spi| ri| ti| ca|\n",
      "l’ a| ni| ma | ch’ io | ve| di | la | vi| sta | va| ni|\n",
      "                                   \n",
      " e | per | l’ a| ni| me | di | ma| ra| vi| ca| vi| ca|\n",
      "che | la | mia | vi| sta | mia | me| sti| ma | va| ni|\n",
      "ch’ io | mi | par| ve | la | mia | men| te | vi| ci| ca|\n",
      " co|\n",
      "man| tra| va | l’ a| ni| ma | d’ a| ma| ra| vi| glia|\n",
      "che | l’ a| ni| ma | do| v’ io | a | la | mia | par| ti|\n",
      "lo | dol| ce | mio | di| cea | mi | di| scen| de|\n",
      "                                   \n",
      " e | io | a | lui | da | me | dis| se | mi | sca| vi|\n",
      "e | a | la | mi| se| ria | don| na | mi | fa| vi|\n",
      "ch’ i’ | mi | di| se| gna | mi | di| se| gna | va| vi|\n",
      "\n",
      " e | io | veg| gio | di | me | d’ a| mor | m’ a| van| ti|\n",
      "co| me | l’ a| ni| ma | di | mi| se| ri | s’ ap| pa| ri|\n",
      "l’ a| ni| ma | che | l’ a| ni| ma | di | ma| ra| va|\n",
      "                                \n",
      " e | per | la | men| te | di | ma| ra| vi| glia| ri|\n",
      "che | l’ a| ni| ma | di | ma| ra| vi| glia| va| va|\n",
      "per | l’ a| ni| ma | di| scen| do | ch’ io | a| va| ri|\n",
      " co|\n",
      "an| tra| di| nan| do | d’ a| mor | di| scen| do| len| do|\n",
      "l’ a| ni| ma | di | ch’ i’ | mi | di| sco| pa | sco| sce|\n",
      "la | mi| se| ri| tà | ch’ i’ | vo’ | la | sua | pia| ce|\n",
      "                       \n",
      " e | per | la | men| te | di | ma| ra| vi| glia| va|\n",
      "e | po| scia | mi | fe| ce | dis| se | l’ a| ma| ce|\n",
      "la | mia | do| vea | per | lo | suo | mi| ra| va| va|\n",
      " co|\n",
      "sì | ch’ i’ | vi| di | mi | vol| si | si | sco| pe|\n",
      "lo | do| ve | so| lo a | l’ al| tro | di| si| ri| so|\n",
      "e | per | lo | sco| glio | di| scen| do | m’ a| ve| ta|\n",
      "                                  \n",
      " e | per | l’ a| ni| me | del | ma| e| stro | mi| so| so|\n",
      "che | l’ a| ni| ma | de | l’ a| ni| ma | se| gua| ta|\n",
      "e | se | l’ a| ni| ma | di | ma| ni| fe| sta| va|\n",
      " co|\n",
      "mi | ve| nìa | per | l’ a| ni| ma | di| man| da|\n",
      "per | l’ a| ni| ma | di| ma | di| man| dar | l’ a| ia|\n",
      "ch’ io | mi | vi| di | ma| ra| vi | l’ a| ni| ma| na|\n",
      "                                      \n",
      " e | per | l’ a| ni| ma | de | l’ a| ni| ma | sa| va|\n",
      "e | per | l’ a| ni| ma | do| ve | la | mia | vi| va|\n",
      "e | io | a | l’ a| ni| ma | che | si | va| nì|\n",
      " e | co|\n",
      "an| co| min| ciai | co| min| cia’ | in| to| sta|\n",
      "la | mia | men| te | che | l’ a| ni| ma | di| si| ra|\n",
      "e | s’ io | vi| di | mi | vol| si a | la | ma| ni| ca|\n",
      "                                  \n",
      " e | poi | co’ | io | a | lui | con| ten| di | si| ra|\n",
      "e | co| min| ciò | co| min| cia’ | io | a| van| ta|\n",
      "l’ a| ni| ma | di| sco| la | con | l’ a| ni| ma| na|\n",
      "\n",
      "epoch lasted: 768.4072420597076\n",
      "Epoch 112 Batch 0 Loss 0.7081 Accuracy 0.7674\n",
      "Epoch 112 Batch 50 Loss 0.6888 Accuracy 0.7649\n",
      "Epoch 112 Batch 100 Loss 0.6904 Accuracy 0.7645\n",
      "Epoch 112 Batch 150 Loss 0.6870 Accuracy 0.7658\n",
      "Epoch 112 Batch 200 Loss 0.6842 Accuracy 0.7666\n",
      "Epoch 112 Batch 250 Loss 0.6855 Accuracy 0.7663\n",
      "Epoch 112 Batch 300 Loss 0.6856 Accuracy 0.7664\n",
      "Epoch 112 Batch 350 Loss 0.6843 Accuracy 0.7669\n",
      "Epoch 112 Batch 400 Loss 0.6849 Accuracy 0.7669\n",
      "Epoch 112 Batch 450 Loss 0.6860 Accuracy 0.7667\n",
      "Epoch 112 Batch 500 Loss 0.6859 Accuracy 0.7668\n",
      "Epoch 112 Batch 550 Loss 0.6856 Accuracy 0.7670\n",
      "discarded batch 568\n",
      "Epoch 112 Batch 600 Loss 0.6860 Accuracy 0.7667\n",
      "Epoch 112 Batch 650 Loss 0.6865 Accuracy 0.7663\n",
      "Epoch 112 Batch 700 Loss 0.6861 Accuracy 0.7665\n",
      "Epoch 112 Batch 750 Loss 0.6866 Accuracy 0.7663\n",
      "Epoch 112 Batch 800 Loss 0.6872 Accuracy 0.7661\n",
      "Epoch 112 Batch 850 Loss 0.6871 Accuracy 0.7661\n",
      "Epoch 112 Batch 900 Loss 0.6875 Accuracy 0.7659\n",
      "Epoch 112 Batch 950 Loss 0.6878 Accuracy 0.7659\n",
      "Epoch 112 Batch 1000 Loss 0.6875 Accuracy 0.7660\n",
      "Epoch 112 Batch 1050 Loss 0.6881 Accuracy 0.7658\n",
      "Epoch 112 Batch 1100 Loss 0.6889 Accuracy 0.7655\n",
      "Epoch 112 Batch 1150 Loss 0.6889 Accuracy 0.7655\n",
      "Epoch 112 Batch 1200 Loss 0.6887 Accuracy 0.7655\n",
      "Epoch 112 Batch 1250 Loss 0.6887 Accuracy 0.7655\n",
      "Epoch 112 Batch 1300 Loss 0.6890 Accuracy 0.7654\n",
      "Epoch 112 Batch 1350 Loss 0.6894 Accuracy 0.7653\n",
      "Epoch 112 Batch 1400 Loss 0.6894 Accuracy 0.7653\n",
      "Epoch 112 Batch 1450 Loss 0.6897 Accuracy 0.7651\n",
      "Epoch 112 Batch 1500 Loss 0.6902 Accuracy 0.7649\n",
      "Epoch 112 Loss 0.6903 Accuracy 0.7649\n",
      "Time taken for 1 epoch: 37.82685208320618 secs\n",
      "\n",
      "epoch lasted: 37.83104991912842\n",
      "Epoch 113 Batch 0 Loss 0.6460 Accuracy 0.7791\n",
      "Epoch 113 Batch 50 Loss 0.6718 Accuracy 0.7712\n",
      "Epoch 113 Batch 100 Loss 0.6799 Accuracy 0.7689\n",
      "Epoch 113 Batch 150 Loss 0.6785 Accuracy 0.7693\n",
      "Epoch 113 Batch 200 Loss 0.6803 Accuracy 0.7685\n",
      "Epoch 113 Batch 250 Loss 0.6807 Accuracy 0.7684\n",
      "Epoch 113 Batch 300 Loss 0.6807 Accuracy 0.7686\n",
      "Epoch 113 Batch 350 Loss 0.6812 Accuracy 0.7684\n",
      "Epoch 113 Batch 400 Loss 0.6828 Accuracy 0.7677\n",
      "Epoch 113 Batch 450 Loss 0.6817 Accuracy 0.7679\n",
      "Epoch 113 Batch 500 Loss 0.6821 Accuracy 0.7678\n",
      "Epoch 113 Batch 550 Loss 0.6833 Accuracy 0.7674\n",
      "Epoch 113 Batch 600 Loss 0.6842 Accuracy 0.7671\n",
      "Epoch 113 Batch 650 Loss 0.6845 Accuracy 0.7671\n",
      "Epoch 113 Batch 700 Loss 0.6852 Accuracy 0.7669\n",
      "Epoch 113 Batch 750 Loss 0.6861 Accuracy 0.7668\n",
      "Epoch 113 Batch 800 Loss 0.6862 Accuracy 0.7666\n",
      "Epoch 113 Batch 850 Loss 0.6866 Accuracy 0.7665\n",
      "Epoch 113 Batch 900 Loss 0.6869 Accuracy 0.7664\n",
      "Epoch 113 Batch 950 Loss 0.6870 Accuracy 0.7663\n",
      "Epoch 113 Batch 1000 Loss 0.6873 Accuracy 0.7662\n",
      "Epoch 113 Batch 1050 Loss 0.6872 Accuracy 0.7662\n",
      "Epoch 113 Batch 1100 Loss 0.6869 Accuracy 0.7662\n",
      "Epoch 113 Batch 1150 Loss 0.6871 Accuracy 0.7661\n",
      "discarded batch 1165\n",
      "Epoch 113 Batch 1200 Loss 0.6876 Accuracy 0.7660\n",
      "Epoch 113 Batch 1250 Loss 0.6880 Accuracy 0.7659\n",
      "Epoch 113 Batch 1300 Loss 0.6885 Accuracy 0.7658\n",
      "Epoch 113 Batch 1350 Loss 0.6892 Accuracy 0.7656\n",
      "Epoch 113 Batch 1400 Loss 0.6892 Accuracy 0.7656\n",
      "Epoch 113 Batch 1450 Loss 0.6895 Accuracy 0.7655\n",
      "Epoch 113 Batch 1500 Loss 0.6899 Accuracy 0.7654\n",
      "Epoch 113 Loss 0.6899 Accuracy 0.7654\n",
      "Time taken for 1 epoch: 37.10388517379761 secs\n",
      "\n",
      "epoch lasted: 37.10731482505798\n",
      "Epoch 114 Batch 0 Loss 0.6721 Accuracy 0.7691\n",
      "Epoch 114 Batch 50 Loss 0.6824 Accuracy 0.7692\n",
      "Epoch 114 Batch 100 Loss 0.6851 Accuracy 0.7682\n",
      "Epoch 114 Batch 150 Loss 0.6827 Accuracy 0.7683\n",
      "Epoch 114 Batch 200 Loss 0.6837 Accuracy 0.7679\n",
      "Epoch 114 Batch 250 Loss 0.6837 Accuracy 0.7677\n",
      "Epoch 114 Batch 300 Loss 0.6831 Accuracy 0.7678\n",
      "Epoch 114 Batch 350 Loss 0.6841 Accuracy 0.7673\n",
      "Epoch 114 Batch 400 Loss 0.6841 Accuracy 0.7670\n",
      "Epoch 114 Batch 450 Loss 0.6840 Accuracy 0.7672\n",
      "Epoch 114 Batch 500 Loss 0.6837 Accuracy 0.7672\n",
      "Epoch 114 Batch 550 Loss 0.6837 Accuracy 0.7671\n",
      "Epoch 114 Batch 600 Loss 0.6840 Accuracy 0.7667\n",
      "Epoch 114 Batch 650 Loss 0.6835 Accuracy 0.7669\n",
      "Epoch 114 Batch 700 Loss 0.6843 Accuracy 0.7666\n",
      "Epoch 114 Batch 750 Loss 0.6851 Accuracy 0.7664\n",
      "discarded batch 776\n",
      "Epoch 114 Batch 800 Loss 0.6860 Accuracy 0.7662\n",
      "Epoch 114 Batch 850 Loss 0.6861 Accuracy 0.7663\n",
      "Epoch 114 Batch 900 Loss 0.6864 Accuracy 0.7662\n",
      "Epoch 114 Batch 950 Loss 0.6866 Accuracy 0.7662\n",
      "Epoch 114 Batch 1000 Loss 0.6868 Accuracy 0.7661\n",
      "Epoch 114 Batch 1050 Loss 0.6868 Accuracy 0.7662\n",
      "Epoch 114 Batch 1100 Loss 0.6874 Accuracy 0.7659\n",
      "Epoch 114 Batch 1150 Loss 0.6879 Accuracy 0.7657\n",
      "Epoch 114 Batch 1200 Loss 0.6885 Accuracy 0.7655\n",
      "Epoch 114 Batch 1250 Loss 0.6882 Accuracy 0.7656\n",
      "Epoch 114 Batch 1300 Loss 0.6886 Accuracy 0.7654\n",
      "Epoch 114 Batch 1350 Loss 0.6883 Accuracy 0.7655\n",
      "Epoch 114 Batch 1400 Loss 0.6881 Accuracy 0.7655\n",
      "Epoch 114 Batch 1450 Loss 0.6880 Accuracy 0.7656\n",
      "Epoch 114 Batch 1500 Loss 0.6881 Accuracy 0.7656\n",
      "Epoch 114 Loss 0.6882 Accuracy 0.7656\n",
      "Time taken for 1 epoch: 37.659674882888794 secs\n",
      "\n",
      "epoch lasted: 37.664456605911255\n",
      "Epoch 115 Batch 0 Loss 0.7183 Accuracy 0.7276\n",
      "Epoch 115 Batch 50 Loss 0.6779 Accuracy 0.7697\n",
      "Epoch 115 Batch 100 Loss 0.6794 Accuracy 0.7687\n",
      "Epoch 115 Batch 150 Loss 0.6820 Accuracy 0.7671\n",
      "Epoch 115 Batch 200 Loss 0.6823 Accuracy 0.7669\n",
      "Epoch 115 Batch 250 Loss 0.6826 Accuracy 0.7671\n",
      "Epoch 115 Batch 300 Loss 0.6841 Accuracy 0.7662\n",
      "Epoch 115 Batch 350 Loss 0.6828 Accuracy 0.7668\n",
      "Epoch 115 Batch 400 Loss 0.6821 Accuracy 0.7669\n",
      "Epoch 115 Batch 450 Loss 0.6822 Accuracy 0.7672\n",
      "Epoch 115 Batch 500 Loss 0.6819 Accuracy 0.7674\n",
      "Epoch 115 Batch 550 Loss 0.6814 Accuracy 0.7677\n",
      "Epoch 115 Batch 600 Loss 0.6828 Accuracy 0.7673\n",
      "Epoch 115 Batch 650 Loss 0.6831 Accuracy 0.7673\n",
      "Epoch 115 Batch 700 Loss 0.6833 Accuracy 0.7671\n",
      "Epoch 115 Batch 750 Loss 0.6835 Accuracy 0.7671\n",
      "Epoch 115 Batch 800 Loss 0.6842 Accuracy 0.7669\n",
      "Epoch 115 Batch 850 Loss 0.6848 Accuracy 0.7668\n",
      "Epoch 115 Batch 900 Loss 0.6847 Accuracy 0.7668\n",
      "Epoch 115 Batch 950 Loss 0.6851 Accuracy 0.7667\n",
      "discarded batch 996\n",
      "Epoch 115 Batch 1000 Loss 0.6855 Accuracy 0.7665\n",
      "Epoch 115 Batch 1050 Loss 0.6856 Accuracy 0.7665\n",
      "Epoch 115 Batch 1100 Loss 0.6861 Accuracy 0.7663\n",
      "Epoch 115 Batch 1150 Loss 0.6863 Accuracy 0.7663\n",
      "Epoch 115 Batch 1200 Loss 0.6867 Accuracy 0.7661\n",
      "Epoch 115 Batch 1250 Loss 0.6872 Accuracy 0.7660\n",
      "Epoch 115 Batch 1300 Loss 0.6881 Accuracy 0.7657\n",
      "Epoch 115 Batch 1350 Loss 0.6885 Accuracy 0.7655\n",
      "Epoch 115 Batch 1400 Loss 0.6883 Accuracy 0.7655\n",
      "Epoch 115 Batch 1450 Loss 0.6878 Accuracy 0.7657\n",
      "Epoch 115 Batch 1500 Loss 0.6881 Accuracy 0.7657\n",
      "Saving checkpoint for epoch 115 at ./checkpoints/train/ckpt-23\n",
      "Epoch 115 Loss 0.6881 Accuracy 0.7655\n",
      "Time taken for 1 epoch: 37.58779835700989 secs\n",
      "\n",
      "epoch lasted: 37.592347145080566\n",
      "Epoch 116 Batch 0 Loss 0.6775 Accuracy 0.7674\n",
      "Epoch 116 Batch 50 Loss 0.6820 Accuracy 0.7684\n",
      "Epoch 116 Batch 100 Loss 0.6789 Accuracy 0.7691\n",
      "Epoch 116 Batch 150 Loss 0.6828 Accuracy 0.7676\n",
      "Epoch 116 Batch 200 Loss 0.6831 Accuracy 0.7676\n",
      "Epoch 116 Batch 250 Loss 0.6837 Accuracy 0.7676\n",
      "Epoch 116 Batch 300 Loss 0.6830 Accuracy 0.7677\n",
      "Epoch 116 Batch 350 Loss 0.6827 Accuracy 0.7677\n",
      "Epoch 116 Batch 400 Loss 0.6808 Accuracy 0.7679\n",
      "Epoch 116 Batch 450 Loss 0.6821 Accuracy 0.7674\n",
      "Epoch 116 Batch 500 Loss 0.6817 Accuracy 0.7677\n",
      "Epoch 116 Batch 550 Loss 0.6815 Accuracy 0.7678\n",
      "Epoch 116 Batch 600 Loss 0.6815 Accuracy 0.7678\n",
      "Epoch 116 Batch 650 Loss 0.6827 Accuracy 0.7676\n",
      "discarded batch 694\n",
      "Epoch 116 Batch 700 Loss 0.6831 Accuracy 0.7674\n",
      "Epoch 116 Batch 750 Loss 0.6836 Accuracy 0.7672\n",
      "Epoch 116 Batch 800 Loss 0.6839 Accuracy 0.7672\n",
      "Epoch 116 Batch 850 Loss 0.6846 Accuracy 0.7671\n",
      "Epoch 116 Batch 900 Loss 0.6845 Accuracy 0.7670\n",
      "Epoch 116 Batch 950 Loss 0.6854 Accuracy 0.7667\n",
      "Epoch 116 Batch 1000 Loss 0.6855 Accuracy 0.7667\n",
      "Epoch 116 Batch 1050 Loss 0.6861 Accuracy 0.7665\n",
      "Epoch 116 Batch 1100 Loss 0.6856 Accuracy 0.7667\n",
      "Epoch 116 Batch 1150 Loss 0.6857 Accuracy 0.7666\n",
      "Epoch 116 Batch 1200 Loss 0.6859 Accuracy 0.7666\n",
      "Epoch 116 Batch 1250 Loss 0.6861 Accuracy 0.7665\n",
      "Epoch 116 Batch 1300 Loss 0.6859 Accuracy 0.7665\n",
      "Epoch 116 Batch 1350 Loss 0.6862 Accuracy 0.7665\n",
      "Epoch 116 Batch 1400 Loss 0.6861 Accuracy 0.7665\n",
      "Epoch 116 Batch 1450 Loss 0.6865 Accuracy 0.7664\n",
      "Epoch 116 Batch 1500 Loss 0.6865 Accuracy 0.7664\n",
      "Epoch 116 Loss 0.6867 Accuracy 0.7663\n",
      "Time taken for 1 epoch: 37.578551054000854 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 116 VALIDATION: Loss 0.7342 Accuracy 0.7567\n",
      "\n",
      "epoch lasted: 37.72854042053223\n",
      "Epoch 117 Batch 0 Loss 0.6957 Accuracy 0.7708\n",
      "Epoch 117 Batch 50 Loss 0.6721 Accuracy 0.7721\n",
      "Epoch 117 Batch 100 Loss 0.6729 Accuracy 0.7709\n",
      "Epoch 117 Batch 150 Loss 0.6757 Accuracy 0.7699\n",
      "Epoch 117 Batch 200 Loss 0.6766 Accuracy 0.7698\n",
      "Epoch 117 Batch 250 Loss 0.6786 Accuracy 0.7691\n",
      "Epoch 117 Batch 300 Loss 0.6781 Accuracy 0.7693\n",
      "Epoch 117 Batch 350 Loss 0.6780 Accuracy 0.7694\n",
      "Epoch 117 Batch 400 Loss 0.6785 Accuracy 0.7690\n",
      "Epoch 117 Batch 450 Loss 0.6792 Accuracy 0.7685\n",
      "Epoch 117 Batch 500 Loss 0.6802 Accuracy 0.7681\n",
      "Epoch 117 Batch 550 Loss 0.6806 Accuracy 0.7678\n",
      "discarded batch 578\n",
      "Epoch 117 Batch 600 Loss 0.6820 Accuracy 0.7675\n",
      "Epoch 117 Batch 650 Loss 0.6816 Accuracy 0.7675\n",
      "Epoch 117 Batch 700 Loss 0.6819 Accuracy 0.7676\n",
      "Epoch 117 Batch 750 Loss 0.6825 Accuracy 0.7675\n",
      "Epoch 117 Batch 800 Loss 0.6827 Accuracy 0.7674\n",
      "Epoch 117 Batch 850 Loss 0.6832 Accuracy 0.7672\n",
      "Epoch 117 Batch 900 Loss 0.6836 Accuracy 0.7671\n",
      "Epoch 117 Batch 950 Loss 0.6837 Accuracy 0.7670\n",
      "Epoch 117 Batch 1000 Loss 0.6837 Accuracy 0.7670\n",
      "Epoch 117 Batch 1050 Loss 0.6841 Accuracy 0.7668\n",
      "Epoch 117 Batch 1100 Loss 0.6843 Accuracy 0.7668\n",
      "Epoch 117 Batch 1150 Loss 0.6844 Accuracy 0.7667\n",
      "Epoch 117 Batch 1200 Loss 0.6850 Accuracy 0.7665\n",
      "Epoch 117 Batch 1250 Loss 0.6850 Accuracy 0.7666\n",
      "Epoch 117 Batch 1300 Loss 0.6848 Accuracy 0.7666\n",
      "Epoch 117 Batch 1350 Loss 0.6854 Accuracy 0.7664\n",
      "Epoch 117 Batch 1400 Loss 0.6857 Accuracy 0.7664\n",
      "Epoch 117 Batch 1450 Loss 0.6859 Accuracy 0.7663\n",
      "Epoch 117 Batch 1500 Loss 0.6861 Accuracy 0.7662\n",
      "Epoch 117 Loss 0.6860 Accuracy 0.7663\n",
      "Time taken for 1 epoch: 38.07930088043213 secs\n",
      "\n",
      "epoch lasted: 38.08297514915466\n",
      "Epoch 118 Batch 0 Loss 0.6901 Accuracy 0.7724\n",
      "Epoch 118 Batch 50 Loss 0.6701 Accuracy 0.7729\n",
      "Epoch 118 Batch 100 Loss 0.6706 Accuracy 0.7720\n",
      "Epoch 118 Batch 150 Loss 0.6753 Accuracy 0.7705\n",
      "Epoch 118 Batch 200 Loss 0.6761 Accuracy 0.7704\n",
      "Epoch 118 Batch 250 Loss 0.6781 Accuracy 0.7695\n",
      "discarded batch 278\n",
      "Epoch 118 Batch 300 Loss 0.6796 Accuracy 0.7692\n",
      "Epoch 118 Batch 350 Loss 0.6813 Accuracy 0.7683\n",
      "Epoch 118 Batch 400 Loss 0.6808 Accuracy 0.7684\n",
      "Epoch 118 Batch 450 Loss 0.6813 Accuracy 0.7682\n",
      "Epoch 118 Batch 500 Loss 0.6817 Accuracy 0.7680\n",
      "Epoch 118 Batch 550 Loss 0.6809 Accuracy 0.7682\n",
      "Epoch 118 Batch 600 Loss 0.6819 Accuracy 0.7681\n",
      "Epoch 118 Batch 650 Loss 0.6829 Accuracy 0.7677\n",
      "Epoch 118 Batch 700 Loss 0.6835 Accuracy 0.7676\n",
      "Epoch 118 Batch 750 Loss 0.6846 Accuracy 0.7673\n",
      "Epoch 118 Batch 800 Loss 0.6846 Accuracy 0.7673\n",
      "Epoch 118 Batch 850 Loss 0.6852 Accuracy 0.7671\n",
      "Epoch 118 Batch 900 Loss 0.6852 Accuracy 0.7670\n",
      "Epoch 118 Batch 950 Loss 0.6853 Accuracy 0.7670\n",
      "Epoch 118 Batch 1000 Loss 0.6853 Accuracy 0.7670\n",
      "Epoch 118 Batch 1050 Loss 0.6853 Accuracy 0.7669\n",
      "Epoch 118 Batch 1100 Loss 0.6849 Accuracy 0.7669\n",
      "Epoch 118 Batch 1150 Loss 0.6847 Accuracy 0.7670\n",
      "Epoch 118 Batch 1200 Loss 0.6844 Accuracy 0.7670\n",
      "Epoch 118 Batch 1250 Loss 0.6849 Accuracy 0.7668\n",
      "Epoch 118 Batch 1300 Loss 0.6852 Accuracy 0.7668\n",
      "Epoch 118 Batch 1350 Loss 0.6850 Accuracy 0.7668\n",
      "Epoch 118 Batch 1400 Loss 0.6847 Accuracy 0.7669\n",
      "Epoch 118 Batch 1450 Loss 0.6850 Accuracy 0.7669\n",
      "Epoch 118 Batch 1500 Loss 0.6851 Accuracy 0.7668\n",
      "Epoch 118 Loss 0.6850 Accuracy 0.7668\n",
      "Time taken for 1 epoch: 37.369802713394165 secs\n",
      "\n",
      "epoch lasted: 37.37490224838257\n",
      "Epoch 119 Batch 0 Loss 0.6918 Accuracy 0.7575\n",
      "Epoch 119 Batch 50 Loss 0.6724 Accuracy 0.7699\n",
      "Epoch 119 Batch 100 Loss 0.6709 Accuracy 0.7714\n",
      "Epoch 119 Batch 150 Loss 0.6748 Accuracy 0.7703\n",
      "Epoch 119 Batch 200 Loss 0.6756 Accuracy 0.7699\n",
      "Epoch 119 Batch 250 Loss 0.6776 Accuracy 0.7691\n",
      "Epoch 119 Batch 300 Loss 0.6779 Accuracy 0.7685\n",
      "Epoch 119 Batch 350 Loss 0.6788 Accuracy 0.7686\n",
      "Epoch 119 Batch 400 Loss 0.6790 Accuracy 0.7684\n",
      "Epoch 119 Batch 450 Loss 0.6797 Accuracy 0.7684\n",
      "Epoch 119 Batch 500 Loss 0.6799 Accuracy 0.7685\n",
      "Epoch 119 Batch 550 Loss 0.6808 Accuracy 0.7679\n",
      "Epoch 119 Batch 600 Loss 0.6814 Accuracy 0.7677\n",
      "Epoch 119 Batch 650 Loss 0.6816 Accuracy 0.7677\n",
      "Epoch 119 Batch 700 Loss 0.6822 Accuracy 0.7676\n",
      "Epoch 119 Batch 750 Loss 0.6828 Accuracy 0.7675\n",
      "Epoch 119 Batch 800 Loss 0.6825 Accuracy 0.7676\n",
      "Epoch 119 Batch 850 Loss 0.6829 Accuracy 0.7673\n",
      "Epoch 119 Batch 900 Loss 0.6835 Accuracy 0.7672\n",
      "Epoch 119 Batch 950 Loss 0.6840 Accuracy 0.7671\n",
      "Epoch 119 Batch 1000 Loss 0.6836 Accuracy 0.7672\n",
      "Epoch 119 Batch 1050 Loss 0.6837 Accuracy 0.7671\n",
      "Epoch 119 Batch 1100 Loss 0.6832 Accuracy 0.7672\n",
      "Epoch 119 Batch 1150 Loss 0.6831 Accuracy 0.7673\n",
      "Epoch 119 Batch 1200 Loss 0.6828 Accuracy 0.7674\n",
      "discarded batch 1202\n",
      "Epoch 119 Batch 1250 Loss 0.6828 Accuracy 0.7673\n",
      "Epoch 119 Batch 1300 Loss 0.6826 Accuracy 0.7674\n",
      "Epoch 119 Batch 1350 Loss 0.6830 Accuracy 0.7672\n",
      "Epoch 119 Batch 1400 Loss 0.6833 Accuracy 0.7671\n",
      "Epoch 119 Batch 1450 Loss 0.6836 Accuracy 0.7671\n",
      "Epoch 119 Batch 1500 Loss 0.6837 Accuracy 0.7670\n",
      "Epoch 119 Loss 0.6838 Accuracy 0.7669\n",
      "Time taken for 1 epoch: 37.154173851013184 secs\n",
      "\n",
      "epoch lasted: 37.158616065979004\n",
      "Epoch 120 Batch 0 Loss 0.6462 Accuracy 0.7724\n",
      "Epoch 120 Batch 50 Loss 0.6762 Accuracy 0.7711\n",
      "Epoch 120 Batch 100 Loss 0.6738 Accuracy 0.7704\n",
      "Epoch 120 Batch 150 Loss 0.6771 Accuracy 0.7693\n",
      "Epoch 120 Batch 200 Loss 0.6768 Accuracy 0.7692\n",
      "Epoch 120 Batch 250 Loss 0.6784 Accuracy 0.7684\n",
      "Epoch 120 Batch 300 Loss 0.6802 Accuracy 0.7677\n",
      "Epoch 120 Batch 350 Loss 0.6781 Accuracy 0.7686\n",
      "discarded batch 393\n",
      "Epoch 120 Batch 400 Loss 0.6782 Accuracy 0.7687\n",
      "Epoch 120 Batch 450 Loss 0.6793 Accuracy 0.7684\n",
      "Epoch 120 Batch 500 Loss 0.6801 Accuracy 0.7681\n",
      "Epoch 120 Batch 550 Loss 0.6797 Accuracy 0.7683\n",
      "Epoch 120 Batch 600 Loss 0.6787 Accuracy 0.7688\n",
      "Epoch 120 Batch 650 Loss 0.6791 Accuracy 0.7688\n",
      "Epoch 120 Batch 700 Loss 0.6791 Accuracy 0.7688\n",
      "Epoch 120 Batch 750 Loss 0.6798 Accuracy 0.7686\n",
      "Epoch 120 Batch 800 Loss 0.6798 Accuracy 0.7685\n",
      "Epoch 120 Batch 850 Loss 0.6795 Accuracy 0.7685\n",
      "Epoch 120 Batch 900 Loss 0.6795 Accuracy 0.7685\n",
      "Epoch 120 Batch 950 Loss 0.6801 Accuracy 0.7682\n",
      "Epoch 120 Batch 1000 Loss 0.6807 Accuracy 0.7679\n",
      "Epoch 120 Batch 1050 Loss 0.6812 Accuracy 0.7676\n",
      "Epoch 120 Batch 1100 Loss 0.6817 Accuracy 0.7674\n",
      "Epoch 120 Batch 1150 Loss 0.6820 Accuracy 0.7673\n",
      "Epoch 120 Batch 1200 Loss 0.6823 Accuracy 0.7671\n",
      "Epoch 120 Batch 1250 Loss 0.6825 Accuracy 0.7671\n",
      "Epoch 120 Batch 1300 Loss 0.6827 Accuracy 0.7671\n",
      "Epoch 120 Batch 1350 Loss 0.6831 Accuracy 0.7671\n",
      "Epoch 120 Batch 1400 Loss 0.6829 Accuracy 0.7671\n",
      "Epoch 120 Batch 1450 Loss 0.6830 Accuracy 0.7671\n",
      "Epoch 120 Batch 1500 Loss 0.6832 Accuracy 0.7672\n",
      "Saving checkpoint for epoch 120 at ./checkpoints/train/ckpt-24\n",
      "Epoch 120 Loss 0.6835 Accuracy 0.7671\n",
      "Time taken for 1 epoch: 38.06857490539551 secs\n",
      "\n",
      "epoch lasted: 38.073506116867065\n",
      "Epoch 121 Batch 0 Loss 0.7132 Accuracy 0.7575\n",
      "Epoch 121 Batch 50 Loss 0.6789 Accuracy 0.7687\n",
      "Epoch 121 Batch 100 Loss 0.6730 Accuracy 0.7695\n",
      "Epoch 121 Batch 150 Loss 0.6756 Accuracy 0.7690\n",
      "Epoch 121 Batch 200 Loss 0.6726 Accuracy 0.7698\n",
      "Epoch 121 Batch 250 Loss 0.6741 Accuracy 0.7699\n",
      "discarded batch 256\n",
      "Epoch 121 Batch 300 Loss 0.6757 Accuracy 0.7697\n",
      "Epoch 121 Batch 350 Loss 0.6756 Accuracy 0.7697\n",
      "Epoch 121 Batch 400 Loss 0.6763 Accuracy 0.7689\n",
      "Epoch 121 Batch 450 Loss 0.6772 Accuracy 0.7686\n",
      "Epoch 121 Batch 500 Loss 0.6770 Accuracy 0.7687\n",
      "Epoch 121 Batch 550 Loss 0.6772 Accuracy 0.7684\n",
      "Epoch 121 Batch 600 Loss 0.6781 Accuracy 0.7681\n",
      "Epoch 121 Batch 650 Loss 0.6787 Accuracy 0.7681\n",
      "Epoch 121 Batch 700 Loss 0.6793 Accuracy 0.7678\n",
      "Epoch 121 Batch 750 Loss 0.6797 Accuracy 0.7677\n",
      "Epoch 121 Batch 800 Loss 0.6800 Accuracy 0.7676\n",
      "Epoch 121 Batch 850 Loss 0.6806 Accuracy 0.7675\n",
      "Epoch 121 Batch 900 Loss 0.6800 Accuracy 0.7678\n",
      "Epoch 121 Batch 950 Loss 0.6805 Accuracy 0.7676\n",
      "Epoch 121 Batch 1000 Loss 0.6804 Accuracy 0.7676\n",
      "Epoch 121 Batch 1050 Loss 0.6810 Accuracy 0.7674\n",
      "Epoch 121 Batch 1100 Loss 0.6816 Accuracy 0.7673\n",
      "Epoch 121 Batch 1150 Loss 0.6819 Accuracy 0.7673\n",
      "Epoch 121 Batch 1200 Loss 0.6820 Accuracy 0.7673\n",
      "Epoch 121 Batch 1250 Loss 0.6820 Accuracy 0.7672\n",
      "Epoch 121 Batch 1300 Loss 0.6823 Accuracy 0.7672\n",
      "Epoch 121 Batch 1350 Loss 0.6824 Accuracy 0.7673\n",
      "Epoch 121 Batch 1400 Loss 0.6825 Accuracy 0.7672\n",
      "Epoch 121 Batch 1450 Loss 0.6829 Accuracy 0.7670\n",
      "Epoch 121 Batch 1500 Loss 0.6826 Accuracy 0.7671\n",
      "Epoch 121 Loss 0.6828 Accuracy 0.7671\n",
      "Time taken for 1 epoch: 38.053913831710815 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 121 VALIDATION: Loss 0.7330 Accuracy 0.7580\n",
      "\n",
      "la | tua | be| ni| gni| tà | non | pur | soc| cor| re|\n",
      "a | chi | do| man| da | ma | mol| te | fï| a| te|\n",
      "li| be| ra| men| te al | di| man| dar | pre| cor| re|\n",
      "                                        \n",
      " ch’ i’ | vi| di | dis| se | ch’ io | mi | ri| spuo| se|\n",
      "co| me | con | li oc| chi | del | cam| min | ch’ a| scor| ri|\n",
      "co| me | co| me | co| sì | com’ | io | di| sce| si|\n",
      "                              \n",
      " co| me | si | pre| ga| va | l’ al| tro | si | scor| ri|\n",
      "che | l’ a| ni| ma | del | ciel | che | si | ri| spo| sta|\n",
      "co| sì | co| me | co| sì | com’ | io | di| scor| ri|\n",
      " che | l’ a| ni| mai | che | l’ al| tro | di| scen| di|\n",
      "co| sì | com’ | io | ch’ a | l’ a| vea | la | sco| sco| glio|\n",
      "co| me | co| me | co| sì | com’ | io | son | pie| ga|\n",
      "                             \n",
      " co| sì | l’ a| ni| ma | che | sì | com’ | io | vi| di|\n",
      "co| me | co| sì | com’ | io | che | l’ a| ni| mai|\n",
      "co| me | si | fa| val| lan| do | co| sì | co| stai|\n",
      " che | l’ a| ni| man| dan| do | co| lor | ch’ i’ | veg| gio|\n",
      "che | l’ a| ni| ma | con| tra | con| tra | s’ ap| pan| do|\n",
      "co| sì | co| me | co| sì | com’ | io | di| sco| sta|\n",
      "                            \n",
      " co| sì | l’ a| ni| me | che | sì | com’ | io | vi| di|\n",
      "co| me | se | di| ce| ra | co| sì | com’ | io|\n",
      "ch’ i’ | vi| di | la | vi| sta | co| me | co| stai|\n",
      " che | l’ ac| qua | di | co| stan| za | che | s’ a| sco| giu| sto|\n",
      "co| sì | com’ | io | di| co | si | ri| spuo| se | sca| ra|\n",
      "co| me | che | si | ri| spuo| se | si | con| ve| ni|\n",
      "                      \n",
      " co| sì | l’ a| vea | la | co| sta | che | si | sca| ra|\n",
      "co| sì | co| stai | co| sì | com’ | io | vi| stai|\n",
      "che | l’ a| ni| ma | di| sce| de| re in| ten| di| na|\n",
      "\n",
      " ch’ i’ | vi| di | co| min| ciai | co| me | si | sco| stai|\n",
      "co| me | se | tu | che | l’ al| tra | sì | si | spe| ra|\n",
      "co| sì | co| me | co| sì | co| me | si | sco| scai|\n",
      "                               \n",
      " co| sì | l’ a| ni| ma | che | sì | com’ | io | ve| de| ra|\n",
      "co| me | se | non | sa| reb| be | co| sì | pre| gna|\n",
      "ch’ i’ | vi| di | la | sua | con| vien | ch’ i’ | ve| ra|\n",
      " co| me | se | tu | vuo’ | ch’ i’ | ho | mi | ri| spuo| se|\n",
      "che | l’ ac| qua | di | sta| va | di | sua | ve| de| ra|\n",
      "co| sì | co| me | con| vien | che | l’ a| ni| me| no|\n",
      "                              \n",
      " co| me | se| gui| na | con | la | sua | mi| sta| va|\n",
      "ch’ io | mi | fac| cia’ | io | di| co | di| cea | ve| no|\n",
      "che | l’ a| ni| ma | con | la | sua | fa| va | sta| va|\n",
      "  \n",
      " che | l’ a| ni| man| da | con | l’ ac| qua | ri| spuo| se|\n",
      "co| sì | com’ | io | con | li oc| chi | m’ a| va’ | mon| do|\n",
      "co| me | co| sì | com’ | io | ch’ io | mi | sco| sco|\n",
      "                          \n",
      " co| me | si | fa| vil| lan | con | li oc| chi | so| no|\n",
      "che | l’ a| ni| ma | del | ciel | che | la | sua | pia| va|\n",
      "co| sì | com’ | io | con| vien | che | la | fo| ce|\n",
      " co| sì | com’ | io | dis| s’ io | ch’ al| tri | si | stet| ta|\n",
      "co| sì | com’ | io | dis| s’ io | mi | sia | ri| spo| scio|\n",
      "co| me | che | se’ | tu | che | la | sua | ma| e| ta|\n",
      "                       \n",
      " ed | el| li a | me | di| spo| sta | mi | fa| vil| li|\n",
      "che | l’ a| ni| ma | di | co| lor | che | la | sco| pe|\n",
      "che | l’ a| ni| ma | che | la | sua | vir| tù | giu| sta|\n",
      " ch’ i’ | vi| di | con | li | spa| re| va | so| lo| co|\n",
      "che | l’ ac| qua | di | co| stai | che | l’ al| tri | su| sta|\n",
      "co| sì | com’ | io | con | li | spa| va | si| cu| sto|\n",
      "                           \n",
      " co| sì | la | co| sta | che | so| lo a | la | sco| sta|\n",
      "co| sì | com’ | io | di| ce| rai | co| me | su| sto|\n",
      "che | l’ a| ni| ma | co| stan| te | che | si | sco| sta|\n",
      " co|\n",
      "noi | ch’ a | l’ a| vea | la | sua | ve| der | la | fo| ca|\n",
      "ch’ io | mi | fa| rel| li | con| vien | ch’ i’ | ri| stan| te|\n",
      "co| me | co| sì | com’ | io | di| co | si | sco| glio|\n",
      "                  \n",
      " co| me | se| gui| ta| va | la | co| sta| va | sci| sta|\n",
      "che | l’ a| ni| ma | di | co| sta | co| stai | pro| glio|\n",
      "che | l’ a| ni| ma | co| stan| te | che | si| sti| sta|\n",
      " co|\n",
      "lo | suo | con| tra| di| nan| do | ch’ a | la | sco| sta|\n",
      "che | l’ ac| qua | di | co| stan| ta | ch’ i’ | ri| vi| sta|\n",
      "co| sì | com’ | io | che | l’ a| ni| ma | si | sco| sta|\n",
      "                    \n",
      " co| sì | la | co| stan| ta | che | son | la | spi| ri|\n",
      "co| sì | com’ | io | di| ce| rai | co| me | sco| sta|\n",
      "che | l’ a| ni| ma | che | la | sua | co| stan| tan| ta|\n",
      " ch’ i’ | vi| di | co| min| ciai | co| me | si | sco| stai|\n",
      "co| sì | com’ | io | ch’ al | pre| ga’ | io | la | pro| sta|\n",
      "co| me | co| sì | co| me | co| sì | si | sco| stai|\n",
      "                            \n",
      " co| sì | l’ a| ni| me | co| me | co| sì | pre| sca|\n",
      "che | l’ a| ni| ma | con | la | sua | mia | se| gui| na|\n",
      "ch’ i’ | vi| di | la | sua | co| sì | com’ | io | fret| ta|\n",
      " che | l’ ac| qua | di | co| sì | co| sì | com’ | io | vi| vi|\n",
      "che | l’ ac| qua | di | co| stai | con| tra | ri| vo|\n",
      "co| sì | com’ | io | di| cea | ma| ra| va | sci| vi|\n",
      "                               \n",
      " co| sì | l’ ac| qua | da | l’ al| tra | so| pra | vi| vo|\n",
      "che | l’ al| tra | co| sì | com’ | io | mi | si| ri| vi|\n",
      "che | l’ a| ni| ma | che | sì | com’ | io | vi| vi|\n",
      " che | l’ a| ni| man| da | co| lui | che | si | sta| va|\n",
      "che | l’ ac| qua | di | sta| va | ch’ al| tri | s’ a| do| ra|\n",
      "co| me | si | fa | che | sì | che | la | sua | va| va|\n",
      "                           \n",
      " co| sì | la | vi| sta | che | sì | co| me | sco| ra|\n",
      "co| sì | com’ | io | di| ce| rai | co| me | sta| va|\n",
      "ch’ io | la | mia | di| scor| da | co| sì | com’ | o| ra|\n",
      " che | l’ a| ni| mai | che | l’ al| tra | con| ten| tai|\n",
      "che | l’ ac| qua | di | co| sì | com’ | io | ri| spo| sco|\n",
      "co| sì | co| me | co| sì | co| sì | com’ | io|\n",
      "                                      \n",
      " co| sì | l’ a| ni| ma | che | se | non | sia| vo| sco|\n",
      "co| sì | co| me | co’ | io | che | la | sua | vi| so|\n",
      "che | l’ a| ni| ma | che | si | fa | sì | ri| spo| sco|\n",
      "\n",
      "epoch lasted: 764.9342153072357\n",
      "Epoch 122 Batch 0 Loss 0.6347 Accuracy 0.7807\n",
      "Epoch 122 Batch 50 Loss 0.6705 Accuracy 0.7707\n",
      "Epoch 122 Batch 100 Loss 0.6704 Accuracy 0.7704\n",
      "Epoch 122 Batch 150 Loss 0.6762 Accuracy 0.7690\n",
      "Epoch 122 Batch 200 Loss 0.6785 Accuracy 0.7688\n",
      "Epoch 122 Batch 250 Loss 0.6791 Accuracy 0.7683\n",
      "Epoch 122 Batch 300 Loss 0.6786 Accuracy 0.7684\n",
      "Epoch 122 Batch 350 Loss 0.6783 Accuracy 0.7683\n",
      "Epoch 122 Batch 400 Loss 0.6793 Accuracy 0.7682\n",
      "Epoch 122 Batch 450 Loss 0.6786 Accuracy 0.7683\n",
      "Epoch 122 Batch 500 Loss 0.6790 Accuracy 0.7681\n",
      "Epoch 122 Batch 550 Loss 0.6787 Accuracy 0.7682\n",
      "Epoch 122 Batch 600 Loss 0.6780 Accuracy 0.7685\n",
      "Epoch 122 Batch 650 Loss 0.6799 Accuracy 0.7678\n",
      "Epoch 122 Batch 700 Loss 0.6799 Accuracy 0.7677\n",
      "Epoch 122 Batch 750 Loss 0.6799 Accuracy 0.7675\n",
      "Epoch 122 Batch 800 Loss 0.6798 Accuracy 0.7676\n",
      "Epoch 122 Batch 850 Loss 0.6803 Accuracy 0.7676\n",
      "Epoch 122 Batch 900 Loss 0.6800 Accuracy 0.7677\n",
      "Epoch 122 Batch 950 Loss 0.6804 Accuracy 0.7677\n",
      "Epoch 122 Batch 1000 Loss 0.6810 Accuracy 0.7676\n",
      "Epoch 122 Batch 1050 Loss 0.6811 Accuracy 0.7676\n",
      "Epoch 122 Batch 1100 Loss 0.6811 Accuracy 0.7676\n",
      "Epoch 122 Batch 1150 Loss 0.6809 Accuracy 0.7677\n",
      "Epoch 122 Batch 1200 Loss 0.6815 Accuracy 0.7675\n",
      "Epoch 122 Batch 1250 Loss 0.6815 Accuracy 0.7674\n",
      "Epoch 122 Batch 1300 Loss 0.6814 Accuracy 0.7675\n",
      "Epoch 122 Batch 1350 Loss 0.6814 Accuracy 0.7675\n",
      "discarded batch 1372\n",
      "Epoch 122 Batch 1400 Loss 0.6815 Accuracy 0.7675\n",
      "Epoch 122 Batch 1450 Loss 0.6818 Accuracy 0.7674\n",
      "Epoch 122 Batch 1500 Loss 0.6820 Accuracy 0.7673\n",
      "Epoch 122 Loss 0.6820 Accuracy 0.7673\n",
      "Time taken for 1 epoch: 37.83760070800781 secs\n",
      "\n",
      "epoch lasted: 37.84164357185364\n",
      "Epoch 123 Batch 0 Loss 0.7094 Accuracy 0.7625\n",
      "Epoch 123 Batch 50 Loss 0.6780 Accuracy 0.7706\n",
      "Epoch 123 Batch 100 Loss 0.6782 Accuracy 0.7701\n",
      "Epoch 123 Batch 150 Loss 0.6782 Accuracy 0.7692\n",
      "Epoch 123 Batch 200 Loss 0.6786 Accuracy 0.7690\n",
      "Epoch 123 Batch 250 Loss 0.6776 Accuracy 0.7692\n",
      "Epoch 123 Batch 300 Loss 0.6754 Accuracy 0.7701\n",
      "Epoch 123 Batch 350 Loss 0.6757 Accuracy 0.7698\n",
      "Epoch 123 Batch 400 Loss 0.6750 Accuracy 0.7698\n",
      "Epoch 123 Batch 450 Loss 0.6752 Accuracy 0.7698\n",
      "Epoch 123 Batch 500 Loss 0.6755 Accuracy 0.7695\n",
      "Epoch 123 Batch 550 Loss 0.6769 Accuracy 0.7691\n",
      "Epoch 123 Batch 600 Loss 0.6769 Accuracy 0.7690\n",
      "Epoch 123 Batch 650 Loss 0.6772 Accuracy 0.7689\n",
      "Epoch 123 Batch 700 Loss 0.6777 Accuracy 0.7688\n",
      "discarded batch 729\n",
      "Epoch 123 Batch 750 Loss 0.6777 Accuracy 0.7688\n",
      "Epoch 123 Batch 800 Loss 0.6781 Accuracy 0.7688\n",
      "Epoch 123 Batch 850 Loss 0.6782 Accuracy 0.7686\n",
      "Epoch 123 Batch 900 Loss 0.6789 Accuracy 0.7685\n",
      "Epoch 123 Batch 950 Loss 0.6793 Accuracy 0.7685\n",
      "Epoch 123 Batch 1000 Loss 0.6793 Accuracy 0.7684\n",
      "Epoch 123 Batch 1050 Loss 0.6797 Accuracy 0.7683\n",
      "Epoch 123 Batch 1100 Loss 0.6796 Accuracy 0.7682\n",
      "Epoch 123 Batch 1150 Loss 0.6797 Accuracy 0.7681\n",
      "Epoch 123 Batch 1200 Loss 0.6802 Accuracy 0.7680\n",
      "Epoch 123 Batch 1250 Loss 0.6805 Accuracy 0.7679\n",
      "Epoch 123 Batch 1300 Loss 0.6806 Accuracy 0.7679\n",
      "Epoch 123 Batch 1350 Loss 0.6807 Accuracy 0.7678\n",
      "Epoch 123 Batch 1400 Loss 0.6814 Accuracy 0.7676\n",
      "Epoch 123 Batch 1450 Loss 0.6816 Accuracy 0.7675\n",
      "Epoch 123 Batch 1500 Loss 0.6817 Accuracy 0.7674\n",
      "Epoch 123 Loss 0.6818 Accuracy 0.7674\n",
      "Time taken for 1 epoch: 36.76870012283325 secs\n",
      "\n",
      "epoch lasted: 36.77227020263672\n",
      "Epoch 124 Batch 0 Loss 0.6201 Accuracy 0.8073\n",
      "Epoch 124 Batch 50 Loss 0.6716 Accuracy 0.7685\n",
      "Epoch 124 Batch 100 Loss 0.6683 Accuracy 0.7710\n",
      "Epoch 124 Batch 150 Loss 0.6717 Accuracy 0.7695\n",
      "Epoch 124 Batch 200 Loss 0.6755 Accuracy 0.7685\n",
      "Epoch 124 Batch 250 Loss 0.6737 Accuracy 0.7687\n",
      "Epoch 124 Batch 300 Loss 0.6751 Accuracy 0.7687\n",
      "Epoch 124 Batch 350 Loss 0.6756 Accuracy 0.7683\n",
      "Epoch 124 Batch 400 Loss 0.6747 Accuracy 0.7685\n",
      "Epoch 124 Batch 450 Loss 0.6760 Accuracy 0.7683\n",
      "Epoch 124 Batch 500 Loss 0.6746 Accuracy 0.7690\n",
      "Epoch 124 Batch 550 Loss 0.6746 Accuracy 0.7691\n",
      "Epoch 124 Batch 600 Loss 0.6749 Accuracy 0.7690\n",
      "Epoch 124 Batch 650 Loss 0.6755 Accuracy 0.7689\n",
      "Epoch 124 Batch 700 Loss 0.6756 Accuracy 0.7691\n",
      "Epoch 124 Batch 750 Loss 0.6758 Accuracy 0.7689\n",
      "Epoch 124 Batch 800 Loss 0.6758 Accuracy 0.7687\n",
      "Epoch 124 Batch 850 Loss 0.6762 Accuracy 0.7687\n",
      "Epoch 124 Batch 900 Loss 0.6765 Accuracy 0.7687\n",
      "Epoch 124 Batch 950 Loss 0.6769 Accuracy 0.7687\n",
      "Epoch 124 Batch 1000 Loss 0.6771 Accuracy 0.7687\n",
      "Epoch 124 Batch 1050 Loss 0.6770 Accuracy 0.7688\n",
      "Epoch 124 Batch 1100 Loss 0.6774 Accuracy 0.7687\n",
      "Epoch 124 Batch 1150 Loss 0.6774 Accuracy 0.7687\n",
      "Epoch 124 Batch 1200 Loss 0.6775 Accuracy 0.7687\n",
      "Epoch 124 Batch 1250 Loss 0.6774 Accuracy 0.7687\n",
      "Epoch 124 Batch 1300 Loss 0.6782 Accuracy 0.7686\n",
      "Epoch 124 Batch 1350 Loss 0.6783 Accuracy 0.7685\n",
      "Epoch 124 Batch 1400 Loss 0.6786 Accuracy 0.7684\n",
      "discarded batch 1431\n",
      "Epoch 124 Batch 1450 Loss 0.6791 Accuracy 0.7682\n",
      "Epoch 124 Batch 1500 Loss 0.6792 Accuracy 0.7682\n",
      "Epoch 124 Loss 0.6796 Accuracy 0.7680\n",
      "Time taken for 1 epoch: 37.4581823348999 secs\n",
      "\n",
      "epoch lasted: 37.462334394454956\n",
      "Epoch 125 Batch 0 Loss 0.6645 Accuracy 0.7658\n",
      "Epoch 125 Batch 50 Loss 0.6638 Accuracy 0.7720\n",
      "Epoch 125 Batch 100 Loss 0.6704 Accuracy 0.7704\n",
      "Epoch 125 Batch 150 Loss 0.6716 Accuracy 0.7691\n",
      "Epoch 125 Batch 200 Loss 0.6714 Accuracy 0.7700\n",
      "Epoch 125 Batch 250 Loss 0.6727 Accuracy 0.7695\n",
      "Epoch 125 Batch 300 Loss 0.6745 Accuracy 0.7693\n",
      "discarded batch 350\n",
      "Epoch 125 Batch 400 Loss 0.6761 Accuracy 0.7690\n",
      "Epoch 125 Batch 450 Loss 0.6756 Accuracy 0.7691\n",
      "Epoch 125 Batch 500 Loss 0.6750 Accuracy 0.7693\n",
      "Epoch 125 Batch 550 Loss 0.6753 Accuracy 0.7691\n",
      "Epoch 125 Batch 600 Loss 0.6755 Accuracy 0.7691\n",
      "Epoch 125 Batch 650 Loss 0.6757 Accuracy 0.7689\n",
      "Epoch 125 Batch 700 Loss 0.6759 Accuracy 0.7691\n",
      "Epoch 125 Batch 750 Loss 0.6760 Accuracy 0.7690\n",
      "Epoch 125 Batch 800 Loss 0.6771 Accuracy 0.7688\n",
      "Epoch 125 Batch 850 Loss 0.6772 Accuracy 0.7688\n",
      "Epoch 125 Batch 900 Loss 0.6770 Accuracy 0.7690\n",
      "Epoch 125 Batch 950 Loss 0.6775 Accuracy 0.7688\n",
      "Epoch 125 Batch 1000 Loss 0.6779 Accuracy 0.7686\n",
      "Epoch 125 Batch 1050 Loss 0.6775 Accuracy 0.7687\n",
      "Epoch 125 Batch 1100 Loss 0.6774 Accuracy 0.7688\n",
      "Epoch 125 Batch 1150 Loss 0.6775 Accuracy 0.7687\n",
      "Epoch 125 Batch 1200 Loss 0.6781 Accuracy 0.7686\n",
      "Epoch 125 Batch 1250 Loss 0.6783 Accuracy 0.7684\n",
      "Epoch 125 Batch 1300 Loss 0.6788 Accuracy 0.7683\n",
      "Epoch 125 Batch 1350 Loss 0.6787 Accuracy 0.7683\n",
      "Epoch 125 Batch 1400 Loss 0.6789 Accuracy 0.7682\n",
      "Epoch 125 Batch 1450 Loss 0.6790 Accuracy 0.7682\n",
      "Epoch 125 Batch 1500 Loss 0.6791 Accuracy 0.7681\n",
      "Saving checkpoint for epoch 125 at ./checkpoints/train/ckpt-25\n",
      "Epoch 125 Loss 0.6793 Accuracy 0.7680\n",
      "Time taken for 1 epoch: 38.1943039894104 secs\n",
      "\n",
      "epoch lasted: 38.198561668395996\n",
      "Epoch 126 Batch 0 Loss 0.6751 Accuracy 0.7591\n",
      "Epoch 126 Batch 50 Loss 0.6647 Accuracy 0.7728\n",
      "Epoch 126 Batch 100 Loss 0.6654 Accuracy 0.7721\n",
      "Epoch 126 Batch 150 Loss 0.6665 Accuracy 0.7723\n",
      "Epoch 126 Batch 200 Loss 0.6668 Accuracy 0.7727\n",
      "Epoch 126 Batch 250 Loss 0.6690 Accuracy 0.7718\n",
      "Epoch 126 Batch 300 Loss 0.6697 Accuracy 0.7711\n",
      "Epoch 126 Batch 350 Loss 0.6722 Accuracy 0.7702\n",
      "Epoch 126 Batch 400 Loss 0.6731 Accuracy 0.7701\n",
      "Epoch 126 Batch 450 Loss 0.6734 Accuracy 0.7699\n",
      "Epoch 126 Batch 500 Loss 0.6723 Accuracy 0.7703\n",
      "Epoch 126 Batch 550 Loss 0.6736 Accuracy 0.7698\n",
      "Epoch 126 Batch 600 Loss 0.6737 Accuracy 0.7697\n",
      "Epoch 126 Batch 650 Loss 0.6745 Accuracy 0.7695\n",
      "Epoch 126 Batch 700 Loss 0.6749 Accuracy 0.7694\n",
      "Epoch 126 Batch 750 Loss 0.6750 Accuracy 0.7694\n",
      "Epoch 126 Batch 800 Loss 0.6751 Accuracy 0.7695\n",
      "Epoch 126 Batch 850 Loss 0.6755 Accuracy 0.7692\n",
      "Epoch 126 Batch 900 Loss 0.6758 Accuracy 0.7692\n",
      "Epoch 126 Batch 950 Loss 0.6758 Accuracy 0.7693\n",
      "Epoch 126 Batch 1000 Loss 0.6762 Accuracy 0.7692\n",
      "Epoch 126 Batch 1050 Loss 0.6761 Accuracy 0.7692\n",
      "Epoch 126 Batch 1100 Loss 0.6764 Accuracy 0.7691\n",
      "Epoch 126 Batch 1150 Loss 0.6769 Accuracy 0.7689\n",
      "Epoch 126 Batch 1200 Loss 0.6770 Accuracy 0.7688\n",
      "Epoch 126 Batch 1250 Loss 0.6773 Accuracy 0.7688\n",
      "Epoch 126 Batch 1300 Loss 0.6776 Accuracy 0.7687\n",
      "Epoch 126 Batch 1350 Loss 0.6782 Accuracy 0.7685\n",
      "Epoch 126 Batch 1400 Loss 0.6784 Accuracy 0.7684\n",
      "Epoch 126 Batch 1450 Loss 0.6784 Accuracy 0.7684\n",
      "discarded batch 1472\n",
      "Epoch 126 Batch 1500 Loss 0.6783 Accuracy 0.7685\n",
      "Epoch 126 Loss 0.6783 Accuracy 0.7686\n",
      "Time taken for 1 epoch: 38.06137752532959 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 126 VALIDATION: Loss 0.7317 Accuracy 0.7592\n",
      "\n",
      "epoch lasted: 38.211004972457886\n",
      "Epoch 127 Batch 0 Loss 0.7296 Accuracy 0.7492\n",
      "Epoch 127 Batch 50 Loss 0.6664 Accuracy 0.7723\n",
      "Epoch 127 Batch 100 Loss 0.6744 Accuracy 0.7706\n",
      "Epoch 127 Batch 150 Loss 0.6744 Accuracy 0.7706\n",
      "Epoch 127 Batch 200 Loss 0.6716 Accuracy 0.7709\n",
      "Epoch 127 Batch 250 Loss 0.6729 Accuracy 0.7702\n",
      "Epoch 127 Batch 300 Loss 0.6740 Accuracy 0.7696\n",
      "Epoch 127 Batch 350 Loss 0.6757 Accuracy 0.7693\n",
      "Epoch 127 Batch 400 Loss 0.6752 Accuracy 0.7693\n",
      "Epoch 127 Batch 450 Loss 0.6743 Accuracy 0.7696\n",
      "Epoch 127 Batch 500 Loss 0.6744 Accuracy 0.7696\n",
      "Epoch 127 Batch 550 Loss 0.6739 Accuracy 0.7697\n",
      "Epoch 127 Batch 600 Loss 0.6746 Accuracy 0.7694\n",
      "Epoch 127 Batch 650 Loss 0.6745 Accuracy 0.7695\n",
      "Epoch 127 Batch 700 Loss 0.6749 Accuracy 0.7695\n",
      "Epoch 127 Batch 750 Loss 0.6751 Accuracy 0.7695\n",
      "Epoch 127 Batch 800 Loss 0.6750 Accuracy 0.7695\n",
      "Epoch 127 Batch 850 Loss 0.6745 Accuracy 0.7697\n",
      "Epoch 127 Batch 900 Loss 0.6746 Accuracy 0.7696\n",
      "Epoch 127 Batch 950 Loss 0.6745 Accuracy 0.7697\n",
      "Epoch 127 Batch 1000 Loss 0.6752 Accuracy 0.7693\n",
      "Epoch 127 Batch 1050 Loss 0.6756 Accuracy 0.7691\n",
      "Epoch 127 Batch 1100 Loss 0.6756 Accuracy 0.7691\n",
      "Epoch 127 Batch 1150 Loss 0.6756 Accuracy 0.7690\n",
      "Epoch 127 Batch 1200 Loss 0.6762 Accuracy 0.7689\n",
      "Epoch 127 Batch 1250 Loss 0.6765 Accuracy 0.7688\n",
      "Epoch 127 Batch 1300 Loss 0.6765 Accuracy 0.7688\n",
      "Epoch 127 Batch 1350 Loss 0.6770 Accuracy 0.7687\n",
      "discarded batch 1362\n",
      "Epoch 127 Batch 1400 Loss 0.6772 Accuracy 0.7686\n",
      "Epoch 127 Batch 1450 Loss 0.6776 Accuracy 0.7685\n",
      "Epoch 127 Batch 1500 Loss 0.6781 Accuracy 0.7684\n",
      "Epoch 127 Loss 0.6782 Accuracy 0.7683\n",
      "Time taken for 1 epoch: 37.245593786239624 secs\n",
      "\n",
      "epoch lasted: 37.249425411224365\n",
      "Epoch 128 Batch 0 Loss 0.6320 Accuracy 0.7774\n",
      "Epoch 128 Batch 50 Loss 0.6645 Accuracy 0.7715\n",
      "Epoch 128 Batch 100 Loss 0.6679 Accuracy 0.7712\n",
      "Epoch 128 Batch 150 Loss 0.6687 Accuracy 0.7717\n",
      "Epoch 128 Batch 200 Loss 0.6680 Accuracy 0.7719\n",
      "Epoch 128 Batch 250 Loss 0.6676 Accuracy 0.7722\n",
      "Epoch 128 Batch 300 Loss 0.6681 Accuracy 0.7719\n",
      "Epoch 128 Batch 350 Loss 0.6670 Accuracy 0.7726\n",
      "Epoch 128 Batch 400 Loss 0.6692 Accuracy 0.7718\n",
      "Epoch 128 Batch 450 Loss 0.6708 Accuracy 0.7711\n",
      "Epoch 128 Batch 500 Loss 0.6725 Accuracy 0.7707\n",
      "Epoch 128 Batch 550 Loss 0.6733 Accuracy 0.7704\n",
      "Epoch 128 Batch 600 Loss 0.6723 Accuracy 0.7707\n",
      "discarded batch 609\n",
      "Epoch 128 Batch 650 Loss 0.6725 Accuracy 0.7706\n",
      "Epoch 128 Batch 700 Loss 0.6727 Accuracy 0.7706\n",
      "Epoch 128 Batch 750 Loss 0.6732 Accuracy 0.7704\n",
      "Epoch 128 Batch 800 Loss 0.6738 Accuracy 0.7702\n",
      "Epoch 128 Batch 850 Loss 0.6742 Accuracy 0.7700\n",
      "Epoch 128 Batch 900 Loss 0.6743 Accuracy 0.7699\n",
      "Epoch 128 Batch 950 Loss 0.6741 Accuracy 0.7700\n",
      "Epoch 128 Batch 1000 Loss 0.6740 Accuracy 0.7700\n",
      "Epoch 128 Batch 1050 Loss 0.6746 Accuracy 0.7698\n",
      "Epoch 128 Batch 1100 Loss 0.6747 Accuracy 0.7698\n",
      "Epoch 128 Batch 1150 Loss 0.6754 Accuracy 0.7696\n",
      "Epoch 128 Batch 1200 Loss 0.6759 Accuracy 0.7695\n",
      "Epoch 128 Batch 1250 Loss 0.6760 Accuracy 0.7695\n",
      "Epoch 128 Batch 1300 Loss 0.6763 Accuracy 0.7694\n",
      "Epoch 128 Batch 1350 Loss 0.6759 Accuracy 0.7695\n",
      "Epoch 128 Batch 1400 Loss 0.6762 Accuracy 0.7694\n",
      "Epoch 128 Batch 1450 Loss 0.6764 Accuracy 0.7693\n",
      "Epoch 128 Batch 1500 Loss 0.6768 Accuracy 0.7693\n",
      "Epoch 128 Loss 0.6771 Accuracy 0.7691\n",
      "Time taken for 1 epoch: 37.01921367645264 secs\n",
      "\n",
      "epoch lasted: 37.02343559265137\n",
      "Epoch 129 Batch 0 Loss 0.7113 Accuracy 0.7475\n",
      "Epoch 129 Batch 50 Loss 0.6668 Accuracy 0.7708\n",
      "Epoch 129 Batch 100 Loss 0.6659 Accuracy 0.7719\n",
      "Epoch 129 Batch 150 Loss 0.6699 Accuracy 0.7709\n",
      "Epoch 129 Batch 200 Loss 0.6720 Accuracy 0.7697\n",
      "Epoch 129 Batch 250 Loss 0.6719 Accuracy 0.7703\n",
      "Epoch 129 Batch 300 Loss 0.6704 Accuracy 0.7710\n",
      "Epoch 129 Batch 350 Loss 0.6713 Accuracy 0.7709\n",
      "Epoch 129 Batch 400 Loss 0.6730 Accuracy 0.7704\n",
      "Epoch 129 Batch 450 Loss 0.6732 Accuracy 0.7704\n",
      "Epoch 129 Batch 500 Loss 0.6729 Accuracy 0.7703\n",
      "Epoch 129 Batch 550 Loss 0.6730 Accuracy 0.7703\n",
      "Epoch 129 Batch 600 Loss 0.6737 Accuracy 0.7701\n",
      "Epoch 129 Batch 650 Loss 0.6740 Accuracy 0.7700\n",
      "Epoch 129 Batch 700 Loss 0.6737 Accuracy 0.7701\n",
      "Epoch 129 Batch 750 Loss 0.6741 Accuracy 0.7700\n",
      "Epoch 129 Batch 800 Loss 0.6743 Accuracy 0.7700\n",
      "Epoch 129 Batch 850 Loss 0.6746 Accuracy 0.7699\n",
      "Epoch 129 Batch 900 Loss 0.6750 Accuracy 0.7698\n",
      "Epoch 129 Batch 950 Loss 0.6751 Accuracy 0.7699\n",
      "Epoch 129 Batch 1000 Loss 0.6753 Accuracy 0.7698\n",
      "Epoch 129 Batch 1050 Loss 0.6751 Accuracy 0.7698\n",
      "Epoch 129 Batch 1100 Loss 0.6751 Accuracy 0.7699\n",
      "Epoch 129 Batch 1150 Loss 0.6750 Accuracy 0.7700\n",
      "discarded batch 1192\n",
      "Epoch 129 Batch 1200 Loss 0.6749 Accuracy 0.7699\n",
      "Epoch 129 Batch 1250 Loss 0.6755 Accuracy 0.7697\n",
      "Epoch 129 Batch 1300 Loss 0.6756 Accuracy 0.7697\n",
      "Epoch 129 Batch 1350 Loss 0.6755 Accuracy 0.7696\n",
      "Epoch 129 Batch 1400 Loss 0.6760 Accuracy 0.7693\n",
      "Epoch 129 Batch 1450 Loss 0.6759 Accuracy 0.7693\n",
      "Epoch 129 Batch 1500 Loss 0.6760 Accuracy 0.7692\n",
      "Epoch 129 Loss 0.6759 Accuracy 0.7693\n",
      "Time taken for 1 epoch: 36.7605767250061 secs\n",
      "\n",
      "epoch lasted: 36.7648069858551\n",
      "Epoch 130 Batch 0 Loss 0.6816 Accuracy 0.7708\n",
      "Epoch 130 Batch 50 Loss 0.6616 Accuracy 0.7734\n",
      "Epoch 130 Batch 100 Loss 0.6647 Accuracy 0.7725\n",
      "Epoch 130 Batch 150 Loss 0.6675 Accuracy 0.7721\n",
      "Epoch 130 Batch 200 Loss 0.6669 Accuracy 0.7722\n",
      "Epoch 130 Batch 250 Loss 0.6667 Accuracy 0.7724\n",
      "Epoch 130 Batch 300 Loss 0.6673 Accuracy 0.7722\n",
      "Epoch 130 Batch 350 Loss 0.6675 Accuracy 0.7721\n",
      "Epoch 130 Batch 400 Loss 0.6686 Accuracy 0.7717\n",
      "Epoch 130 Batch 450 Loss 0.6682 Accuracy 0.7715\n",
      "Epoch 130 Batch 500 Loss 0.6690 Accuracy 0.7710\n",
      "Epoch 130 Batch 550 Loss 0.6693 Accuracy 0.7709\n",
      "discarded batch 585\n",
      "Epoch 130 Batch 600 Loss 0.6699 Accuracy 0.7707\n",
      "Epoch 130 Batch 650 Loss 0.6709 Accuracy 0.7703\n",
      "Epoch 130 Batch 700 Loss 0.6711 Accuracy 0.7703\n",
      "Epoch 130 Batch 750 Loss 0.6710 Accuracy 0.7703\n",
      "Epoch 130 Batch 800 Loss 0.6712 Accuracy 0.7701\n",
      "Epoch 130 Batch 850 Loss 0.6717 Accuracy 0.7700\n",
      "Epoch 130 Batch 900 Loss 0.6715 Accuracy 0.7702\n",
      "Epoch 130 Batch 950 Loss 0.6720 Accuracy 0.7700\n",
      "Epoch 130 Batch 1000 Loss 0.6723 Accuracy 0.7700\n",
      "Epoch 130 Batch 1050 Loss 0.6723 Accuracy 0.7700\n",
      "Epoch 130 Batch 1100 Loss 0.6732 Accuracy 0.7698\n",
      "Epoch 130 Batch 1150 Loss 0.6733 Accuracy 0.7697\n",
      "Epoch 130 Batch 1200 Loss 0.6737 Accuracy 0.7696\n",
      "Epoch 130 Batch 1250 Loss 0.6737 Accuracy 0.7697\n",
      "Epoch 130 Batch 1300 Loss 0.6742 Accuracy 0.7695\n",
      "Epoch 130 Batch 1350 Loss 0.6744 Accuracy 0.7695\n",
      "Epoch 130 Batch 1400 Loss 0.6749 Accuracy 0.7694\n",
      "Epoch 130 Batch 1450 Loss 0.6753 Accuracy 0.7693\n",
      "Epoch 130 Batch 1500 Loss 0.6751 Accuracy 0.7694\n",
      "Saving checkpoint for epoch 130 at ./checkpoints/train/ckpt-26\n",
      "Epoch 130 Loss 0.6751 Accuracy 0.7695\n",
      "Time taken for 1 epoch: 37.028669357299805 secs\n",
      "\n",
      "epoch lasted: 37.03282380104065\n",
      "Epoch 131 Batch 0 Loss 0.6251 Accuracy 0.7807\n",
      "Epoch 131 Batch 50 Loss 0.6702 Accuracy 0.7706\n",
      "Epoch 131 Batch 100 Loss 0.6733 Accuracy 0.7695\n",
      "Epoch 131 Batch 150 Loss 0.6723 Accuracy 0.7705\n",
      "Epoch 131 Batch 200 Loss 0.6701 Accuracy 0.7710\n",
      "Epoch 131 Batch 250 Loss 0.6715 Accuracy 0.7705\n",
      "Epoch 131 Batch 300 Loss 0.6724 Accuracy 0.7701\n",
      "Epoch 131 Batch 350 Loss 0.6720 Accuracy 0.7703\n",
      "Epoch 131 Batch 400 Loss 0.6719 Accuracy 0.7703\n",
      "Epoch 131 Batch 450 Loss 0.6710 Accuracy 0.7705\n",
      "Epoch 131 Batch 500 Loss 0.6722 Accuracy 0.7702\n",
      "Epoch 131 Batch 550 Loss 0.6715 Accuracy 0.7705\n",
      "discarded batch 572\n",
      "Epoch 131 Batch 600 Loss 0.6718 Accuracy 0.7703\n",
      "Epoch 131 Batch 650 Loss 0.6719 Accuracy 0.7705\n",
      "Epoch 131 Batch 700 Loss 0.6717 Accuracy 0.7705\n",
      "Epoch 131 Batch 750 Loss 0.6722 Accuracy 0.7704\n",
      "Epoch 131 Batch 800 Loss 0.6727 Accuracy 0.7701\n",
      "Epoch 131 Batch 850 Loss 0.6727 Accuracy 0.7702\n",
      "Epoch 131 Batch 900 Loss 0.6733 Accuracy 0.7700\n",
      "Epoch 131 Batch 950 Loss 0.6735 Accuracy 0.7699\n",
      "Epoch 131 Batch 1000 Loss 0.6735 Accuracy 0.7698\n",
      "Epoch 131 Batch 1050 Loss 0.6733 Accuracy 0.7700\n",
      "Epoch 131 Batch 1100 Loss 0.6738 Accuracy 0.7697\n",
      "Epoch 131 Batch 1150 Loss 0.6740 Accuracy 0.7696\n",
      "Epoch 131 Batch 1200 Loss 0.6741 Accuracy 0.7696\n",
      "Epoch 131 Batch 1250 Loss 0.6740 Accuracy 0.7697\n",
      "Epoch 131 Batch 1300 Loss 0.6742 Accuracy 0.7695\n",
      "Epoch 131 Batch 1350 Loss 0.6746 Accuracy 0.7695\n",
      "Epoch 131 Batch 1400 Loss 0.6746 Accuracy 0.7695\n",
      "Epoch 131 Batch 1450 Loss 0.6747 Accuracy 0.7694\n",
      "Epoch 131 Batch 1500 Loss 0.6748 Accuracy 0.7694\n",
      "Epoch 131 Loss 0.6747 Accuracy 0.7694\n",
      "Time taken for 1 epoch: 36.9108407497406 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 131 VALIDATION: Loss 0.7354 Accuracy 0.7589\n",
      "\n",
      "la | tua | be| ni| gni| tà | non | pur | soc| cor| re|\n",
      "a | chi | do| man| da | ma | mol| te | fï| a| te|\n",
      "li| be| ra| men| te al | di| man| dar | pre| cor| re|\n",
      "                                        \n",
      " co| me | l’ a| mo| re a| mo| re a | me | con| te| sto|\n",
      "co| sì | co| me | se| con| da | la | mon| da| ta|\n",
      "co| sì | com’ | io | m’ a| vea | la | ma| te| se| se|\n",
      "                                         \n",
      " co| me | so| le | suo | pa| re| te | mi | pa| ta|\n",
      "e | co| me | co| me | so| lo a| vea | pa| rea|\n",
      "per | lo | suo | mi| se| ro a| mo| re a | me | ca| to|\n",
      " ma|\n",
      "                                                              i | co| me | co| me | suo| lo|\n",
      "co| tan| do| min| cia|\n",
      "ed | el| li a | me | con | lo | suo | ma| e| sto|\n",
      "  \n",
      "  \n",
      " e | co| me | co| me | co| me | suo | ma| drea|\n",
      "me|\n",
      "co| me | so| lo a| mo| re a | me | con | a| mo| ro|\n",
      "co| sì | mi | fos| se | ma| dre | co| me | sa| li|\n",
      "                                              \n",
      " e | co| me | co| me | suo | mo| do| min| ca| to|\n",
      "ve| di | ma| dre | co| me | so| lo| na| to| na|\n",
      "co| me | sa| li| ce | sa| li|\n",
      "  \n",
      " co| me | su| sca| to | m’ a| mo| ra | co| ston| da|\n",
      "lo | co| lo| ro| so| so| so|\n",
      "ma | per | lo | sco| po| na|\n",
      "co| sì | com’ | io | mi | vo| stro | mo| vea | ma| to|\n",
      "                                   \n",
      " co| me | so| lo | so| lon| do | san| za | po| sa|\n",
      "e | co| me | co| sì | com’ | io | mi | fa| cea|\n",
      "po| sco | m’ a| vea | pa| rea|\n",
      "ma | per | lo | sco| sta| le|\n",
      " che | me | co| me | so| lo a | me | con | lo | sco| sco|\n",
      "co| me | co| me | con | lo | spi| ro | co| ta| lea|\n",
      "e | co| me | suo | co| me | suo | pa| rea | mo| sco|\n",
      "                                      \n",
      " e | co| me | so| le | sco| per| to | m’ a| da| ma|\n",
      "e | co| me | co| me | so| lo| sfo| sa| men| te|\n",
      "co| sa | l’ om| bra | mo| do|\n",
      "con | lo | suo | ma| e| stro|\n",
      " co| me | la | ma| dre | che | mi | fa| cea | mo| do|\n",
      "co| me | so| lo|\n",
      "co| me | con | lo | suo | mio | do| mo|\n",
      "con | lo | sco| per| to | co| sto | co| sto| mo|\n",
      "                                         \n",
      " e | co| me | so| lo | sta| va| mo a | lo | sco| sa|\n",
      "e | co| me | co| stui | co| tan| do | san| to| mo|\n",
      "ve| di | la | mon| da| nan| no a | la | ma| dre| sta|\n",
      "  \n",
      " co| me | co| me | co| me | co| me | si | co| sa|\n",
      "ve| di| ce | mi | fa| me a | me | co| me | so| ma|\n",
      "co| sì | mi | fos| se | ma| dre | co| me | co| sa|\n",
      "                          \n",
      "co| me | co| sì | mi | fa| me|\n",
      "con | la | mia | me| mo| glia | ma| ni| fa| mi| co|\n",
      "co| me | co| me | con| vien | ch’ al | mio | di| ma|\n",
      " e|\n",
      "                                                                                                                                                                                                     \n",
      " co| me | so| le | so| lon| do | so| vra | pia|\n",
      "e | io | mi | par| lai|\n",
      "ma | per | lo | suo | pia| ce| ra|\n",
      "co| me | tu | ma| re| na | mo| do | mio | du| ca|\n",
      "                                                                                                                                                                                                         \n",
      " e | co| me | suo | ma| dre| sti | ma| drea | ma| no|\n",
      "e | con | lo | spi| ro | ma| dre a | lo | suo | ma| no|\n",
      "co| me | mi| se| ro a | me | con | la | ma| dra| no|\n",
      "  \n",
      " co|\n",
      "e | co| me | se| do | mi | fa| ma | di| sce| sti|\n",
      "lo | co| sto| ro a | lo | spi| ri| to| sa | mo| do|\n",
      "co| sì | com’ | el| li a| mo| re a| mo| re a| ma| no|\n",
      "                                        \n",
      " ma | pe| na | me| sti| mo| na| to | m’ a| do| ma|\n",
      "ed | el| li a | me | con | lo | suo | mo| do | sa| no|\n",
      "co| me | tu | me| na| to | che | la | ma| do| ma|\n",
      "  \n",
      "\n",
      "epoch lasted: 478.7579734325409\n",
      "Epoch 132 Batch 0 Loss 0.6252 Accuracy 0.7957\n",
      "Epoch 132 Batch 50 Loss 0.6580 Accuracy 0.7758\n",
      "Epoch 132 Batch 100 Loss 0.6610 Accuracy 0.7745\n",
      "Epoch 132 Batch 150 Loss 0.6603 Accuracy 0.7744\n",
      "Epoch 132 Batch 200 Loss 0.6621 Accuracy 0.7737\n",
      "Epoch 132 Batch 250 Loss 0.6643 Accuracy 0.7727\n",
      "Epoch 132 Batch 300 Loss 0.6661 Accuracy 0.7719\n",
      "Epoch 132 Batch 350 Loss 0.6679 Accuracy 0.7714\n",
      "Epoch 132 Batch 400 Loss 0.6688 Accuracy 0.7714\n",
      "Epoch 132 Batch 450 Loss 0.6701 Accuracy 0.7710\n",
      "Epoch 132 Batch 500 Loss 0.6703 Accuracy 0.7710\n",
      "Epoch 132 Batch 550 Loss 0.6716 Accuracy 0.7708\n",
      "Epoch 132 Batch 600 Loss 0.6708 Accuracy 0.7711\n",
      "Epoch 132 Batch 650 Loss 0.6709 Accuracy 0.7711\n",
      "Epoch 132 Batch 700 Loss 0.6709 Accuracy 0.7712\n",
      "Epoch 132 Batch 750 Loss 0.6709 Accuracy 0.7712\n",
      "Epoch 132 Batch 800 Loss 0.6708 Accuracy 0.7711\n",
      "Epoch 132 Batch 850 Loss 0.6711 Accuracy 0.7709\n",
      "Epoch 132 Batch 900 Loss 0.6715 Accuracy 0.7708\n",
      "Epoch 132 Batch 950 Loss 0.6714 Accuracy 0.7709\n",
      "Epoch 132 Batch 1000 Loss 0.6721 Accuracy 0.7706\n",
      "Epoch 132 Batch 1050 Loss 0.6717 Accuracy 0.7708\n",
      "Epoch 132 Batch 1100 Loss 0.6717 Accuracy 0.7708\n",
      "discarded batch 1102\n",
      "Epoch 132 Batch 1150 Loss 0.6715 Accuracy 0.7708\n",
      "Epoch 132 Batch 1200 Loss 0.6720 Accuracy 0.7707\n",
      "Epoch 132 Batch 1250 Loss 0.6727 Accuracy 0.7705\n",
      "Epoch 132 Batch 1300 Loss 0.6729 Accuracy 0.7704\n",
      "Epoch 132 Batch 1350 Loss 0.6730 Accuracy 0.7704\n",
      "Epoch 132 Batch 1400 Loss 0.6732 Accuracy 0.7704\n",
      "Epoch 132 Batch 1450 Loss 0.6736 Accuracy 0.7703\n",
      "Epoch 132 Batch 1500 Loss 0.6738 Accuracy 0.7702\n",
      "Epoch 132 Loss 0.6738 Accuracy 0.7702\n",
      "Time taken for 1 epoch: 38.60256862640381 secs\n",
      "\n",
      "epoch lasted: 38.606494665145874\n",
      "Epoch 133 Batch 0 Loss 0.6561 Accuracy 0.7625\n",
      "Epoch 133 Batch 50 Loss 0.6684 Accuracy 0.7709\n",
      "Epoch 133 Batch 100 Loss 0.6674 Accuracy 0.7715\n",
      "Epoch 133 Batch 150 Loss 0.6654 Accuracy 0.7728\n",
      "Epoch 133 Batch 200 Loss 0.6683 Accuracy 0.7717\n",
      "discarded batch 236\n",
      "Epoch 133 Batch 250 Loss 0.6651 Accuracy 0.7732\n",
      "Epoch 133 Batch 300 Loss 0.6648 Accuracy 0.7731\n",
      "Epoch 133 Batch 350 Loss 0.6657 Accuracy 0.7726\n",
      "Epoch 133 Batch 400 Loss 0.6660 Accuracy 0.7725\n",
      "Epoch 133 Batch 450 Loss 0.6664 Accuracy 0.7726\n",
      "Epoch 133 Batch 500 Loss 0.6663 Accuracy 0.7728\n",
      "Epoch 133 Batch 550 Loss 0.6668 Accuracy 0.7728\n",
      "Epoch 133 Batch 600 Loss 0.6668 Accuracy 0.7727\n",
      "Epoch 133 Batch 650 Loss 0.6666 Accuracy 0.7727\n",
      "Epoch 133 Batch 700 Loss 0.6671 Accuracy 0.7724\n",
      "Epoch 133 Batch 750 Loss 0.6679 Accuracy 0.7722\n",
      "Epoch 133 Batch 800 Loss 0.6682 Accuracy 0.7721\n",
      "Epoch 133 Batch 850 Loss 0.6689 Accuracy 0.7719\n",
      "Epoch 133 Batch 900 Loss 0.6691 Accuracy 0.7717\n",
      "Epoch 133 Batch 950 Loss 0.6699 Accuracy 0.7715\n",
      "Epoch 133 Batch 1000 Loss 0.6699 Accuracy 0.7715\n",
      "Epoch 133 Batch 1050 Loss 0.6701 Accuracy 0.7713\n",
      "Epoch 133 Batch 1100 Loss 0.6708 Accuracy 0.7710\n",
      "Epoch 133 Batch 1150 Loss 0.6712 Accuracy 0.7708\n",
      "Epoch 133 Batch 1200 Loss 0.6712 Accuracy 0.7707\n",
      "Epoch 133 Batch 1250 Loss 0.6715 Accuracy 0.7706\n",
      "Epoch 133 Batch 1300 Loss 0.6718 Accuracy 0.7705\n",
      "Epoch 133 Batch 1350 Loss 0.6715 Accuracy 0.7706\n",
      "Epoch 133 Batch 1400 Loss 0.6717 Accuracy 0.7705\n",
      "Epoch 133 Batch 1450 Loss 0.6725 Accuracy 0.7703\n",
      "Epoch 133 Batch 1500 Loss 0.6725 Accuracy 0.7703\n",
      "Epoch 133 Loss 0.6729 Accuracy 0.7701\n",
      "Time taken for 1 epoch: 38.140634298324585 secs\n",
      "\n",
      "epoch lasted: 38.144054651260376\n",
      "Epoch 134 Batch 0 Loss 0.6478 Accuracy 0.7841\n",
      "Epoch 134 Batch 50 Loss 0.6533 Accuracy 0.7757\n",
      "Epoch 134 Batch 100 Loss 0.6591 Accuracy 0.7754\n",
      "Epoch 134 Batch 150 Loss 0.6580 Accuracy 0.7756\n",
      "Epoch 134 Batch 200 Loss 0.6619 Accuracy 0.7733\n",
      "Epoch 134 Batch 250 Loss 0.6662 Accuracy 0.7720\n",
      "Epoch 134 Batch 300 Loss 0.6664 Accuracy 0.7718\n",
      "Epoch 134 Batch 350 Loss 0.6675 Accuracy 0.7713\n",
      "Epoch 134 Batch 400 Loss 0.6679 Accuracy 0.7711\n",
      "Epoch 134 Batch 450 Loss 0.6677 Accuracy 0.7714\n",
      "Epoch 134 Batch 500 Loss 0.6685 Accuracy 0.7712\n",
      "Epoch 134 Batch 550 Loss 0.6689 Accuracy 0.7710\n",
      "Epoch 134 Batch 600 Loss 0.6680 Accuracy 0.7713\n",
      "Epoch 134 Batch 650 Loss 0.6681 Accuracy 0.7712\n",
      "Epoch 134 Batch 700 Loss 0.6680 Accuracy 0.7712\n",
      "Epoch 134 Batch 750 Loss 0.6680 Accuracy 0.7713\n",
      "Epoch 134 Batch 800 Loss 0.6681 Accuracy 0.7713\n",
      "Epoch 134 Batch 850 Loss 0.6684 Accuracy 0.7713\n",
      "Epoch 134 Batch 900 Loss 0.6685 Accuracy 0.7713\n",
      "discarded batch 939\n",
      "Epoch 134 Batch 950 Loss 0.6688 Accuracy 0.7713\n",
      "Epoch 134 Batch 1000 Loss 0.6695 Accuracy 0.7712\n",
      "Epoch 134 Batch 1050 Loss 0.6702 Accuracy 0.7708\n",
      "Epoch 134 Batch 1100 Loss 0.6707 Accuracy 0.7706\n",
      "Epoch 134 Batch 1150 Loss 0.6712 Accuracy 0.7706\n",
      "Epoch 134 Batch 1200 Loss 0.6715 Accuracy 0.7705\n",
      "Epoch 134 Batch 1250 Loss 0.6716 Accuracy 0.7706\n",
      "Epoch 134 Batch 1300 Loss 0.6717 Accuracy 0.7707\n",
      "Epoch 134 Batch 1350 Loss 0.6716 Accuracy 0.7707\n",
      "Epoch 134 Batch 1400 Loss 0.6715 Accuracy 0.7708\n",
      "Epoch 134 Batch 1450 Loss 0.6713 Accuracy 0.7708\n",
      "Epoch 134 Batch 1500 Loss 0.6715 Accuracy 0.7707\n",
      "Epoch 134 Loss 0.6717 Accuracy 0.7706\n",
      "Time taken for 1 epoch: 37.75251007080078 secs\n",
      "\n",
      "epoch lasted: 37.75756549835205\n",
      "Epoch 135 Batch 0 Loss 0.5969 Accuracy 0.8040\n",
      "Epoch 135 Batch 50 Loss 0.6586 Accuracy 0.7736\n",
      "Epoch 135 Batch 100 Loss 0.6639 Accuracy 0.7735\n",
      "Epoch 135 Batch 150 Loss 0.6652 Accuracy 0.7730\n",
      "Epoch 135 Batch 200 Loss 0.6646 Accuracy 0.7732\n",
      "Epoch 135 Batch 250 Loss 0.6645 Accuracy 0.7730\n",
      "Epoch 135 Batch 300 Loss 0.6643 Accuracy 0.7725\n",
      "Epoch 135 Batch 350 Loss 0.6647 Accuracy 0.7722\n",
      "Epoch 135 Batch 400 Loss 0.6643 Accuracy 0.7726\n",
      "Epoch 135 Batch 450 Loss 0.6659 Accuracy 0.7720\n",
      "Epoch 135 Batch 500 Loss 0.6665 Accuracy 0.7719\n",
      "Epoch 135 Batch 550 Loss 0.6673 Accuracy 0.7717\n",
      "Epoch 135 Batch 600 Loss 0.6671 Accuracy 0.7718\n",
      "Epoch 135 Batch 650 Loss 0.6681 Accuracy 0.7716\n",
      "Epoch 135 Batch 700 Loss 0.6682 Accuracy 0.7717\n",
      "Epoch 135 Batch 750 Loss 0.6681 Accuracy 0.7715\n",
      "Epoch 135 Batch 800 Loss 0.6683 Accuracy 0.7714\n",
      "Epoch 135 Batch 850 Loss 0.6685 Accuracy 0.7714\n",
      "Epoch 135 Batch 900 Loss 0.6688 Accuracy 0.7714\n",
      "Epoch 135 Batch 950 Loss 0.6685 Accuracy 0.7714\n",
      "Epoch 135 Batch 1000 Loss 0.6681 Accuracy 0.7716\n",
      "Epoch 135 Batch 1050 Loss 0.6684 Accuracy 0.7715\n",
      "discarded batch 1062\n",
      "Epoch 135 Batch 1100 Loss 0.6693 Accuracy 0.7714\n",
      "Epoch 135 Batch 1150 Loss 0.6696 Accuracy 0.7713\n",
      "Epoch 135 Batch 1200 Loss 0.6696 Accuracy 0.7712\n",
      "Epoch 135 Batch 1250 Loss 0.6700 Accuracy 0.7711\n",
      "Epoch 135 Batch 1300 Loss 0.6705 Accuracy 0.7710\n",
      "Epoch 135 Batch 1350 Loss 0.6705 Accuracy 0.7710\n",
      "Epoch 135 Batch 1400 Loss 0.6703 Accuracy 0.7709\n",
      "Epoch 135 Batch 1450 Loss 0.6707 Accuracy 0.7707\n",
      "Epoch 135 Batch 1500 Loss 0.6711 Accuracy 0.7706\n",
      "Saving checkpoint for epoch 135 at ./checkpoints/train/ckpt-27\n",
      "Epoch 135 Loss 0.6710 Accuracy 0.7707\n",
      "Time taken for 1 epoch: 37.511393785476685 secs\n",
      "\n",
      "epoch lasted: 37.5144829750061\n",
      "Epoch 136 Batch 0 Loss 0.6528 Accuracy 0.7508\n",
      "Epoch 136 Batch 50 Loss 0.6581 Accuracy 0.7742\n",
      "Epoch 136 Batch 100 Loss 0.6631 Accuracy 0.7725\n",
      "Epoch 136 Batch 150 Loss 0.6664 Accuracy 0.7715\n",
      "Epoch 136 Batch 200 Loss 0.6656 Accuracy 0.7719\n",
      "Epoch 136 Batch 250 Loss 0.6659 Accuracy 0.7718\n",
      "Epoch 136 Batch 300 Loss 0.6652 Accuracy 0.7720\n",
      "Epoch 136 Batch 350 Loss 0.6658 Accuracy 0.7721\n",
      "Epoch 136 Batch 400 Loss 0.6648 Accuracy 0.7722\n",
      "Epoch 136 Batch 450 Loss 0.6654 Accuracy 0.7720\n",
      "Epoch 136 Batch 500 Loss 0.6659 Accuracy 0.7717\n",
      "Epoch 136 Batch 550 Loss 0.6650 Accuracy 0.7721\n",
      "Epoch 136 Batch 600 Loss 0.6654 Accuracy 0.7723\n",
      "Epoch 136 Batch 650 Loss 0.6654 Accuracy 0.7723\n",
      "Epoch 136 Batch 700 Loss 0.6662 Accuracy 0.7720\n",
      "Epoch 136 Batch 750 Loss 0.6667 Accuracy 0.7719\n",
      "Epoch 136 Batch 800 Loss 0.6670 Accuracy 0.7720\n",
      "Epoch 136 Batch 850 Loss 0.6665 Accuracy 0.7722\n",
      "discarded batch 895\n",
      "Epoch 136 Batch 900 Loss 0.6668 Accuracy 0.7720\n",
      "Epoch 136 Batch 950 Loss 0.6670 Accuracy 0.7719\n",
      "Epoch 136 Batch 1000 Loss 0.6678 Accuracy 0.7716\n",
      "Epoch 136 Batch 1050 Loss 0.6681 Accuracy 0.7715\n",
      "Epoch 136 Batch 1100 Loss 0.6681 Accuracy 0.7715\n",
      "Epoch 136 Batch 1150 Loss 0.6682 Accuracy 0.7716\n",
      "Epoch 136 Batch 1200 Loss 0.6687 Accuracy 0.7715\n",
      "Epoch 136 Batch 1250 Loss 0.6687 Accuracy 0.7715\n",
      "Epoch 136 Batch 1300 Loss 0.6694 Accuracy 0.7712\n",
      "Epoch 136 Batch 1350 Loss 0.6695 Accuracy 0.7711\n",
      "Epoch 136 Batch 1400 Loss 0.6696 Accuracy 0.7712\n",
      "Epoch 136 Batch 1450 Loss 0.6699 Accuracy 0.7711\n",
      "Epoch 136 Batch 1500 Loss 0.6703 Accuracy 0.7710\n",
      "Epoch 136 Loss 0.6705 Accuracy 0.7709\n",
      "Time taken for 1 epoch: 36.77536058425903 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 136 VALIDATION: Loss 0.7326 Accuracy 0.7550\n",
      "\n",
      "epoch lasted: 36.923177003860474\n",
      "Epoch 137 Batch 0 Loss 0.6246 Accuracy 0.7741\n",
      "Epoch 137 Batch 50 Loss 0.6669 Accuracy 0.7738\n",
      "Epoch 137 Batch 100 Loss 0.6602 Accuracy 0.7749\n",
      "Epoch 137 Batch 150 Loss 0.6652 Accuracy 0.7727\n",
      "Epoch 137 Batch 200 Loss 0.6682 Accuracy 0.7715\n",
      "Epoch 137 Batch 250 Loss 0.6687 Accuracy 0.7709\n",
      "Epoch 137 Batch 300 Loss 0.6689 Accuracy 0.7711\n",
      "Epoch 137 Batch 350 Loss 0.6686 Accuracy 0.7713\n",
      "Epoch 137 Batch 400 Loss 0.6682 Accuracy 0.7716\n",
      "Epoch 137 Batch 450 Loss 0.6685 Accuracy 0.7716\n",
      "Epoch 137 Batch 500 Loss 0.6687 Accuracy 0.7715\n",
      "Epoch 137 Batch 550 Loss 0.6681 Accuracy 0.7717\n",
      "Epoch 137 Batch 600 Loss 0.6673 Accuracy 0.7718\n",
      "Epoch 137 Batch 650 Loss 0.6671 Accuracy 0.7720\n",
      "Epoch 137 Batch 700 Loss 0.6678 Accuracy 0.7717\n",
      "Epoch 137 Batch 750 Loss 0.6680 Accuracy 0.7717\n",
      "Epoch 137 Batch 800 Loss 0.6674 Accuracy 0.7718\n",
      "Epoch 137 Batch 850 Loss 0.6673 Accuracy 0.7719\n",
      "Epoch 137 Batch 900 Loss 0.6675 Accuracy 0.7720\n",
      "Epoch 137 Batch 950 Loss 0.6679 Accuracy 0.7718\n",
      "Epoch 137 Batch 1000 Loss 0.6679 Accuracy 0.7719\n",
      "Epoch 137 Batch 1050 Loss 0.6679 Accuracy 0.7718\n",
      "Epoch 137 Batch 1100 Loss 0.6683 Accuracy 0.7717\n",
      "discarded batch 1137\n",
      "Epoch 137 Batch 1150 Loss 0.6684 Accuracy 0.7716\n",
      "Epoch 137 Batch 1200 Loss 0.6682 Accuracy 0.7717\n",
      "Epoch 137 Batch 1250 Loss 0.6688 Accuracy 0.7716\n",
      "Epoch 137 Batch 1300 Loss 0.6687 Accuracy 0.7715\n",
      "Epoch 137 Batch 1350 Loss 0.6688 Accuracy 0.7714\n",
      "Epoch 137 Batch 1400 Loss 0.6693 Accuracy 0.7713\n",
      "Epoch 137 Batch 1450 Loss 0.6696 Accuracy 0.7711\n",
      "Epoch 137 Batch 1500 Loss 0.6698 Accuracy 0.7711\n",
      "Epoch 137 Loss 0.6701 Accuracy 0.7709\n",
      "Time taken for 1 epoch: 37.13090419769287 secs\n",
      "\n",
      "epoch lasted: 37.13435697555542\n",
      "Epoch 138 Batch 0 Loss 0.6243 Accuracy 0.7924\n",
      "Epoch 138 Batch 50 Loss 0.6436 Accuracy 0.7799\n",
      "Epoch 138 Batch 100 Loss 0.6578 Accuracy 0.7744\n",
      "Epoch 138 Batch 150 Loss 0.6571 Accuracy 0.7748\n",
      "Epoch 138 Batch 200 Loss 0.6551 Accuracy 0.7756\n",
      "Epoch 138 Batch 250 Loss 0.6595 Accuracy 0.7737\n",
      "Epoch 138 Batch 300 Loss 0.6605 Accuracy 0.7737\n",
      "Epoch 138 Batch 350 Loss 0.6619 Accuracy 0.7730\n",
      "Epoch 138 Batch 400 Loss 0.6631 Accuracy 0.7728\n",
      "Epoch 138 Batch 450 Loss 0.6629 Accuracy 0.7727\n",
      "Epoch 138 Batch 500 Loss 0.6640 Accuracy 0.7728\n",
      "discarded batch 546\n",
      "Epoch 138 Batch 550 Loss 0.6640 Accuracy 0.7728\n",
      "Epoch 138 Batch 600 Loss 0.6637 Accuracy 0.7730\n",
      "Epoch 138 Batch 650 Loss 0.6644 Accuracy 0.7728\n",
      "Epoch 138 Batch 700 Loss 0.6650 Accuracy 0.7725\n",
      "Epoch 138 Batch 750 Loss 0.6653 Accuracy 0.7724\n",
      "Epoch 138 Batch 800 Loss 0.6656 Accuracy 0.7724\n",
      "Epoch 138 Batch 850 Loss 0.6655 Accuracy 0.7723\n",
      "Epoch 138 Batch 900 Loss 0.6659 Accuracy 0.7721\n",
      "Epoch 138 Batch 950 Loss 0.6661 Accuracy 0.7719\n",
      "Epoch 138 Batch 1000 Loss 0.6663 Accuracy 0.7719\n",
      "Epoch 138 Batch 1050 Loss 0.6669 Accuracy 0.7716\n",
      "Epoch 138 Batch 1100 Loss 0.6669 Accuracy 0.7717\n",
      "Epoch 138 Batch 1150 Loss 0.6667 Accuracy 0.7718\n",
      "Epoch 138 Batch 1200 Loss 0.6674 Accuracy 0.7717\n",
      "Epoch 138 Batch 1250 Loss 0.6675 Accuracy 0.7716\n",
      "Epoch 138 Batch 1300 Loss 0.6679 Accuracy 0.7715\n",
      "Epoch 138 Batch 1350 Loss 0.6680 Accuracy 0.7715\n",
      "Epoch 138 Batch 1400 Loss 0.6687 Accuracy 0.7713\n",
      "Epoch 138 Batch 1450 Loss 0.6687 Accuracy 0.7713\n",
      "Epoch 138 Batch 1500 Loss 0.6686 Accuracy 0.7714\n",
      "Epoch 138 Loss 0.6688 Accuracy 0.7714\n",
      "Time taken for 1 epoch: 36.80189061164856 secs\n",
      "\n",
      "epoch lasted: 36.806368589401245\n",
      "Epoch 139 Batch 0 Loss 0.8224 Accuracy 0.7159\n",
      "Epoch 139 Batch 50 Loss 0.6641 Accuracy 0.7735\n",
      "Epoch 139 Batch 100 Loss 0.6626 Accuracy 0.7730\n",
      "Epoch 139 Batch 150 Loss 0.6608 Accuracy 0.7734\n",
      "Epoch 139 Batch 200 Loss 0.6599 Accuracy 0.7739\n",
      "Epoch 139 Batch 250 Loss 0.6605 Accuracy 0.7740\n",
      "Epoch 139 Batch 300 Loss 0.6609 Accuracy 0.7742\n",
      "discarded batch 309\n",
      "Epoch 139 Batch 350 Loss 0.6596 Accuracy 0.7744\n",
      "Epoch 139 Batch 400 Loss 0.6603 Accuracy 0.7741\n",
      "Epoch 139 Batch 450 Loss 0.6620 Accuracy 0.7738\n",
      "Epoch 139 Batch 500 Loss 0.6615 Accuracy 0.7740\n",
      "Epoch 139 Batch 550 Loss 0.6622 Accuracy 0.7738\n",
      "Epoch 139 Batch 600 Loss 0.6626 Accuracy 0.7737\n",
      "Epoch 139 Batch 650 Loss 0.6627 Accuracy 0.7735\n",
      "Epoch 139 Batch 700 Loss 0.6632 Accuracy 0.7733\n",
      "Epoch 139 Batch 750 Loss 0.6639 Accuracy 0.7730\n",
      "Epoch 139 Batch 800 Loss 0.6639 Accuracy 0.7728\n",
      "Epoch 139 Batch 850 Loss 0.6644 Accuracy 0.7728\n",
      "Epoch 139 Batch 900 Loss 0.6645 Accuracy 0.7728\n",
      "Epoch 139 Batch 950 Loss 0.6646 Accuracy 0.7727\n",
      "Epoch 139 Batch 1000 Loss 0.6650 Accuracy 0.7726\n",
      "Epoch 139 Batch 1050 Loss 0.6653 Accuracy 0.7726\n",
      "Epoch 139 Batch 1100 Loss 0.6657 Accuracy 0.7725\n",
      "Epoch 139 Batch 1150 Loss 0.6662 Accuracy 0.7724\n",
      "Epoch 139 Batch 1200 Loss 0.6670 Accuracy 0.7722\n",
      "Epoch 139 Batch 1250 Loss 0.6670 Accuracy 0.7722\n",
      "Epoch 139 Batch 1300 Loss 0.6673 Accuracy 0.7721\n",
      "Epoch 139 Batch 1350 Loss 0.6678 Accuracy 0.7719\n",
      "Epoch 139 Batch 1400 Loss 0.6676 Accuracy 0.7719\n",
      "Epoch 139 Batch 1450 Loss 0.6679 Accuracy 0.7718\n",
      "Epoch 139 Batch 1500 Loss 0.6679 Accuracy 0.7718\n",
      "Epoch 139 Loss 0.6683 Accuracy 0.7716\n",
      "Time taken for 1 epoch: 36.98121428489685 secs\n",
      "\n",
      "epoch lasted: 36.98563885688782\n",
      "Epoch 140 Batch 0 Loss 0.6039 Accuracy 0.7841\n",
      "Epoch 140 Batch 50 Loss 0.6522 Accuracy 0.7750\n",
      "Epoch 140 Batch 100 Loss 0.6592 Accuracy 0.7747\n",
      "Epoch 140 Batch 150 Loss 0.6621 Accuracy 0.7742\n",
      "Epoch 140 Batch 200 Loss 0.6628 Accuracy 0.7742\n",
      "Epoch 140 Batch 250 Loss 0.6637 Accuracy 0.7736\n",
      "Epoch 140 Batch 300 Loss 0.6646 Accuracy 0.7733\n",
      "Epoch 140 Batch 350 Loss 0.6648 Accuracy 0.7732\n",
      "Epoch 140 Batch 400 Loss 0.6655 Accuracy 0.7729\n",
      "Epoch 140 Batch 450 Loss 0.6662 Accuracy 0.7726\n",
      "Epoch 140 Batch 500 Loss 0.6660 Accuracy 0.7726\n",
      "Epoch 140 Batch 550 Loss 0.6667 Accuracy 0.7723\n",
      "Epoch 140 Batch 600 Loss 0.6668 Accuracy 0.7722\n",
      "Epoch 140 Batch 650 Loss 0.6666 Accuracy 0.7721\n",
      "Epoch 140 Batch 700 Loss 0.6670 Accuracy 0.7720\n",
      "Epoch 140 Batch 750 Loss 0.6662 Accuracy 0.7723\n",
      "Epoch 140 Batch 800 Loss 0.6668 Accuracy 0.7722\n",
      "Epoch 140 Batch 850 Loss 0.6674 Accuracy 0.7720\n",
      "Epoch 140 Batch 900 Loss 0.6675 Accuracy 0.7719\n",
      "Epoch 140 Batch 950 Loss 0.6675 Accuracy 0.7719\n",
      "Epoch 140 Batch 1000 Loss 0.6678 Accuracy 0.7718\n",
      "Epoch 140 Batch 1050 Loss 0.6676 Accuracy 0.7718\n",
      "Epoch 140 Batch 1100 Loss 0.6674 Accuracy 0.7719\n",
      "Epoch 140 Batch 1150 Loss 0.6672 Accuracy 0.7720\n",
      "Epoch 140 Batch 1200 Loss 0.6672 Accuracy 0.7720\n",
      "Epoch 140 Batch 1250 Loss 0.6672 Accuracy 0.7719\n",
      "Epoch 140 Batch 1300 Loss 0.6669 Accuracy 0.7721\n",
      "Epoch 140 Batch 1350 Loss 0.6669 Accuracy 0.7720\n",
      "Epoch 140 Batch 1400 Loss 0.6668 Accuracy 0.7721\n",
      "Epoch 140 Batch 1450 Loss 0.6671 Accuracy 0.7720\n",
      "discarded batch 1464\n",
      "Epoch 140 Batch 1500 Loss 0.6672 Accuracy 0.7720\n",
      "Saving checkpoint for epoch 140 at ./checkpoints/train/ckpt-28\n",
      "Epoch 140 Loss 0.6676 Accuracy 0.7719\n",
      "Time taken for 1 epoch: 37.16484975814819 secs\n",
      "\n",
      "epoch lasted: 37.169456243515015\n",
      "Epoch 141 Batch 0 Loss 0.6226 Accuracy 0.7957\n",
      "Epoch 141 Batch 50 Loss 0.6618 Accuracy 0.7719\n",
      "Epoch 141 Batch 100 Loss 0.6642 Accuracy 0.7713\n",
      "Epoch 141 Batch 150 Loss 0.6674 Accuracy 0.7710\n",
      "Epoch 141 Batch 200 Loss 0.6624 Accuracy 0.7731\n",
      "Epoch 141 Batch 250 Loss 0.6614 Accuracy 0.7736\n",
      "Epoch 141 Batch 300 Loss 0.6622 Accuracy 0.7738\n",
      "Epoch 141 Batch 350 Loss 0.6634 Accuracy 0.7735\n",
      "Epoch 141 Batch 400 Loss 0.6636 Accuracy 0.7734\n",
      "Epoch 141 Batch 450 Loss 0.6637 Accuracy 0.7734\n",
      "Epoch 141 Batch 500 Loss 0.6626 Accuracy 0.7740\n",
      "Epoch 141 Batch 550 Loss 0.6633 Accuracy 0.7736\n",
      "Epoch 141 Batch 600 Loss 0.6642 Accuracy 0.7734\n",
      "Epoch 141 Batch 650 Loss 0.6642 Accuracy 0.7733\n",
      "Epoch 141 Batch 700 Loss 0.6634 Accuracy 0.7736\n",
      "Epoch 141 Batch 750 Loss 0.6634 Accuracy 0.7737\n",
      "Epoch 141 Batch 800 Loss 0.6638 Accuracy 0.7736\n",
      "Epoch 141 Batch 850 Loss 0.6648 Accuracy 0.7731\n",
      "Epoch 141 Batch 900 Loss 0.6647 Accuracy 0.7732\n",
      "Epoch 141 Batch 950 Loss 0.6651 Accuracy 0.7731\n",
      "Epoch 141 Batch 1000 Loss 0.6652 Accuracy 0.7731\n",
      "Epoch 141 Batch 1050 Loss 0.6652 Accuracy 0.7731\n",
      "discarded batch 1072\n",
      "Epoch 141 Batch 1100 Loss 0.6651 Accuracy 0.7732\n",
      "Epoch 141 Batch 1150 Loss 0.6655 Accuracy 0.7730\n",
      "Epoch 141 Batch 1200 Loss 0.6653 Accuracy 0.7730\n",
      "Epoch 141 Batch 1250 Loss 0.6657 Accuracy 0.7729\n",
      "Epoch 141 Batch 1300 Loss 0.6663 Accuracy 0.7726\n",
      "Epoch 141 Batch 1350 Loss 0.6666 Accuracy 0.7726\n",
      "Epoch 141 Batch 1400 Loss 0.6665 Accuracy 0.7726\n",
      "Epoch 141 Batch 1450 Loss 0.6669 Accuracy 0.7724\n",
      "Epoch 141 Batch 1500 Loss 0.6672 Accuracy 0.7724\n",
      "Epoch 141 Loss 0.6673 Accuracy 0.7723\n",
      "Time taken for 1 epoch: 36.90901708602905 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 141 VALIDATION: Loss 0.7349 Accuracy 0.7607\n",
      "\n",
      "la | tua | be| ni| gni| tà | non | pur | soc| cor| re|\n",
      "a | chi | do| man| da | ma | mol| te | fï| a| te|\n",
      "li| be| ra| men| te al | di| man| dar | pre| cor| re|\n",
      "                                        \n",
      " ch’ io | ma| e| stro | mi | par| lai | di | ma| la| te|\n",
      "co| sì | com’ | io | do| vrrai | co| lui | che | so| gli|\n",
      "co| me | co| me | co| sì | co| me | si | sca| te|\n",
      "                                    \n",
      " co| me | si | co| lui | che | so| lo | co| sa| ta|\n",
      "la | mia | mia | ma| ni| fe| sta | ma| ta| glia|\n",
      "co| sì | co| me | sa| li| ce|\n",
      "che | so| lo a | l’ a| ma| ta|\n",
      " che | so| lo a | lui | che | per | lo | suo | s’ a| scon| do|\n",
      "co| sì | di | mo| do | co| lui | che | s’ a| sca| la|\n",
      "e | co| me | si | vol| se | co| sì | la | scon| ca|\n",
      "                                \n",
      " co| sì | di| sce| sa | ma| la| de a | la | ga| la|\n",
      "a | la | mia | ma| ni| fe| sta | co| stui | pa| ta|\n",
      "la | mia | mi| se | co| sì | com’ | io | di| ces| si|\n",
      " che | se | la | mia | ma| la| de | so| lo | schio|\n",
      "la | mia | me| stie| ra | ch’ io | mi | par| lai | mo| sco|\n",
      "co| sì | mi | par| lai | con | l’ a| ni| ma | so|\n",
      "                                       \n",
      " co| me | si | mos| se | co| lui | che | so| glio|\n",
      "co| me | co| sì | co| sì | com’ | io | do| vei|\n",
      "co| me | si | ri| spuo| se|\n",
      "co| me | sa| rai | co| sì | fo| co|\n",
      " co|\n",
      "e | la | mia | co| me | co| me | so| lo in | ca| sa|\n",
      "co| sì | co| me | con | l’ a| ma | co| sì | so| la|\n",
      "e | co| sì | l’ a| ni| ma | co| sì | dol| ce| si|\n",
      "                                         \n",
      " co| me | si | co| lui | che | so| lo a | la | go| la|\n",
      "co| sì | com’ | io | di| ce| de| smo | co| sa| ra|\n",
      "la | mia | mi| se | co| me | co| sì | tor| na| ro|\n",
      "  \n",
      " co|\n",
      "e | se | la | mia | co| sì | dol| ci | di| man| da|\n",
      "co| me | co| sto| la | co| sì | co| sì | com’ | io|\n",
      "co| me | sa| rà | dol| ci | sa| rai | la | man| ca|\n",
      "                                        \n",
      " co| me | l’ a| ni| ma | co| sì | di | ma| na| ta| ta|\n",
      "co| sì | com’ | io | di| ce| rai | la | man | ca| ga|\n",
      "che | l’ a| ni| ma | co| sì | dol| ce | di | ma| ta|\n",
      " che | se | la | mia | do| ve | so| lo | s’ a| scon| da|\n",
      "co| me | se | tu | che | sa| rai | con| te| mi| glia|\n",
      "co| sì | la | mia | ma| ni| fe| sti| na| ta| ta|\n",
      "                                         \n",
      " co| sì | la | mia | do| ve | so| lo | sco| glia| ti|\n",
      "co| sì | co| me | se | tu | co| sì | ve| drai|\n",
      "e | co| me | sa| rai|\n",
      "co| sì | co| me | se | tu | se’ | tuoi|\n",
      " che | la | mia | me| na | mia | ma| na | mia | re| sto|\n",
      "co| me | co| stor | mi | fa| rà | di | sua | me| stu| ca|\n",
      "co| sì | co| me | co| sì | co| me | con| ve| sto|\n",
      "                                    \n",
      " co| me | se| gui| ta| men| te | mi | sco| glia| ti|\n",
      "la | mia | mi| se| ria | co| stui | co| stui | co| sto|\n",
      "co| me | si | mo| vea | sco| glio| ne a | la | gra| tia|\n",
      " co|\n",
      "e | se | la | mia | co| sì | co| me | s’ a| scoi|\n",
      "co| me | se | tu | ch’ al | mio | do| vria | mi | schio| glio|\n",
      "co| sì | co| me | sa| rà | dol| ce | si | moi|\n",
      "                                    \n",
      " co| me | sa| rai | co| sa | di| cen| do | so| glio|\n",
      "co| me | se | tu | co| stai | co| sì | di| sco| glio|\n",
      "che | la | mia | ma| na | mia | ma| no a | lo | sco| glio|\n",
      " e | io|\n",
      "la | mia | ma| do| na| ta | che | s’ a| sco| sai|\n",
      "co| me | co| stui | co| stui | con| vien | ch’ a| ma| no|\n",
      "co| sì | co| me | co| sì | co| me | si | sa| lo|\n",
      "                                   \n",
      " co| me | se| gui| ta| men| te | se| gui| ta| no|\n",
      "co| sì | com’ | io | vi| di | co| lui | che | sa| no|\n",
      "se | tu | co| sì | co| me | se | tu | co| sa| no|\n",
      "  \n",
      "ed | el| li a | me | con | l’ al| to | mi| ra| vi| glio|\n",
      "che | l’ al| to | mio | a | la | mia | men| te | sco| se|\n",
      "co| me | se | tu | ma| ni a | la | mia | ma| li| | glio|\n",
      " che | se | la | mia | ma| la| de | so| lo | sca| la|\n",
      "co| me | se | tu | ch’ io | ma | tua | ma| ni| fa| mia|\n",
      "co| sì | m’ a| vea | la | mia | ma| na| na | sa| la|\n",
      "                                      \n",
      " co| sì | dol| ce | co| me | sa| rai | co| sa| mia|\n",
      "che | se’ | mi | se| gui| tai | co| tan| do | sa| la|\n",
      "co| me | si | par| tiii | co| me | se | non | sa| lia|\n",
      " e | co| me | sa|\n",
      "co| me | sa| rai | ch’ al | mio | a| stis| se|\n",
      "co| me | se | tu | ch’ al | mon| do | sa| ria | mai|\n",
      "ed | el| li a | mo| ri | co| sì | co| me | sciol| to|\n",
      "                             \n",
      " co| me | la | mia | ma| ni| fe| sta | mi | scai|\n",
      "co| me | co| sì | co| sì | com’ | io | ma| e| sto|\n",
      "ma | per | lo | sco| glio| sa| to | di| scor| di| no|\n",
      "\n",
      " co| me | se | la | mia | me| na | co| sta | mo| sta|\n",
      "co| sì | co| me | se | tu | con | l’ a| ni| ma| glio|\n",
      "co| sì | co| me | sa| rà | dol| ce | sa| li| no|\n",
      "                                           \n",
      " co| me | sa| rai | co| stui | co| sì | voi | sa| glio|\n",
      "co| sì | co| me | se | tu | co| sì | vi | sa| li|\n",
      "co| me | se | tu | ch’ al | mon| do | sa| rai | pa| ga|\n",
      " che | se | la | mia | ma| na| ta | m’ a| vea | mo| col| le|\n",
      "co| me | sa| rei | che | so| lo | sa| rai | con| to|\n",
      "co| sì | la | mia | ma| na| na | dol| ce | scol| le|\n",
      "                                  \n",
      " co| me | sa| rai | co| sa | vo| glia | co| san| na|\n",
      "che | l’ a| ni| ma | dol| ce | da | la | sua | mol| le|\n",
      "che | l’ a| ma| te| ra | sco| glio| ne a | la | gol| la|\n",
      " ed|\n",
      "oi | che | se | di | mo| do | mi | si | ri| spo| sa|\n",
      "co| me | se | tu | co| stor | mi | fa | me | so| la|\n",
      "co| sì | mi | fa | ch’ al | mio | ma| e| stro | mo| sa|\n",
      "                                  \n",
      " co| me | la | mia | ma| ni| fe| sta | mia | mo| da|\n",
      "e | co| sì | co| sì | mi | fa| cea | mi | par| ta|\n",
      "la | mia | mi| se| ria | mia | me| stie| ri | sco| glia|\n",
      "\n",
      "epoch lasted: 735.3725023269653\n",
      "Epoch 142 Batch 0 Loss 0.7179 Accuracy 0.7674\n",
      "Epoch 142 Batch 50 Loss 0.6644 Accuracy 0.7719\n",
      "Epoch 142 Batch 100 Loss 0.6614 Accuracy 0.7729\n",
      "Epoch 142 Batch 150 Loss 0.6607 Accuracy 0.7733\n",
      "Epoch 142 Batch 200 Loss 0.6603 Accuracy 0.7737\n",
      "Epoch 142 Batch 250 Loss 0.6596 Accuracy 0.7743\n",
      "Epoch 142 Batch 300 Loss 0.6600 Accuracy 0.7745\n",
      "Epoch 142 Batch 350 Loss 0.6602 Accuracy 0.7744\n",
      "Epoch 142 Batch 400 Loss 0.6616 Accuracy 0.7739\n",
      "Epoch 142 Batch 450 Loss 0.6614 Accuracy 0.7740\n",
      "discarded batch 460\n",
      "Epoch 142 Batch 500 Loss 0.6609 Accuracy 0.7743\n",
      "Epoch 142 Batch 550 Loss 0.6617 Accuracy 0.7740\n",
      "Epoch 142 Batch 600 Loss 0.6627 Accuracy 0.7735\n",
      "Epoch 142 Batch 650 Loss 0.6634 Accuracy 0.7732\n",
      "Epoch 142 Batch 700 Loss 0.6634 Accuracy 0.7734\n",
      "Epoch 142 Batch 750 Loss 0.6639 Accuracy 0.7733\n",
      "Epoch 142 Batch 800 Loss 0.6641 Accuracy 0.7734\n",
      "Epoch 142 Batch 850 Loss 0.6640 Accuracy 0.7735\n",
      "Epoch 142 Batch 900 Loss 0.6638 Accuracy 0.7737\n",
      "Epoch 142 Batch 950 Loss 0.6637 Accuracy 0.7739\n",
      "Epoch 142 Batch 1000 Loss 0.6638 Accuracy 0.7738\n",
      "Epoch 142 Batch 1050 Loss 0.6643 Accuracy 0.7737\n",
      "Epoch 142 Batch 1100 Loss 0.6643 Accuracy 0.7736\n",
      "Epoch 142 Batch 1150 Loss 0.6646 Accuracy 0.7734\n",
      "Epoch 142 Batch 1200 Loss 0.6650 Accuracy 0.7733\n",
      "Epoch 142 Batch 1250 Loss 0.6653 Accuracy 0.7732\n",
      "Epoch 142 Batch 1300 Loss 0.6652 Accuracy 0.7733\n",
      "Epoch 142 Batch 1350 Loss 0.6654 Accuracy 0.7732\n",
      "Epoch 142 Batch 1400 Loss 0.6658 Accuracy 0.7731\n",
      "Epoch 142 Batch 1450 Loss 0.6655 Accuracy 0.7732\n",
      "Epoch 142 Batch 1500 Loss 0.6660 Accuracy 0.7730\n",
      "Epoch 142 Loss 0.6665 Accuracy 0.7728\n",
      "Time taken for 1 epoch: 37.896236419677734 secs\n",
      "\n",
      "epoch lasted: 37.900078773498535\n",
      "Epoch 143 Batch 0 Loss 0.6402 Accuracy 0.7791\n",
      "Epoch 143 Batch 50 Loss 0.6545 Accuracy 0.7780\n",
      "Epoch 143 Batch 100 Loss 0.6610 Accuracy 0.7757\n",
      "Epoch 143 Batch 150 Loss 0.6571 Accuracy 0.7765\n",
      "Epoch 143 Batch 200 Loss 0.6577 Accuracy 0.7765\n",
      "Epoch 143 Batch 250 Loss 0.6604 Accuracy 0.7758\n",
      "Epoch 143 Batch 300 Loss 0.6598 Accuracy 0.7760\n",
      "Epoch 143 Batch 350 Loss 0.6598 Accuracy 0.7758\n",
      "Epoch 143 Batch 400 Loss 0.6599 Accuracy 0.7756\n",
      "Epoch 143 Batch 450 Loss 0.6596 Accuracy 0.7756\n",
      "discarded batch 486\n",
      "Epoch 143 Batch 500 Loss 0.6605 Accuracy 0.7753\n",
      "Epoch 143 Batch 550 Loss 0.6608 Accuracy 0.7748\n",
      "Epoch 143 Batch 600 Loss 0.6613 Accuracy 0.7746\n",
      "Epoch 143 Batch 650 Loss 0.6613 Accuracy 0.7744\n",
      "Epoch 143 Batch 700 Loss 0.6615 Accuracy 0.7745\n",
      "Epoch 143 Batch 750 Loss 0.6618 Accuracy 0.7743\n",
      "Epoch 143 Batch 800 Loss 0.6624 Accuracy 0.7740\n",
      "Epoch 143 Batch 850 Loss 0.6630 Accuracy 0.7737\n",
      "Epoch 143 Batch 900 Loss 0.6631 Accuracy 0.7736\n",
      "Epoch 143 Batch 950 Loss 0.6633 Accuracy 0.7735\n",
      "Epoch 143 Batch 1000 Loss 0.6636 Accuracy 0.7733\n",
      "Epoch 143 Batch 1050 Loss 0.6640 Accuracy 0.7731\n",
      "Epoch 143 Batch 1100 Loss 0.6638 Accuracy 0.7733\n",
      "Epoch 143 Batch 1150 Loss 0.6638 Accuracy 0.7733\n",
      "Epoch 143 Batch 1200 Loss 0.6643 Accuracy 0.7731\n",
      "Epoch 143 Batch 1250 Loss 0.6644 Accuracy 0.7731\n",
      "Epoch 143 Batch 1300 Loss 0.6646 Accuracy 0.7731\n",
      "Epoch 143 Batch 1350 Loss 0.6648 Accuracy 0.7730\n",
      "Epoch 143 Batch 1400 Loss 0.6652 Accuracy 0.7729\n",
      "Epoch 143 Batch 1450 Loss 0.6652 Accuracy 0.7729\n",
      "Epoch 143 Batch 1500 Loss 0.6656 Accuracy 0.7728\n",
      "Epoch 143 Loss 0.6655 Accuracy 0.7728\n",
      "Time taken for 1 epoch: 37.4380156993866 secs\n",
      "\n",
      "epoch lasted: 37.441569805145264\n",
      "Epoch 144 Batch 0 Loss 0.5867 Accuracy 0.7973\n",
      "Epoch 144 Batch 50 Loss 0.6585 Accuracy 0.7728\n",
      "Epoch 144 Batch 100 Loss 0.6606 Accuracy 0.7730\n",
      "Epoch 144 Batch 150 Loss 0.6640 Accuracy 0.7719\n",
      "Epoch 144 Batch 200 Loss 0.6638 Accuracy 0.7720\n",
      "Epoch 144 Batch 250 Loss 0.6626 Accuracy 0.7724\n",
      "Epoch 144 Batch 300 Loss 0.6627 Accuracy 0.7725\n",
      "Epoch 144 Batch 350 Loss 0.6625 Accuracy 0.7724\n",
      "Epoch 144 Batch 400 Loss 0.6617 Accuracy 0.7726\n",
      "Epoch 144 Batch 450 Loss 0.6621 Accuracy 0.7728\n",
      "Epoch 144 Batch 500 Loss 0.6622 Accuracy 0.7728\n",
      "Epoch 144 Batch 550 Loss 0.6628 Accuracy 0.7725\n",
      "Epoch 144 Batch 600 Loss 0.6634 Accuracy 0.7724\n",
      "Epoch 144 Batch 650 Loss 0.6637 Accuracy 0.7725\n",
      "discarded batch 699\n",
      "Epoch 144 Batch 700 Loss 0.6642 Accuracy 0.7724\n",
      "Epoch 144 Batch 750 Loss 0.6631 Accuracy 0.7729\n",
      "Epoch 144 Batch 800 Loss 0.6636 Accuracy 0.7727\n",
      "Epoch 144 Batch 850 Loss 0.6634 Accuracy 0.7728\n",
      "Epoch 144 Batch 900 Loss 0.6636 Accuracy 0.7728\n",
      "Epoch 144 Batch 950 Loss 0.6637 Accuracy 0.7728\n",
      "Epoch 144 Batch 1000 Loss 0.6641 Accuracy 0.7727\n",
      "Epoch 144 Batch 1050 Loss 0.6638 Accuracy 0.7728\n",
      "Epoch 144 Batch 1100 Loss 0.6640 Accuracy 0.7728\n",
      "Epoch 144 Batch 1150 Loss 0.6641 Accuracy 0.7727\n",
      "Epoch 144 Batch 1200 Loss 0.6637 Accuracy 0.7729\n",
      "Epoch 144 Batch 1250 Loss 0.6636 Accuracy 0.7730\n",
      "Epoch 144 Batch 1300 Loss 0.6641 Accuracy 0.7730\n",
      "Epoch 144 Batch 1350 Loss 0.6643 Accuracy 0.7730\n",
      "Epoch 144 Batch 1400 Loss 0.6645 Accuracy 0.7729\n",
      "Epoch 144 Batch 1450 Loss 0.6647 Accuracy 0.7729\n",
      "Epoch 144 Batch 1500 Loss 0.6646 Accuracy 0.7729\n",
      "Epoch 144 Loss 0.6647 Accuracy 0.7729\n",
      "Time taken for 1 epoch: 37.53778648376465 secs\n",
      "\n",
      "epoch lasted: 37.54222846031189\n",
      "Epoch 145 Batch 0 Loss 0.6901 Accuracy 0.7658\n",
      "Epoch 145 Batch 50 Loss 0.6522 Accuracy 0.7772\n",
      "Epoch 145 Batch 100 Loss 0.6500 Accuracy 0.7780\n",
      "Epoch 145 Batch 150 Loss 0.6536 Accuracy 0.7769\n",
      "Epoch 145 Batch 200 Loss 0.6538 Accuracy 0.7773\n",
      "Epoch 145 Batch 250 Loss 0.6558 Accuracy 0.7769\n",
      "Epoch 145 Batch 300 Loss 0.6571 Accuracy 0.7762\n",
      "Epoch 145 Batch 350 Loss 0.6573 Accuracy 0.7761\n",
      "Epoch 145 Batch 400 Loss 0.6565 Accuracy 0.7764\n",
      "Epoch 145 Batch 450 Loss 0.6581 Accuracy 0.7757\n",
      "Epoch 145 Batch 500 Loss 0.6589 Accuracy 0.7754\n",
      "Epoch 145 Batch 550 Loss 0.6587 Accuracy 0.7754\n",
      "Epoch 145 Batch 600 Loss 0.6594 Accuracy 0.7750\n",
      "Epoch 145 Batch 650 Loss 0.6597 Accuracy 0.7751\n",
      "Epoch 145 Batch 700 Loss 0.6598 Accuracy 0.7750\n",
      "Epoch 145 Batch 750 Loss 0.6603 Accuracy 0.7747\n",
      "Epoch 145 Batch 800 Loss 0.6604 Accuracy 0.7747\n",
      "Epoch 145 Batch 850 Loss 0.6601 Accuracy 0.7748\n",
      "Epoch 145 Batch 900 Loss 0.6606 Accuracy 0.7745\n",
      "discarded batch 921\n",
      "Epoch 145 Batch 950 Loss 0.6610 Accuracy 0.7744\n",
      "Epoch 145 Batch 1000 Loss 0.6610 Accuracy 0.7744\n",
      "Epoch 145 Batch 1050 Loss 0.6611 Accuracy 0.7745\n",
      "Epoch 145 Batch 1100 Loss 0.6618 Accuracy 0.7742\n",
      "Epoch 145 Batch 1150 Loss 0.6628 Accuracy 0.7739\n",
      "Epoch 145 Batch 1200 Loss 0.6630 Accuracy 0.7738\n",
      "Epoch 145 Batch 1250 Loss 0.6634 Accuracy 0.7736\n",
      "Epoch 145 Batch 1300 Loss 0.6636 Accuracy 0.7736\n",
      "Epoch 145 Batch 1350 Loss 0.6636 Accuracy 0.7736\n",
      "Epoch 145 Batch 1400 Loss 0.6635 Accuracy 0.7736\n",
      "Epoch 145 Batch 1450 Loss 0.6637 Accuracy 0.7736\n",
      "Epoch 145 Batch 1500 Loss 0.6639 Accuracy 0.7735\n",
      "Saving checkpoint for epoch 145 at ./checkpoints/train/ckpt-29\n",
      "Epoch 145 Loss 0.6641 Accuracy 0.7735\n",
      "Time taken for 1 epoch: 37.38321781158447 secs\n",
      "\n",
      "epoch lasted: 37.38657283782959\n",
      "Epoch 146 Batch 0 Loss 0.6444 Accuracy 0.7641\n",
      "Epoch 146 Batch 50 Loss 0.6531 Accuracy 0.7748\n",
      "Epoch 146 Batch 100 Loss 0.6571 Accuracy 0.7733\n",
      "Epoch 146 Batch 150 Loss 0.6583 Accuracy 0.7740\n",
      "Epoch 146 Batch 200 Loss 0.6569 Accuracy 0.7745\n",
      "Epoch 146 Batch 250 Loss 0.6568 Accuracy 0.7750\n",
      "Epoch 146 Batch 300 Loss 0.6569 Accuracy 0.7751\n",
      "Epoch 146 Batch 350 Loss 0.6579 Accuracy 0.7748\n",
      "Epoch 146 Batch 400 Loss 0.6581 Accuracy 0.7750\n",
      "Epoch 146 Batch 450 Loss 0.6591 Accuracy 0.7747\n",
      "Epoch 146 Batch 500 Loss 0.6596 Accuracy 0.7745\n",
      "Epoch 146 Batch 550 Loss 0.6601 Accuracy 0.7744\n",
      "Epoch 146 Batch 600 Loss 0.6604 Accuracy 0.7741\n",
      "discarded batch 616\n",
      "Epoch 146 Batch 650 Loss 0.6607 Accuracy 0.7741\n",
      "Epoch 146 Batch 700 Loss 0.6615 Accuracy 0.7738\n",
      "Epoch 146 Batch 750 Loss 0.6608 Accuracy 0.7739\n",
      "Epoch 146 Batch 800 Loss 0.6620 Accuracy 0.7735\n",
      "Epoch 146 Batch 850 Loss 0.6622 Accuracy 0.7735\n",
      "Epoch 146 Batch 900 Loss 0.6625 Accuracy 0.7735\n",
      "Epoch 146 Batch 950 Loss 0.6628 Accuracy 0.7733\n",
      "Epoch 146 Batch 1000 Loss 0.6627 Accuracy 0.7734\n",
      "Epoch 146 Batch 1050 Loss 0.6628 Accuracy 0.7734\n",
      "Epoch 146 Batch 1100 Loss 0.6634 Accuracy 0.7732\n",
      "Epoch 146 Batch 1150 Loss 0.6631 Accuracy 0.7734\n",
      "Epoch 146 Batch 1200 Loss 0.6631 Accuracy 0.7734\n",
      "Epoch 146 Batch 1250 Loss 0.6633 Accuracy 0.7733\n",
      "Epoch 146 Batch 1300 Loss 0.6633 Accuracy 0.7733\n",
      "Epoch 146 Batch 1350 Loss 0.6634 Accuracy 0.7733\n",
      "Epoch 146 Batch 1400 Loss 0.6631 Accuracy 0.7734\n",
      "Epoch 146 Batch 1450 Loss 0.6631 Accuracy 0.7735\n",
      "Epoch 146 Batch 1500 Loss 0.6630 Accuracy 0.7735\n",
      "Epoch 146 Loss 0.6633 Accuracy 0.7735\n",
      "Time taken for 1 epoch: 36.96831011772156 secs\n",
      "\n",
      "discarded batch 15\n",
      "Epoch 146 VALIDATION: Loss 0.7394 Accuracy 0.7585\n",
      "\n",
      "epoch lasted: 37.12033152580261\n",
      "Epoch 147 Batch 0 Loss 0.6482 Accuracy 0.7907\n",
      "Epoch 147 Batch 50 Loss 0.6678 Accuracy 0.7725\n",
      "Epoch 147 Batch 100 Loss 0.6599 Accuracy 0.7745\n",
      "Epoch 147 Batch 150 Loss 0.6583 Accuracy 0.7756\n",
      "Epoch 147 Batch 200 Loss 0.6569 Accuracy 0.7762\n",
      "Epoch 147 Batch 250 Loss 0.6562 Accuracy 0.7765\n",
      "Epoch 147 Batch 300 Loss 0.6562 Accuracy 0.7763\n",
      "Epoch 147 Batch 350 Loss 0.6556 Accuracy 0.7766\n",
      "Epoch 147 Batch 400 Loss 0.6550 Accuracy 0.7766\n",
      "Epoch 147 Batch 450 Loss 0.6553 Accuracy 0.7765\n",
      "Epoch 147 Batch 500 Loss 0.6553 Accuracy 0.7763\n",
      "Epoch 147 Batch 550 Loss 0.6564 Accuracy 0.7759\n",
      "Epoch 147 Batch 600 Loss 0.6574 Accuracy 0.7758\n",
      "Epoch 147 Batch 650 Loss 0.6580 Accuracy 0.7754\n",
      "Epoch 147 Batch 700 Loss 0.6591 Accuracy 0.7750\n",
      "Epoch 147 Batch 750 Loss 0.6589 Accuracy 0.7750\n",
      "Epoch 147 Batch 800 Loss 0.6592 Accuracy 0.7748\n",
      "Epoch 147 Batch 850 Loss 0.6593 Accuracy 0.7748\n",
      "Epoch 147 Batch 900 Loss 0.6591 Accuracy 0.7749\n",
      "Epoch 147 Batch 950 Loss 0.6590 Accuracy 0.7749\n",
      "Epoch 147 Batch 1000 Loss 0.6593 Accuracy 0.7748\n",
      "Epoch 147 Batch 1050 Loss 0.6593 Accuracy 0.7749\n",
      "discarded batch 1067\n",
      "Epoch 147 Batch 1100 Loss 0.6594 Accuracy 0.7748\n",
      "Epoch 147 Batch 1150 Loss 0.6599 Accuracy 0.7748\n",
      "Epoch 147 Batch 1200 Loss 0.6603 Accuracy 0.7746\n",
      "Epoch 147 Batch 1250 Loss 0.6610 Accuracy 0.7743\n",
      "Epoch 147 Batch 1300 Loss 0.6609 Accuracy 0.7743\n",
      "Epoch 147 Batch 1350 Loss 0.6614 Accuracy 0.7740\n",
      "Epoch 147 Batch 1400 Loss 0.6617 Accuracy 0.7739\n",
      "Epoch 147 Batch 1450 Loss 0.6617 Accuracy 0.7739\n",
      "Epoch 147 Batch 1500 Loss 0.6617 Accuracy 0.7739\n",
      "Epoch 147 Loss 0.6623 Accuracy 0.7738\n",
      "Time taken for 1 epoch: 37.23811602592468 secs\n",
      "\n",
      "epoch lasted: 37.24166750907898\n",
      "Epoch 148 Batch 0 Loss 0.6797 Accuracy 0.7774\n",
      "Epoch 148 Batch 50 Loss 0.6620 Accuracy 0.7713\n",
      "Epoch 148 Batch 100 Loss 0.6577 Accuracy 0.7730\n",
      "Epoch 148 Batch 150 Loss 0.6566 Accuracy 0.7741\n",
      "Epoch 148 Batch 200 Loss 0.6592 Accuracy 0.7739\n",
      "Epoch 148 Batch 250 Loss 0.6576 Accuracy 0.7746\n",
      "Epoch 148 Batch 300 Loss 0.6589 Accuracy 0.7743\n",
      "Epoch 148 Batch 350 Loss 0.6590 Accuracy 0.7744\n",
      "Epoch 148 Batch 400 Loss 0.6592 Accuracy 0.7746\n",
      "Epoch 148 Batch 450 Loss 0.6595 Accuracy 0.7746\n",
      "Epoch 148 Batch 500 Loss 0.6593 Accuracy 0.7751\n",
      "Epoch 148 Batch 550 Loss 0.6593 Accuracy 0.7750\n",
      "Epoch 148 Batch 600 Loss 0.6592 Accuracy 0.7752\n",
      "Epoch 148 Batch 650 Loss 0.6598 Accuracy 0.7750\n",
      "Epoch 148 Batch 700 Loss 0.6603 Accuracy 0.7747\n",
      "Epoch 148 Batch 750 Loss 0.6605 Accuracy 0.7747\n",
      "Epoch 148 Batch 800 Loss 0.6599 Accuracy 0.7748\n",
      "Epoch 148 Batch 850 Loss 0.6597 Accuracy 0.7748\n",
      "Epoch 148 Batch 900 Loss 0.6597 Accuracy 0.7748\n",
      "Epoch 148 Batch 950 Loss 0.6598 Accuracy 0.7748\n",
      "Epoch 148 Batch 1000 Loss 0.6599 Accuracy 0.7747\n",
      "Epoch 148 Batch 1050 Loss 0.6603 Accuracy 0.7745\n",
      "discarded batch 1087\n",
      "Epoch 148 Batch 1100 Loss 0.6603 Accuracy 0.7746\n",
      "Epoch 148 Batch 1150 Loss 0.6602 Accuracy 0.7746\n",
      "Epoch 148 Batch 1200 Loss 0.6605 Accuracy 0.7745\n",
      "Epoch 148 Batch 1250 Loss 0.6610 Accuracy 0.7744\n",
      "Epoch 148 Batch 1300 Loss 0.6614 Accuracy 0.7741\n",
      "Epoch 148 Batch 1350 Loss 0.6614 Accuracy 0.7742\n",
      "Epoch 148 Batch 1400 Loss 0.6618 Accuracy 0.7740\n",
      "Epoch 148 Batch 1450 Loss 0.6621 Accuracy 0.7739\n",
      "Epoch 148 Batch 1500 Loss 0.6620 Accuracy 0.7739\n",
      "Epoch 148 Loss 0.6621 Accuracy 0.7739\n",
      "Time taken for 1 epoch: 36.946600914001465 secs\n",
      "\n",
      "epoch lasted: 36.95109820365906\n",
      "Epoch 149 Batch 0 Loss 0.6175 Accuracy 0.7741\n",
      "Epoch 149 Batch 50 Loss 0.6463 Accuracy 0.7781\n",
      "Epoch 149 Batch 100 Loss 0.6486 Accuracy 0.7777\n",
      "Epoch 149 Batch 150 Loss 0.6540 Accuracy 0.7764\n",
      "Epoch 149 Batch 200 Loss 0.6567 Accuracy 0.7758\n",
      "Epoch 149 Batch 250 Loss 0.6573 Accuracy 0.7757\n",
      "Epoch 149 Batch 300 Loss 0.6571 Accuracy 0.7758\n",
      "Epoch 149 Batch 350 Loss 0.6573 Accuracy 0.7754\n",
      "Epoch 149 Batch 400 Loss 0.6572 Accuracy 0.7755\n",
      "Epoch 149 Batch 450 Loss 0.6582 Accuracy 0.7749\n",
      "Epoch 149 Batch 500 Loss 0.6581 Accuracy 0.7747\n",
      "Epoch 149 Batch 550 Loss 0.6589 Accuracy 0.7746\n",
      "Epoch 149 Batch 600 Loss 0.6583 Accuracy 0.7747\n",
      "Epoch 149 Batch 650 Loss 0.6588 Accuracy 0.7746\n",
      "Epoch 149 Batch 700 Loss 0.6586 Accuracy 0.7747\n",
      "Epoch 149 Batch 750 Loss 0.6587 Accuracy 0.7747\n",
      "Epoch 149 Batch 800 Loss 0.6585 Accuracy 0.7748\n",
      "Epoch 149 Batch 850 Loss 0.6589 Accuracy 0.7745\n",
      "Epoch 149 Batch 900 Loss 0.6597 Accuracy 0.7743\n",
      "Epoch 149 Batch 950 Loss 0.6596 Accuracy 0.7745\n",
      "Epoch 149 Batch 1000 Loss 0.6600 Accuracy 0.7744\n",
      "Epoch 149 Batch 1050 Loss 0.6602 Accuracy 0.7743\n",
      "discarded batch 1057\n",
      "Epoch 149 Batch 1100 Loss 0.6604 Accuracy 0.7743\n",
      "Epoch 149 Batch 1150 Loss 0.6608 Accuracy 0.7740\n",
      "Epoch 149 Batch 1200 Loss 0.6612 Accuracy 0.7738\n",
      "Epoch 149 Batch 1250 Loss 0.6614 Accuracy 0.7738\n",
      "Epoch 149 Batch 1300 Loss 0.6614 Accuracy 0.7738\n",
      "Epoch 149 Batch 1350 Loss 0.6619 Accuracy 0.7737\n",
      "Epoch 149 Batch 1400 Loss 0.6619 Accuracy 0.7737\n",
      "Epoch 149 Batch 1450 Loss 0.6622 Accuracy 0.7735\n",
      "Epoch 149 Batch 1500 Loss 0.6624 Accuracy 0.7734\n",
      "Epoch 149 Loss 0.6626 Accuracy 0.7733\n",
      "Time taken for 1 epoch: 36.941234827041626 secs\n",
      "\n",
      "epoch lasted: 36.945451974868774\n",
      "Epoch 150 Batch 0 Loss 0.6235 Accuracy 0.7841\n",
      "Epoch 150 Batch 50 Loss 0.6516 Accuracy 0.7772\n",
      "Epoch 150 Batch 100 Loss 0.6482 Accuracy 0.7799\n",
      "Epoch 150 Batch 150 Loss 0.6558 Accuracy 0.7765\n",
      "Epoch 150 Batch 200 Loss 0.6587 Accuracy 0.7763\n",
      "Epoch 150 Batch 250 Loss 0.6560 Accuracy 0.7771\n",
      "Epoch 150 Batch 300 Loss 0.6569 Accuracy 0.7764\n",
      "Epoch 150 Batch 350 Loss 0.6570 Accuracy 0.7759\n",
      "Epoch 150 Batch 400 Loss 0.6574 Accuracy 0.7758\n",
      "Epoch 150 Batch 450 Loss 0.6570 Accuracy 0.7757\n",
      "Epoch 150 Batch 500 Loss 0.6566 Accuracy 0.7758\n",
      "Epoch 150 Batch 550 Loss 0.6559 Accuracy 0.7762\n",
      "discarded batch 553\n",
      "Epoch 150 Batch 600 Loss 0.6559 Accuracy 0.7760\n",
      "Epoch 150 Batch 650 Loss 0.6561 Accuracy 0.7759\n",
      "Epoch 150 Batch 700 Loss 0.6571 Accuracy 0.7756\n",
      "Epoch 150 Batch 750 Loss 0.6571 Accuracy 0.7756\n",
      "Epoch 150 Batch 800 Loss 0.6580 Accuracy 0.7752\n",
      "Epoch 150 Batch 850 Loss 0.6574 Accuracy 0.7754\n",
      "Epoch 150 Batch 900 Loss 0.6576 Accuracy 0.7754\n",
      "Epoch 150 Batch 950 Loss 0.6584 Accuracy 0.7751\n",
      "Epoch 150 Batch 1000 Loss 0.6576 Accuracy 0.7753\n",
      "Epoch 150 Batch 1050 Loss 0.6579 Accuracy 0.7752\n",
      "Epoch 150 Batch 1100 Loss 0.6581 Accuracy 0.7751\n",
      "Epoch 150 Batch 1150 Loss 0.6587 Accuracy 0.7749\n",
      "Epoch 150 Batch 1200 Loss 0.6591 Accuracy 0.7748\n",
      "Epoch 150 Batch 1250 Loss 0.6593 Accuracy 0.7747\n",
      "Epoch 150 Batch 1300 Loss 0.6591 Accuracy 0.7747\n",
      "Epoch 150 Batch 1350 Loss 0.6595 Accuracy 0.7745\n",
      "Epoch 150 Batch 1400 Loss 0.6598 Accuracy 0.7744\n",
      "Epoch 150 Batch 1450 Loss 0.6601 Accuracy 0.7744\n",
      "Epoch 150 Batch 1500 Loss 0.6604 Accuracy 0.7743\n",
      "Saving checkpoint for epoch 150 at ./checkpoints/train/ckpt-30\n",
      "Epoch 150 Loss 0.6608 Accuracy 0.7742\n",
      "Time taken for 1 epoch: 37.285717487335205 secs\n",
      "\n",
      "epoch lasted: 37.28993558883667\n"
     ]
    }
   ],
   "source": [
    "#@title Train loop\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    random.shuffle(batches)\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    for (batch, (inp, tar)) in enumerate(batches):\n",
    "        if (len(inp) != batch_len or len(tar) != batch_len):\n",
    "            print(\"discarded batch\", batch)\n",
    "            continue\n",
    "        train_step(np.expand_dims(inp, axis=0), np.expand_dims(tar, axis=0))\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, train_loss.result(), train_accuracy.result()))\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
    "\n",
    "    wandb.log({\n",
    "        'train_loss': train_loss.result(),\n",
    "        'train_accuracy': train_accuracy.result()\n",
    "    }, step=epoch+1)\n",
    "\n",
    "    # validation\n",
    "    if epoch % 5 == 0:\n",
    "        loss_l, acc_l = [], []\n",
    "        for (batch, (inp, tar)) in enumerate(val_b):\n",
    "            val_loss.reset_states()\n",
    "            val_accuracy.reset_states()\n",
    "            \n",
    "            if (len(inp) != batch_len or len(tar) != batch_len):\n",
    "                print(\"discarded batch\", batch)\n",
    "                continue\n",
    "\n",
    "            val_step(np.expand_dims(inp, axis=0), np.expand_dims(tar, axis=0))\n",
    "\n",
    "            loss_l.append(val_loss.result())\n",
    "            acc_l.append(val_accuracy.result())\n",
    "\n",
    "        loss_mean = sum(loss_l)/len(loss_l)\n",
    "        acc_mean = sum(acc_l)/len(acc_l)\n",
    "        print('Epoch {} VALIDATION: Loss {:.4f} Accuracy {:.4f}\\n'.format(epoch + 1, loss_mean, acc_mean))\n",
    "\n",
    "        wandb.log({\n",
    "            'val_loss': loss_mean,\n",
    "            'val_accuracy': acc_mean\n",
    "        }, step=epoch+1)\n",
    "\n",
    "    # generation\n",
    "    if epoch in generate_at:\n",
    "        generate(1)\n",
    "    \n",
    "    print(\"epoch lasted: {}\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "aHHI5hivp5T7",
    "outputId": "80e524dd-6f2d-45ab-b1b1-f94186729d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1900, 128)\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n%load_ext tensorboard\\n%tensorboard --logdir .\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Parameter persistence\n",
    "\n",
    "transformer.save_weights(\"./optimus_rhyme\")\n",
    "#transformer.load_weights(\"./optimus_rhyme\")\n",
    "\n",
    "\n",
    "#emb_enc_w = transformer.encoder.embedding.get_weights()[0]\n",
    "emb_enc_w = transformer.decoder.embedding.get_weights()[0]\n",
    "print(emb_enc_w.shape)\n",
    "\n",
    "out_v = open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(poetry_sy_lm_dataset.vocabulary.dictionary):\n",
    "  vec = emb_enc_w[num] # skip 0, it's padding.\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "\n",
    "'''\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir .\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7_wygFNRwgf"
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQpP7oiF3iEE",
    "outputId": "ae09fcb1-ff70-4548-d3d8-6c14115177e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la | tua | be| ni| gni| tà | non | pur | soc| cor| re|\n",
      "a | chi | do| man| da | ma | mol| te | fï| a| te|\n",
      "li| be| ra| men| te al | di| man| dar | pre| cor| re|\n",
      "                                        \n",
      " che | m’ a| vea | ma | dal | mio | dol| ce | mi | sca| te|\n",
      "co| me | se | tu | ma| tria | mi | stra| di| ma| ce|\n",
      "co| me | se | tu | me| na| to | sa| li| ce| to|\n",
      "                                        \n",
      " co| me | se| con| da | la | mia | ma| ni| fa| ce|\n",
      "co| me | la | mia | men| tre | che | m’ a| vea|\n",
      "per | lo | suo | mi | fe| ce | mio | du| ca | ma| cra|\n",
      " che | m’ a| vea | ma | da | l’ a| mor | mi | ri| spuo|\n",
      "co| me | se | tu | mi | ve| drai | con| ten| du| ca|\n",
      "co| me | la | mia | men| tre | mi | fa| cea | mu| ca|\n",
      "                                      \n",
      " co| me | se| con| da | l’ a| ma | ma| te| ria|\n",
      "due | co| me | mi | di| cea | ma | di| cea | mu| ca|\n",
      "che | mi | fa | me | di | mor| ta | mi | fa | ma|\n",
      "\n",
      " co| me | se | tu | mi | fa | che | m’ a| vea | mos| se|\n",
      "co| me | se | tu | ma | me | tu | mi | scor| ti| ma|\n",
      "co| me | se| con| da | la | man | do| lo| man| da|\n",
      "                                       \n",
      " co| me | se| con| da | la | mia | ma| ni| fe| sta|\n",
      "co| me | se | tu | se’ | mai | non | mi | se| con| da|\n",
      "co| me | se | tu | mi | fa | me | con | la | men| te|\n",
      "  \n",
      " co|\n",
      "me | se | non | mi | ma| ra | ma | non | si | sca| da|\n",
      "co| me | se | tu | ma| re in| cen| do | mi | sco| sco|\n",
      "co| me | se | tu | ma | se | tu | ma | se| con| da|\n",
      "                                  \n",
      " co| me | se| con| da | la | mia | ma| ni| fe| sta|\n",
      "per | co| me | se | tu | co| me | sa| rai | man| co|\n",
      "co| me | se | tu | co| me | l’ a| mo| re a | me| sta|\n",
      " che | mi | fa | ma | di| sce| sta | mi | ri| spuo| se|\n",
      "co| me | se | tu | mi | sco| lo| mai| se | mo| ma|\n",
      "che | m’ a| vea | ma| re a | me | con | le | sue|\n",
      "                                            \n",
      " co| me | se| con| da | l’ al| to | mi | fa | ma| gu| sca|\n",
      "co| me | se | tu | co| me | se | tu | mi | sco| se|\n",
      "co| me | se | tu | vuo’ | che | mai | non | m’ a| scu| sca|\n",
      " ma | dol| ce | mi | fa | ma | co| me | si | mo| me|\n",
      "lo | mio | du| ca | mio | du| ca | mi | fa | mai|\n",
      "sì | com’ | io | ma| e| stro | mi | fe| ce|\n",
      "                                                      \n",
      " co| me | se| con| da | mi | fa | ma| ni| fe| sti|\n",
      "co| me | la | mia | men| tre | co| me | mi | fai|\n",
      "sì | com’ | io | mi | fe| ce| tra| ta | mi | fe| ce|\n",
      "\n",
      " co| me | se | tu | ma | tu | ma | che | m’ a| scol| la|\n",
      "co| me | se | tu | mi | fa | co| me | se| col| le|\n",
      "e | co| me | se| con| da in| cor| di | mo| vel| la|\n",
      "                                        \n",
      " co| me | se| con| da | la | mia | ma| ni| ma| ga|\n",
      "e | co| me | la | mia | men| te | mi | fe| ce|\n",
      "co| me | si | mo| stra| men| te | co| me | l’ u| ma|\n",
      "\n",
      " co| me | se | tu | ma | ma | co| me | sa| to| stra|\n",
      "la | mia | ma | me| do| ra | co| me | tua | mo| stra|\n",
      "che | m’ a| vea | ma| to | con | l’ a| mo| re a| mo| re|\n",
      "                                    \n",
      " co| me | se| con| da | mio | di| man | di| sce| sta|\n",
      "la | mia | ma | me| de| sma | con | l’ al| to | sco| re|\n",
      "co| me | la | man | dol| ce | mio | di| man| da| me|\n",
      "\n",
      " co|\n",
      "e | se | non | mi | ma| na | ma | non | si | sce| ma|\n",
      "co| me | se | tu | ma| ni| fe| stia | mi | sco| sce|\n",
      "co| me | se | tu | me| stie| de a | me | so| le|\n",
      "                                       \n",
      " co| me | se| con| da | la | mia | ma| ni| fe| sta|\n",
      "co| me | se | tu | se’ | tu | ma | se | tu | mi| se|\n",
      "co| me | se | tu | co| me | l’ u| ma| na | mi| se|\n",
      "\n",
      " co| me | se | tu | ma| e|\n",
      "che | m’ a| vea | mi | se| co|\n",
      "co| me | se | tu | ma | tua | mai | non | so | mo| me|\n",
      "che | m’ a| vea | la | mia | ma| ne| stra | mo| do|\n",
      "                                   \n",
      " co| me | se| con| da | la | mia | ma| ni| fe| sta|\n",
      "co| me | se | tu | co| me | se | tu | ma | pro| do|\n",
      "che | m’ a| vea | man| da| to | mi | fe| ce | sce| sta|\n",
      " co|\n",
      "an| cor | m’ a| vac| ca| to | mi | fa | mo| me|\n",
      "lo | mio | do| man| da | mio | di| ce| trai|\n",
      "co| me | sce| ma| to| ra|\n",
      "co| me | co| me | se | tu | com’ | io | ve| co|\n",
      "                             \n",
      " co| me | se| con| da | l’ a| ma | dol| ce | sca| la|\n",
      "co| me | la | mia | men| te | mi | fa | mag| gio| re|\n",
      "co| me | la | mia | men| te | mi | fa | man | da| la|\n",
      "                          \n",
      "co| me | l’ un | ma| nï| ol | due|\n",
      "co| me | se | tu | mi | vec| chi | m’ a| vea | man| da|\n",
      "poi | com’ | io | mi | vol| se | mi | ma| e| strue|\n",
      "\n",
      " co| me | se | tu | ma| dre | co| me | se | tue|\n",
      "co| me | se | tu | con| ver| si | ch’ al | mio | mi| schia| stro|\n",
      "co| me | co| me | se | tu | m’ a| vea | ma| nu| ta|\n",
      "                                 \n",
      " co| me | se| con| da | la | mia | ma| ni| fe| sta|\n",
      "co| me | se| con| da | la | mia | men| te | mu| ta|\n",
      "che | mi | fa | me | di| scer| na | mi | fa | me| sta|\n",
      " co|\n",
      "e | co| me | se | tu | m’ a| mor | mi | ri| spon| da|\n",
      "co| me | se | tu | ma | tua | mai | ma| ra| vi| sta|\n",
      "co| me | se| con| da | l’ a| mo| re in | su | pen| sa|\n",
      "                                  \n",
      " co| me | se| con| da | la | mia | ma| ni| fe| sta|\n",
      "co| me | se | tu | se’ | mai | non | mi | se| con| da|\n",
      "co| me | la | mia | men| te | mi | fa | man | du| ca|\n",
      " che | m’ a| vea|\n",
      "la | mi | do| man| da | mi | ri| spuo| se|\n",
      "co| me | mi | ve| drai | con | lo | suo | ma| ren| te|\n",
      "co| me | con| vien | ch’ io | mi | rag| gio | mu| ca|\n",
      "                               \n",
      " e | co| me | se| con| da | mi | fa | me | scen| de|\n",
      "se | tu | ma| e| stro | mi | fa | che | la | man| ca|\n",
      "co| me | la | mia | men| te | mi | fa | me | den| tro|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30K9u1Y2A7l8",
    "outputId": "333d26b5-30d8-4f6c-99aa-7f5adcbd9755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la | tua | be| ni| gni| tà | non | pur | soc| cor| re|\n",
      "a | chi | do| man| da | ma | mol| te | fï| a| te|\n",
      "li| be| ra| men| te al | di| man| dar | pre| cor| re|\n",
      "                                        \n",
      " del | ciel | mi | fa | che | tut| to | mi | rac| col| se|\n",
      "che | se | non | pa| dre | che | mag| gior | mi | mo| ri|\n",
      "sot| tol | ve| der| te in | più | che | la | sua|\n",
      "                                  \n",
      " co| me | si | puo| te | ch’ a| vea | se| me | van| na|\n",
      "pe| rò | co| me | san| za | sen | va | che | m’ a| sco|\n",
      "a | la | se| me | di| sman| da | la | ci| ma | man| na|\n",
      "\n",
      " ma|\n",
      "dre | mi | fre| du| ta | co| lui | che | si | scal| le|\n",
      "por| se | mi | fa| cea | me | can| tan| do| man| do|\n",
      "ma | tu | mi | du| ca e ’l | mio | mi| se| ra | mol| le|\n",
      "                             \n",
      " ma | tu | m’ a| mor | mi | fa | ma| nï| an| do| ma|\n",
      "e | me| ste | me | tor| ra | mi | re | dol| ca | mol| le|\n",
      "dol| ce | mio | ma| re| sta| ri| no a | la | co| ma|\n",
      "\n",
      " co|\n",
      "noi | che | s’ ar| rei | met| to | lo | suo | ma| na| to|\n",
      "con | par| te a | mar | ma| ra| ta | m’ ac| co| men| ta|\n",
      "per| ch’ io | me | pe| rò | lo | dol| ce | cam| pa| to|\n",
      "                        \n",
      " e | con | lo | don| fo | de | la | mi| ra | mag| gio|\n",
      "co| me | si | mos| se | con | lo | mal | sa| li| ma| to|\n",
      "co| me | la | mi| se| ra | con| fa | man | di| man| da|\n",
      "\n",
      " ch’ a | pre| gio | mi | pa| rea | la | mia | mia | re| to|\n",
      "co| tan| do ’l | dub| bio | mio | a| mo| re a | pre| sta|\n",
      "e | co| me | la | mia | dol| ce | mi | si | mi| se|\n",
      "                              \n",
      " e | io | ma| e| stro | co| sì | m’ ap| pa| sta| sta|\n",
      "dol| ce | mi | di| cea; | e | lo ’n| tel| le | mi| se|\n",
      "con| ce| det| ta | lo| co | di| spe| ra| men| ti| to|\n",
      " can| tan| do | ch’ a | con| tra | mi | di| scen| do| mi|\n",
      "lo | col| mo | mi | vol| se il | so| per | mi | fo| co|\n",
      "ca| ra | co| me | tua | vo| lon| tien | mi | sco| mi|\n",
      "                                 \n",
      " e | se | tu | mar | non | mi | rag| gio e | ser| pi| glio|\n",
      "co| sì ’l | com| men| do a | sue | ma| ni| fe| se| se|\n",
      "che | mi | fa | che | non | vol| se | se | na| scel| lo|\n",
      "\n",
      "                         \n",
      "mol| te | si | vol| ge | sa| li|\n",
      "ma | doo| man| da | mi | vol| si | so| la in | sùe|\n",
      "con | le | man | cam| mi| na| ne in | su | pec| ca|\n",
      "\n",
      " e | l’ om| bre | che | per| ché | non | m’ a| spet| ta| mi|\n",
      "con | la | se| col| ta | me| des| se | mi | schia| ne|\n",
      "con| tra | mi | fe| di e | po| co | so| pra | men| da|\n",
      "                             \n",
      " e | li | spi| ra| ti | dol| ce | mi| se| ria | ma| na|\n",
      "co| me ’l | co| lor | de | la | ca| ron | si| cu| ra|\n",
      "dal | suo | man| to| man| dar | m’ a| mo| re a | man| na|\n",
      "  \n",
      " e | so| lo a | ma | dal | mio | da | sua | per| cuol| la|\n",
      "el| li o| ve | so| lo a | ca| ro | col| po | ma| ro|\n",
      "e | io | mo | mo| do in| cen| di a | la | sua | ma| ta|\n",
      " sot| to | se | ne | la | mag| gior | mi | sa| lo| ro|\n",
      "ch’ el| li a| ma| ti | sca| gliai | de | la | ma| ra|\n",
      "po| co | so| le al | com’ | io | mo | dol | mio|\n",
      "                                           \n",
      " e ’l | ma| e| stro | de | la | men| te | mi| se| gli|\n",
      "poi | ma| e| stro | mi | fa | che | m’ a| vean | man| co|\n",
      "che | mi | fol| la| mi | mos| se | ma | dal | ma| ta|\n",
      "  \n",
      " che | la | me| se | li | sper| ra | mi | fle| ge|\n",
      "do| man| dia | mi il| las| se| ro | so| man| da|\n",
      "con | lo | ma| la| de|\n",
      "che | lo | sco| glio | del | suo | più | lun| ge|\n",
      "                            \n",
      " e | co| me ’l | ma| e| tra | me| mo a | lei | ri| spo;|\n",
      "e | se| guì | mi| glia| me| ria | lor | dol| ce| mi|\n",
      "e | com| po | mio | du| re | mi | mag| gior | mi | fo| ra|\n",
      " che | lo ’m| pe|\n",
      "la | mag| gior | mo| ve| ria | mia | mia;|\n",
      "ma | ta| ce | me | tu | sai | che | la| to | mu| na|\n",
      "ma ’n| tel | man| do | mi | di| sio | mag| gio| me|\n",
      "                                   \n",
      " e | con | lo ’n| fi| no a | me | tan| to | lo | su| na|\n",
      "di | ma| re | che | m’ a| vea | me | ca| to| las| si|\n",
      "san| za| bi| le e | com’ | o| mo in| fi| nï| an| da;|\n",
      " e | co|\n",
      "a | la | man | dol| ce | so| mi| na | me| sta|\n",
      "lo | du| ca | dol| ce | mi | fa| cea | mag| gi| ne|\n",
      "ma | por| ta | mi | si | cuo| ri e | dal | suo | me| sta|\n",
      "                                   \n",
      " e | di| scen| do | la | cit| tà | ma | non | ve| de|\n",
      "co| me | la | ca| ri| ce | mia | mi | so| lu| ma|\n",
      "per| cos| se | mi | sen | l’ a| ma| rà | ci| ma| li|\n",
      "\n",
      " e | com’ | io | co| sto | con | col | mio | ma| ra| va|\n",
      "che | di| co ’l | se| mi| le | suo | cam| po | sa| ra|\n",
      "che ’l | mon| dol| lo a | co| ma | me | so| spet| to|\n",
      "                                  \n",
      " al | ciel | di | co| lor | mi | di| si| ra | mi| ra| ra|\n",
      "pren| cel| men| da | co| stan| to | mi | se| gua| ra|\n",
      "ed | el | mi | fe| ce | ma| re| ria | co| sì;|\n",
      "\n",
      "\n",
      " ma | per | co| min| ciai | con| vien | che | m’ a| man| ce|\n",
      "ch’ el| la | pros| so | le| tè | don| na | ra| de| sta|\n",
      "co| me | se | non | m’ a| mor | mi | si | rac| col| pe| ra|\n",
      "                      \n",
      " e | se | la | mia | ma| ni| fe| ria | co| lo| ma|\n",
      "lo | mio | ma| e| stro | ma | dim| mi | mi | schiu| so| ma|\n",
      "co| me | mi | fe| de il | ca| pol| le a | la | mon| de|\n",
      " che ’l | du| ca | m’ a| man| da| me in| ten| di | ma| cri| ma|\n",
      "per | col| pa | che | tan| to | m’ in| tra| va| mo| ro|\n",
      "co| me | sen| tia | ma | se | tu | ma| nag| gi| | li;|\n",
      "                          \n",
      " e | io | ma| e| stro | mi | fe| ce a | man | do| ma;|\n",
      "e | io | mi | par| la| va| ma in| fi| ne| rai|\n",
      "in | me | tu | lo | dub| bi | me | lo | dub| bia| il|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#k=3\n",
    "generate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "byt1GoaLA585",
    "outputId": "f59c9f76-83aa-42dc-87f6-e4c8ba5c9519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la | tua | be| ni| gni| tà | non | pur | soc| cor| re|\n",
      "a | chi | do| man| da | ma | mol| te | fï| a| te|\n",
      "li| be| ra| men| te al | di| man| dar | pre| cor| re|\n",
      "                                        \n",
      " da | mol| le | par| ti a | me | dis| se a | ma| tu| ra|\n",
      "da | per | me | da | le | sue | mar | tre | rac| col| te|\n",
      "lo | mio | cor| se il | po| e| ta| mo e | la | ru| sa|\n",
      "                               \n",
      " che | tra| smu| to | ma | do| man| da | mi | stol| ta|\n",
      "som| mo| ras| se | mi | fe | di| vel| la| men| te|\n",
      "dal | mon| do | lo | sol| le e | san| to | mi | fa| ce|\n",
      " dim| mi | mi | se| ne|\n",
      "poi | con| ce| se i | ca| sen| ti|\n",
      "ch’ io | mi | vol| si a | cui | lo | mo| do | man| te|\n",
      "che | dim| mi | ve| de| re a | la | vi| sta un | di| ce|\n",
      "                              \n",
      " l’ i| ma| ge| li| bè | del | mun| ce | mio | cal| do|\n",
      "per | lo | mio | dun| que | co| lor | con| to| mai|\n",
      "col | mio | dol| ce | mio | con | pre| sen| to | mal| do|\n",
      "\n",
      " che | se | la | ma| de ’l | co| ma è | le | ma| ni| me|\n",
      "com’ | el| li an| co| ra | me | son | mo | di| man| do|\n",
      "la | me| sco | mi | fa| cea | che | la | sua | mu| ma|\n",
      "                                \n",
      " con | man | so| lo| re e | man | per| cuo | so | pian| da|\n",
      "e | io | con| di| ca o | san| mi| na| men| du| a;|\n",
      "ma | mea | la | ci ap| pa| ron | tua | me| ti | man| da|\n",
      "                         \n",
      "mo | per | mi | vol| si ’l | mon| do|\n",
      "mo| stram| mi | mi | do| vria | mi | si | ma| ro| me|\n",
      "per | con | lo | com| pa| gno ad | a| ma| ne in| te|\n",
      "\n",
      " e | co| min| ciò il | mio | com| mes| so a | piùe|\n",
      "cim| mi | ma| ci| na| va| sta in| cen| di | co| sta|\n",
      "pen| sa | co’ | io | do| man| da| ta | m’ ha | sea|\n",
      "                                           \n",
      " po| mo | m’ e| ra | da | mia | so| spla | sa| li| to|\n",
      "par| la | di| man| dar | mo | al | mon| do i | pa| co|\n",
      "an| co| min| cia| me a | can| zo i | man| co| sì;|\n",
      " ma | la | col| la | mi | s’ am| mi | ri| ma| via al | pan| da|\n",
      "co| me | vi | ma | poi | co| me ’l | dol| ce ’l | mio|\n",
      "com’ | a| mi| ra| bi| li| to | mi | fe| ce a | cia|\n",
      "                              \n",
      " e | la | dol| ce | co| ne | mi | par| ve | mio|\n",
      "e | mo| dol| ce | co| lol | che | die| no ad | es| se|\n",
      "a | mu| se | sol | du| ra| men| te | la | mar| tì| ra|\n",
      " sì | ch’ om| bra | me| de| smo a | ma | ne| ma| ri| co|\n",
      "do| man| da | la | ma| dra| men| te | mi | fa| ma;|\n",
      "con | pru| mo | com’ | or | del | mio | com’ | io|\n",
      "                                         \n",
      " per| ch’ io | do| co | se | non | po| ten| da | ra| ma|\n",
      "di| ma| ta o| ma| ra | ma | sì | ma | per | te| ma|\n",
      "e | dal | mon| do el | fio| re| di | la | vi| de i | pas| se|\n",
      "\n",
      " e|\n",
      "stal | mon| do | co| ma | me | con | cam| mo| ta|\n",
      "pres| sa | ma| dre | co| me ’l | ma| ni| man| di| tra|\n",
      "com’ io | se| col| si a | lo | ma| dre o| mai | ma| la;|\n",
      "                                  \n",
      " co| sì | mi | di| spo| sto | m’ ha | ma| ni| fe| stra|\n",
      "pe| rò | ch’ ad | a| me| stro | san| za | ca| ta| la|\n",
      "con | al| lor | mi | mu| ta | ma| to| ra | me| stra|\n",
      " da | sua | mi| gliai | con | li oc| chi | mi | di| man| de|\n",
      "po| sa | mi | ve| nim| mo | cu’ | io | ti | so| bo|\n",
      "can| tor | ve| nìa | mo | del | suo | po| tul| lan| de|\n",
      "                                \n",
      " e | con| tra | sa| lir | con | li al| lu| ma | sco| bo|\n",
      "cen| to| mi | si | mo| ve e | cer| ca | se | bru| ma|\n",
      "me| ce | le | mie | di | cam| mi| gliar | ma| no|\n",
      " el| li o| mai | con| ten| do | m’ a| vea | la | co| na| le|\n",
      "mi | vi| d’ io | mi | scan| da| rà | dal | com| pres| so|\n",
      "qua| lun| que | me| ce| smo a | l’ a| men| te | co| le|\n",
      "                          \n",
      " co| me | con| ve| ne| men| da | m’ ap| pe| ta| re|\n",
      "cel | mio | ma | mu| ta | le | spi| ro | det| tan| do|\n",
      "che | me | la | man| te | ch’ om’ | mi | fa| vol| la| ra|\n",
      "\n",
      " co| min| ciai | mi | rag| gio| sa | la | sa| lu| ra|\n",
      "co| me ’l | sa| prei | de | lo | suo | ma| e| scon| do|\n",
      "e | li | ma| e| stri o| mai | dol| ce in| cen| du| ta|\n",
      "                                   \n",
      " con | le | ma| e| stro | mi | fe| ce | le | me| sco|\n",
      "con | la | mon| da | lui | ma| ta| de | do| lor | ven| ta|\n",
      "lo | du| ca | mio | ma | sen| ti| men | la | mea|\n",
      " ed | el| la | mi | de | la | mi | fe| ce e | rui|\n",
      "la | lom| bo | ma | cal| da | mi | dal | mio|\n",
      "co| min| cia a | me | po| co|\n",
      "ma | tu | se’ | miei | par| lan | co| lui|\n",
      "                               \n",
      " sì | me | mo| do| mil| la| men| ti a | me | vuo|\n",
      "sal| vi | lo | co| me | cie| lo in | su | la | lui|\n",
      "io | a| ve| stra | me | che | don| na | la | mul| ta| la|\n",
      " co|\n",
      "                                                                       ve| ni e ’ | dì | la | mal | mon| na|\n",
      "se| du| ma | col | ma| re| bro an| co| ra | mui|\n",
      "  ei|\n",
      "                    \n",
      "ma | lui | so | per | cor| so| ne|\n",
      "e | io | re | del | ciel | mi | fa | di| la| ma| re|\n",
      "par| te a | la | mi| na| va | me | sù | non | scol| po|\n",
      "\n",
      " ma | sal | tu | mi | che | le | se| me | sa| lìo|\n",
      "che | tal | mi | fe| ce al | ca| lor | mio | drit| ta| va|\n",
      "do| v’ el| li | mi | fe| ce | se | tu | m’ e| ta|\n",
      "                                        \n",
      " e | la | spe| ra | di | cuo | so| lo | mi | fei|\n",
      "mos| so | me| ma| na | che | tu | nol| vi| de|\n",
      "pos| son | li om| bra | piac| que | con | an| ta| mia|\n",
      " co|\n",
      "e | la | mi | fa| ma | che | tu | vo’ | miei | miei|\n",
      "dub| bi | ch’ io | vu| di| me | del | pen| to | chio| mi|\n",
      "l’ om| bra | mo | di | mo| ve| ta | mi | ri| col| se|\n",
      "                               \n",
      " con | le | san| za | so| pra | sì | man | se| col| mi|\n",
      "e | del | ca| lo| lo e ’l | ma| ni| fel | col| ma| le|\n",
      "co| me | tua | pia| cë| a | me | a| mo| re a| ma|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#k=5\n",
    "generate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m14PfVkTA8mk",
    "outputId": "411436f7-2f60-4650-b7c3-40f80a1f9cb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la | tua | be| ni| gni| tà | non | pur | soc| cor| re|\n",
      "a | chi | do| man| da | ma | mol| te | fï| a| te|\n",
      "li| be| ra| men| te al | di| man| dar | pre| cor| re|\n",
      "                                        \n",
      " con | lei | pa| re| mi | per | lo | co| min| cian| to|\n",
      "lo | so| le e| ra | dot| to | m’ ha | m’ a| mo| ra| sco|\n",
      "son | che | dal | vec| chiom| mo | con| si| glior| si|\n",
      "                                 \n",
      " la | me| na | che | la | vi| sta | si | dor| na| sco|\n",
      "con | ciò | che | cer| to | li o| no | ca| ra il | sa| pa|\n",
      "co| sì | com’ | el | sol | fu | da | la | sua | ma| sa|\n",
      "                                           \n",
      " tan| ti | si | col| se|\n",
      "lo | mio | due | due | san| te a | me | con | pro| ca|\n",
      "con | la | via | la| sce| na | mia | dor| di| na|\n",
      "\n",
      " ma | per| ché | te| sta | man | da | li | mar| ti| ca|\n",
      "cui | ma| e| bel| lo a | la | tua | re| du| dan| ti|\n",
      "vol| si e | di| si| ma| lia|\n",
      "                                                              \n",
      " a | man | con | lui | a | me | se | la | si| pa| sa|\n",
      "si| mil| me | co| me | len| to| men| te | mi| glia|\n",
      "ma | per| ché i | mi | smar| ra | m’ a| mo| rìa|\n",
      "\n",
      " dol| lon| ia | ve| di al| lo| ma | co| sa | ro| ma|\n",
      "sal | tuo | ma| e| stro | si | mon| do | fu | mar| ra|\n",
      "me | l’ u| na | sua | ve| du| ta | de | la | go| ma;|\n",
      "                                      \n",
      " e | mi | ma| nï| a | ma| da| di| ma | por| to|\n",
      "lo | dol| cen| da| me a| vea | tut| ta | con| te| sa|\n",
      "el | cu’ | io | son | cam| mi| no a| na| ri| ce| ra|\n",
      " quel | mon| to| men| to| sto | po| ta | mio | men| co|\n",
      "po| sa | mi ’l | san| gue | mio | mio | con| te| mi| no|\n",
      "co| me ’l | sent| to ’l | mon| te | de | l’ ap| pren| ta|\n",
      "                              \n",
      " che | so| lo | so| pe| ran | con| sor| si | ma| re| sva|\n",
      "ma | to| co | tan| to | man| to| la i | man | va| na|\n",
      "di| mon| do an| co| ra | tra | più | dal | suo | ve| sta|\n",
      " can| to| lo| me | che | di | co| sa | si | mo| do|\n",
      "lo | mio | vo | sem| pre | mi | ver| tu| to | lo| ma|\n",
      "che | par| te al | mon| tai | pe| co| no in| for| ma|\n",
      "                                         \n",
      " co| sì ’l | ma| e| stro | vol| se | com’ | en | giu| da;|\n",
      "ma | pe| rò | ch’ io | la | cre| do| man| da in| tor| ma|\n",
      "el | di | che | ma| no a | la | dol| ce | ri| spo| sa|\n",
      "\n",
      " e | cri| sto il | ma| e| stro | co| me | di | va| co|\n",
      "m’ in| va| nal| men| te | co| lui | che | tu | mol| le|\n",
      "ve| du| rai | la | sua | men| te e | per| su| sca| co|\n",
      "                                  \n",
      " co| me | vuol | mor| te | co| sa a| man| da| mal| le|\n",
      "co| lor | d’ in| fi| da| re a| ni| mo | lui | dul| se|\n",
      "ma | pe| de| mi o | de| sto | per| ché | non | cam| pa|\n",
      " non | ve| nìa|\n",
      "mai | sì | che | di | me | tor| tal | ma| re|\n",
      "pe| la| me | mi | vuol| se | che | le | ce| ne| ste|\n",
      "per | ché | ti | per | me | tuo | co| no| sce| ma| ro|\n",
      "                               \n",
      " con | le | ca| ni| ma u| ma| no a| vea | lo | ve| sta|\n",
      "che | par | tu | mi | scheg| gen| do | mag | da| ro| ro|\n",
      "a| cheo| mo el | fon| do | con| tin| si| men| sa|\n",
      "\n",
      " e | tan| to | poi | che | nel | mi | dol| ci | scu| de;|\n",
      "e | cer| ca| mi| ca| ro in | al| cu| lo| po | scen| sa|\n",
      "co| no| sce | ch’ io | non | sia | ve| tu| me in| tri|\n",
      "                               \n",
      " mi | do| man| da | mo | fui | me | non | ma| nen| de|\n",
      "per| ché | di | lu | due | de | la | pas| sa | ri| spop| pa|\n",
      "da | po| tor | tem| po | ma | per | an| da| men| te|\n",
      "\n",
      "                        \n",
      "cac| ca|\n",
      " e | l’ a| mar | ne| ta | mi | per | la | cor| na in| to| stan| te|\n",
      "ma | po| co e | don| na | por| se| gna | for| tu| pa;|\n",
      "pe| rò | la | co| man| da | la | man| cia| mi| ta|\n",
      "                           \n",
      " d’ os| so | che | sen | se| gua| ca | mia | pa| lu| pia|\n",
      "che | poi | co| me | sua | via | si | mo| ve | su| per| pe|\n",
      "im| pe| ra | cia| scu| ra | me | là | do| vu| pia|\n",
      " e|\n",
      "                                                      e | si | cre| di| cli | tua | vo| cri| ti| ti|\n",
      "che | la | mia | ma| dre | ch’ u| dir| tu| a | scom| mo|\n",
      "  \n",
      " e | là | do| po | me | an| co| ra | si | ma| sco|\n",
      "a | cui | ma| e| stro an| to| ri| si e | du| ca| ma|\n",
      "del | man| co | si | vol| se a | con| co| min| cia| sco|\n",
      " e | ciò | co| me | suo ’l | mon| do | ciò | co| nan| de|\n",
      "le | la| ce | che | tu | non | tel | cam| min | pen| sa|\n",
      "sia | par| la| ro ad | en| tro a | me | do| lor | fi| ma;|\n",
      "                           \n",
      " e | se | la | me| cen| da | mi | van | l’ u| na | scen| sa|\n",
      "sì | com’ | io | mi | par| lar | lor | da | cor | di| ma|\n",
      "me | el | mi | di| ser| mi | con | la | mu| na|\n",
      "                                                                                                                        r| ma|\n",
      "ed | er | can| tor | suo | mi | mien | me|\n",
      " e | me | al | pa| e| stro e | at| ti | m’ a| da| sna|\n",
      "sì | mi | ve| drai | co| lor | di| cea e | chiou| li| lu|\n",
      "che | sa| to | pro| ca| sca| ta | che | la | fa| ce|\n",
      "                                   \n",
      " sot| to ’l | mon| te | per | se| co | sì | so| mai|\n",
      "di | mia | con | cur | ma| no al | to| mo | pa| ro| a|\n",
      "la | don| na | mal | fo| co e ’l | co| mo | ma| dai|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#k=7\n",
    "generate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8IutQOgA-La"
   },
   "outputs": [],
   "source": [
    "#k=9\n",
    "generate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wvxbavxBHo5"
   },
   "outputs": [],
   "source": [
    "#k=1\n",
    "#t=0.9\n",
    "generate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8iCCpu2BLP1"
   },
   "outputs": [],
   "source": [
    "#k=1\n",
    "#t=0.7\n",
    "generate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgAfhU_sBLp9"
   },
   "outputs": [],
   "source": [
    "#k=1\n",
    "#t=0.5\n",
    "generate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1ZDrJmsU_Vj"
   },
   "outputs": [],
   "source": [
    "#k=5\n",
    "#t=0.5\n",
    "generate(1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Jv5bxDVqR0zv"
   ],
   "name": "DeepComedy_v2_letter_tokenization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
